{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10024ca9-7e94-4f5c-a982-b4537b919d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "  font-family: Verdana, \"DejaVu Sans\", \"Bitstream Vera Sans\", Geneva, sans-serif;\n",
       "  font-weight: bold;\n",
       "}\n",
       "body {\n",
       "  font-family: Verdana, \"DejaVu Sans\", \"Bitstream Vera Sans\", Geneva, sans-serif;\n",
       "  font-weight: 200;\n",
       "}\n",
       "h1 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 30px ;\n",
       "  color: white;\n",
       "  background: #b11d01;\n",
       "  text-align: center;\n",
       "}\n",
       "h2 {\n",
       "  border: 3px solid #333;\n",
       "  padding: 18px ;\n",
       "  color: #b11d01;\n",
       "  background: #ffffff;\n",
       "  text-align: center;\n",
       "}\n",
       "h3 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 12px ;\n",
       "  color: #000000;\n",
       "  background: #c1c1c1;\n",
       "  text-align: left;\n",
       "}\n",
       "h4 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 2px ;\n",
       "  color: #000000;\n",
       "  background: #d9fffc;\n",
       "  text-align: left;\n",
       "}\n",
       "h5 {\n",
       "  border: 1px solid #333;\n",
       "  padding: 2px ;\n",
       "  color: #000000;\n",
       "  background: #ffffff;\n",
       "  text-align: left;\n",
       "}\n",
       ".warn {    \n",
       "    background-color: #fcf2f2;\n",
       "    border-color: #dFb5b4;\n",
       "    border-left: 5px solid #dfb5b4;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       ".rq {    \n",
       "    background-color: #e2e2e2;\n",
       "    border-color: #969696;\n",
       "    border-left: 5px solid #969696;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Start at:** Monday 20 June 2022, 15:07:28  \n",
       "**Hostname:** insa-11357 (Linux)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p style=\"text-align: center\"><img width=\"800px\" src=\"./svg/logoPytChem.svg\" style=\"margin-left:auto; margin-right:auto\"/></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import visualID_Eng as vID\n",
    "vID.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e4b9b",
   "metadata": {},
   "source": [
    "# Prediction by an artificial neural network of the solubility of CO<sub>2</sub> in ionic liquids\n",
    "\n",
    "<div class=\"rq\">\n",
    "<b>Reference</b>: \n",
    "Z. Song, H. Shi, X. Zhang & T. Zhou (**2020**), Prediction of CO<sub>2</sub> solubility in ionic liquids using machine learning methods, [<i>Chem. Eng. Sci.</i> <b>223</b>: 115752](https://www.doi.org/10.1016/j.ces.2020.115752) \n",
    "<br>\n",
    "<p style=\"text-align: center\"><img width=\"650px\" src=\"./CO2-images/AbstractANNCO2-SongEtal.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_AbstractSong\"></p>\n",
    "<br>\n",
    "The main results are graphically reported below.\n",
    "<br>\n",
    "<p style=\"text-align: center\"><img width=\"900px\" src=\"./CO2-images/ANNCO2-SongEtal-Results.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_ResultsSong\"></p>\n",
    "<br>\n",
    "Yet, it seems sthat no standardization process of the data has been applied. \n",
    "    \n",
    "<span style=\"color:red\">Moreover, a spurious separation of the data between training and test sets has been applied: \"<i>Instead of performing random selection, we employ a hybrid artificial-random strategy to decompose the dataset. Specifically, the data points consisting of the least frequently used groups are equally divided into five folders\"</i></span> \n",
    "<br><br>\n",
    "<b>It raises doubts about the stability of the algorithm developped in this paper (*unless the authors forgot to mention that data were standardized*).</b>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84860c4-3fcf-45f5-80a3-607ead68da80",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "<span style=\"font-weight:bold\">The goal of this exercise is to apply the <i>K</i>-fold cross-validation the ANN part of this article, <i>i.e.</i> without standardized data. </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af39c092-f4b7-460b-b698-32e0f2c55461",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os,sys\n",
    "from IPython.display import display\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   OFF = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89a89d-fb12-4ba1-8382-1609a0c4f848",
   "metadata": {},
   "source": [
    "<a id=\"data-read\"></a>\n",
    "## **1.** Database reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d2251b-1f22-42f5-b930-e6032fe55170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IL</th>\n",
       "      <th>cation</th>\n",
       "      <th>anion</th>\n",
       "      <th>x_CO2</th>\n",
       "      <th>T (K)</th>\n",
       "      <th>P (bar)</th>\n",
       "      <th>[CH3]</th>\n",
       "      <th>[CH2]</th>\n",
       "      <th>[CH]</th>\n",
       "      <th>[OCH2]</th>\n",
       "      <th>...</th>\n",
       "      <th>[MeSO3]</th>\n",
       "      <th>[TfO]</th>\n",
       "      <th>[NfO]</th>\n",
       "      <th>[TDfO]</th>\n",
       "      <th>[TOS]</th>\n",
       "      <th>[C12PhSO3]</th>\n",
       "      <th>[DMPO4]</th>\n",
       "      <th>[DEPO4]</th>\n",
       "      <th>[DBPO4]</th>\n",
       "      <th>[methide]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.610</td>\n",
       "      <td>363.15</td>\n",
       "      <td>246.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.500</td>\n",
       "      <td>383.15</td>\n",
       "      <td>235.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.610</td>\n",
       "      <td>353.15</td>\n",
       "      <td>223.30</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.500</td>\n",
       "      <td>373.15</td>\n",
       "      <td>198.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.610</td>\n",
       "      <td>343.15</td>\n",
       "      <td>188.50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10111</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.592</td>\n",
       "      <td>298.15</td>\n",
       "      <td>35.86</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10112</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.239</td>\n",
       "      <td>343.15</td>\n",
       "      <td>27.54</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10113</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.396</td>\n",
       "      <td>298.15</td>\n",
       "      <td>20.15</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10114</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.140</td>\n",
       "      <td>343.15</td>\n",
       "      <td>17.93</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10115</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.139</td>\n",
       "      <td>323.15</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10116 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 IL  cation   anion  x_CO2   T (K)  P (bar)  [CH3]  [CH2]  \\\n",
       "0       [BMIM][BF4]  [BMIM]   [BF4]  0.610  363.15   246.00      1      3   \n",
       "1       [BMIM][BF4]  [BMIM]   [BF4]  0.500  383.15   235.00      1      3   \n",
       "2       [BMIM][BF4]  [BMIM]   [BF4]  0.610  353.15   223.30      1      3   \n",
       "3       [BMIM][BF4]  [BMIM]   [BF4]  0.500  373.15   198.00      1      3   \n",
       "4       [BMIM][BF4]  [BMIM]   [BF4]  0.610  343.15   188.50      1      3   \n",
       "...             ...     ...     ...    ...     ...      ...    ...    ...   \n",
       "10111  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.592  298.15    35.86      1      5   \n",
       "10112  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.239  343.15    27.54      1      5   \n",
       "10113  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.396  298.15    20.15      1      5   \n",
       "10114  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.140  343.15    17.93      1      5   \n",
       "10115  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.139  323.15     8.00      1      5   \n",
       "\n",
       "       [CH]  [OCH2]  ...  [MeSO3]  [TfO]  [NfO]  [TDfO]  [TOS]  [C12PhSO3]  \\\n",
       "0         0       0  ...        0      0      0       0      0           0   \n",
       "1         0       0  ...        0      0      0       0      0           0   \n",
       "2         0       0  ...        0      0      0       0      0           0   \n",
       "3         0       0  ...        0      0      0       0      0           0   \n",
       "4         0       0  ...        0      0      0       0      0           0   \n",
       "...     ...     ...  ...      ...    ...    ...     ...    ...         ...   \n",
       "10111     0       0  ...        0      0      0       0      0           0   \n",
       "10112     0       0  ...        0      0      0       0      0           0   \n",
       "10113     0       0  ...        0      0      0       0      0           0   \n",
       "10114     0       0  ...        0      0      0       0      0           0   \n",
       "10115     0       0  ...        0      0      0       0      0           0   \n",
       "\n",
       "       [DMPO4]  [DEPO4]  [DBPO4]  [methide]  \n",
       "0            0        0        0          0  \n",
       "1            0        0        0          0  \n",
       "2            0        0        0          0  \n",
       "3            0        0        0          0  \n",
       "4            0        0        0          0  \n",
       "...        ...      ...      ...        ...  \n",
       "10111        0        0        0          0  \n",
       "10112        0        0        0          0  \n",
       "10113        0        0        0          0  \n",
       "10114        0        0        0          0  \n",
       "10115        0        0        0          0  \n",
       "\n",
       "[10116 rows x 57 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_22d38\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_22d38_level0_col0\" class=\"col_heading level0 col0\" >x_CO2</th>\n",
       "      <th id=\"T_22d38_level0_col1\" class=\"col_heading level0 col1\" >T (K)</th>\n",
       "      <th id=\"T_22d38_level0_col2\" class=\"col_heading level0 col2\" >P (bar)</th>\n",
       "      <th id=\"T_22d38_level0_col3\" class=\"col_heading level0 col3\" >[CH3]</th>\n",
       "      <th id=\"T_22d38_level0_col4\" class=\"col_heading level0 col4\" >[CH2]</th>\n",
       "      <th id=\"T_22d38_level0_col5\" class=\"col_heading level0 col5\" >[CH]</th>\n",
       "      <th id=\"T_22d38_level0_col6\" class=\"col_heading level0 col6\" >[OCH2]</th>\n",
       "      <th id=\"T_22d38_level0_col7\" class=\"col_heading level0 col7\" >[OCH3]</th>\n",
       "      <th id=\"T_22d38_level0_col8\" class=\"col_heading level0 col8\" >[CF2]</th>\n",
       "      <th id=\"T_22d38_level0_col9\" class=\"col_heading level0 col9\" >[CF3]</th>\n",
       "      <th id=\"T_22d38_level0_col10\" class=\"col_heading level0 col10\" >[OH]</th>\n",
       "      <th id=\"T_22d38_level0_col11\" class=\"col_heading level0 col11\" >CH=CH</th>\n",
       "      <th id=\"T_22d38_level0_col12\" class=\"col_heading level0 col12\" >CH=CH2</th>\n",
       "      <th id=\"T_22d38_level0_col13\" class=\"col_heading level0 col13\" >[Im13]</th>\n",
       "      <th id=\"T_22d38_level0_col14\" class=\"col_heading level0 col14\" >[MIm]</th>\n",
       "      <th id=\"T_22d38_level0_col15\" class=\"col_heading level0 col15\" >[MMIM]</th>\n",
       "      <th id=\"T_22d38_level0_col16\" class=\"col_heading level0 col16\" >[Py]</th>\n",
       "      <th id=\"T_22d38_level0_col17\" class=\"col_heading level0 col17\" >[MPy]</th>\n",
       "      <th id=\"T_22d38_level0_col18\" class=\"col_heading level0 col18\" >[MPyrro]</th>\n",
       "      <th id=\"T_22d38_level0_col19\" class=\"col_heading level0 col19\" >[MPip]</th>\n",
       "      <th id=\"T_22d38_level0_col20\" class=\"col_heading level0 col20\" >[NH3]</th>\n",
       "      <th id=\"T_22d38_level0_col21\" class=\"col_heading level0 col21\" >[NH2]</th>\n",
       "      <th id=\"T_22d38_level0_col22\" class=\"col_heading level0 col22\" >[NH]</th>\n",
       "      <th id=\"T_22d38_level0_col23\" class=\"col_heading level0 col23\" >[N]</th>\n",
       "      <th id=\"T_22d38_level0_col24\" class=\"col_heading level0 col24\" >[P]</th>\n",
       "      <th id=\"T_22d38_level0_col25\" class=\"col_heading level0 col25\" >[S]</th>\n",
       "      <th id=\"T_22d38_level0_col26\" class=\"col_heading level0 col26\" >[BF4]</th>\n",
       "      <th id=\"T_22d38_level0_col27\" class=\"col_heading level0 col27\" >[Cl]</th>\n",
       "      <th id=\"T_22d38_level0_col28\" class=\"col_heading level0 col28\" >[DCA]</th>\n",
       "      <th id=\"T_22d38_level0_col29\" class=\"col_heading level0 col29\" >[NO3]</th>\n",
       "      <th id=\"T_22d38_level0_col30\" class=\"col_heading level0 col30\" >[PF6]</th>\n",
       "      <th id=\"T_22d38_level0_col31\" class=\"col_heading level0 col31\" >[SCN]</th>\n",
       "      <th id=\"T_22d38_level0_col32\" class=\"col_heading level0 col32\" >[TCB]</th>\n",
       "      <th id=\"T_22d38_level0_col33\" class=\"col_heading level0 col33\" >[C(CN)3]</th>\n",
       "      <th id=\"T_22d38_level0_col34\" class=\"col_heading level0 col34\" >[HSO4]</th>\n",
       "      <th id=\"T_22d38_level0_col35\" class=\"col_heading level0 col35\" >[FSA]</th>\n",
       "      <th id=\"T_22d38_level0_col36\" class=\"col_heading level0 col36\" >[Tf2N]</th>\n",
       "      <th id=\"T_22d38_level0_col37\" class=\"col_heading level0 col37\" >[BETA]</th>\n",
       "      <th id=\"T_22d38_level0_col38\" class=\"col_heading level0 col38\" >[FOR]</th>\n",
       "      <th id=\"T_22d38_level0_col39\" class=\"col_heading level0 col39\" >[TFA]</th>\n",
       "      <th id=\"T_22d38_level0_col40\" class=\"col_heading level0 col40\" >[C3F7CO2]</th>\n",
       "      <th id=\"T_22d38_level0_col41\" class=\"col_heading level0 col41\" >[MeSO4]</th>\n",
       "      <th id=\"T_22d38_level0_col42\" class=\"col_heading level0 col42\" >[EtSO4]</th>\n",
       "      <th id=\"T_22d38_level0_col43\" class=\"col_heading level0 col43\" >[MDEGSO4]</th>\n",
       "      <th id=\"T_22d38_level0_col44\" class=\"col_heading level0 col44\" >[MeSO3]</th>\n",
       "      <th id=\"T_22d38_level0_col45\" class=\"col_heading level0 col45\" >[TfO]</th>\n",
       "      <th id=\"T_22d38_level0_col46\" class=\"col_heading level0 col46\" >[NfO]</th>\n",
       "      <th id=\"T_22d38_level0_col47\" class=\"col_heading level0 col47\" >[TDfO]</th>\n",
       "      <th id=\"T_22d38_level0_col48\" class=\"col_heading level0 col48\" >[TOS]</th>\n",
       "      <th id=\"T_22d38_level0_col49\" class=\"col_heading level0 col49\" >[C12PhSO3]</th>\n",
       "      <th id=\"T_22d38_level0_col50\" class=\"col_heading level0 col50\" >[DMPO4]</th>\n",
       "      <th id=\"T_22d38_level0_col51\" class=\"col_heading level0 col51\" >[DEPO4]</th>\n",
       "      <th id=\"T_22d38_level0_col52\" class=\"col_heading level0 col52\" >[DBPO4]</th>\n",
       "      <th id=\"T_22d38_level0_col53\" class=\"col_heading level0 col53\" >[methide]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_22d38_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_22d38_row0_col0\" class=\"data row0 col0\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col1\" class=\"data row0 col1\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col2\" class=\"data row0 col2\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col3\" class=\"data row0 col3\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col4\" class=\"data row0 col4\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col5\" class=\"data row0 col5\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col6\" class=\"data row0 col6\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col7\" class=\"data row0 col7\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col8\" class=\"data row0 col8\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col9\" class=\"data row0 col9\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col10\" class=\"data row0 col10\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col11\" class=\"data row0 col11\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col12\" class=\"data row0 col12\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col13\" class=\"data row0 col13\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col14\" class=\"data row0 col14\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col15\" class=\"data row0 col15\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col16\" class=\"data row0 col16\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col17\" class=\"data row0 col17\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col18\" class=\"data row0 col18\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col19\" class=\"data row0 col19\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col20\" class=\"data row0 col20\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col21\" class=\"data row0 col21\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col22\" class=\"data row0 col22\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col23\" class=\"data row0 col23\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col24\" class=\"data row0 col24\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col25\" class=\"data row0 col25\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col26\" class=\"data row0 col26\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col27\" class=\"data row0 col27\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col28\" class=\"data row0 col28\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col29\" class=\"data row0 col29\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col30\" class=\"data row0 col30\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col31\" class=\"data row0 col31\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col32\" class=\"data row0 col32\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col33\" class=\"data row0 col33\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col34\" class=\"data row0 col34\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col35\" class=\"data row0 col35\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col36\" class=\"data row0 col36\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col37\" class=\"data row0 col37\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col38\" class=\"data row0 col38\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col39\" class=\"data row0 col39\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col40\" class=\"data row0 col40\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col41\" class=\"data row0 col41\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col42\" class=\"data row0 col42\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col43\" class=\"data row0 col43\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col44\" class=\"data row0 col44\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col45\" class=\"data row0 col45\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col46\" class=\"data row0 col46\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col47\" class=\"data row0 col47\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col48\" class=\"data row0 col48\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col49\" class=\"data row0 col49\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col50\" class=\"data row0 col50\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col51\" class=\"data row0 col51\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col52\" class=\"data row0 col52\" >10116.00</td>\n",
       "      <td id=\"T_22d38_row0_col53\" class=\"data row0 col53\" >10116.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22d38_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_22d38_row1_col0\" class=\"data row1 col0\" >0.33</td>\n",
       "      <td id=\"T_22d38_row1_col1\" class=\"data row1 col1\" >325.27</td>\n",
       "      <td id=\"T_22d38_row1_col2\" class=\"data row1 col2\" >54.21</td>\n",
       "      <td id=\"T_22d38_row1_col3\" class=\"data row1 col3\" >1.18</td>\n",
       "      <td id=\"T_22d38_row1_col4\" class=\"data row1 col4\" >4.72</td>\n",
       "      <td id=\"T_22d38_row1_col5\" class=\"data row1 col5\" >0.02</td>\n",
       "      <td id=\"T_22d38_row1_col6\" class=\"data row1 col6\" >0.02</td>\n",
       "      <td id=\"T_22d38_row1_col7\" class=\"data row1 col7\" >0.04</td>\n",
       "      <td id=\"T_22d38_row1_col8\" class=\"data row1 col8\" >0.04</td>\n",
       "      <td id=\"T_22d38_row1_col9\" class=\"data row1 col9\" >0.01</td>\n",
       "      <td id=\"T_22d38_row1_col10\" class=\"data row1 col10\" >0.06</td>\n",
       "      <td id=\"T_22d38_row1_col11\" class=\"data row1 col11\" >0.00</td>\n",
       "      <td id=\"T_22d38_row1_col12\" class=\"data row1 col12\" >0.00</td>\n",
       "      <td id=\"T_22d38_row1_col13\" class=\"data row1 col13\" >0.01</td>\n",
       "      <td id=\"T_22d38_row1_col14\" class=\"data row1 col14\" >0.77</td>\n",
       "      <td id=\"T_22d38_row1_col15\" class=\"data row1 col15\" >0.01</td>\n",
       "      <td id=\"T_22d38_row1_col16\" class=\"data row1 col16\" >0.00</td>\n",
       "      <td id=\"T_22d38_row1_col17\" class=\"data row1 col17\" >0.01</td>\n",
       "      <td id=\"T_22d38_row1_col18\" class=\"data row1 col18\" >0.09</td>\n",
       "      <td id=\"T_22d38_row1_col19\" class=\"data row1 col19\" >0.00</td>\n",
       "      <td id=\"T_22d38_row1_col20\" class=\"data row1 col20\" >0.00</td>\n",
       "      <td id=\"T_22d38_row1_col21\" class=\"data row1 col21\" >0.01</td>\n",
       "      <td id=\"T_22d38_row1_col22\" class=\"data row1 col22\" >0.00</td>\n",
       "      <td id=\"T_22d38_row1_col23\" class=\"data row1 col23\" >0.02</td>\n",
       "      <td id=\"T_22d38_row1_col24\" class=\"data row1 col24\" >0.05</td>\n",
       "      <td id=\"T_22d38_row1_col25\" class=\"data row1 col25\" >0.00</td>\n",
       "      <td id=\"T_22d38_row1_col26\" class=\"data row1 col26\" >0.11</td>\n",
       "      <td id=\"T_22d38_row1_col27\" class=\"data row1 col27\" >0.02</td>\n",
       "      <td id=\"T_22d38_row1_col28\" class=\"data row1 col28\" >0.02</td>\n",
       "      <td id=\"T_22d38_row1_col29\" class=\"data row1 col29\" >0.02</td>\n",
       "      <td id=\"T_22d38_row1_col30\" class=\"data row1 col30\" >0.11</td>\n",
       "      <td id=\"T_22d38_row1_col31\" class=\"data row1 col31\" >0.02</td>\n",
       "      <td id=\"T_22d38_row1_col32\" class=\"data row1 col32\" >0.01</td>\n",
       "      <td id=\"T_22d38_row1_col33\" class=\"data row1 col33\" >0.07</td>\n",
       "      <td id=\"T_22d38_row1_col34\" class=\"data row1 col34\" >0.00</td>\n",
       "      <td id=\"T_22d38_row1_col35\" class=\"data row1 col35\" >0.01</td>\n",
       "      <td id=\"T_22d38_row1_col36\" class=\"data row1 col36\" >0.43</td>\n",
       "      <td id=\"T_22d38_row1_col37\" class=\"data row1 col37\" >0.00</td>\n",
       "      <td id=\"T_22d38_row1_col38\" class=\"data row1 col38\" >0.01</td>\n",
       "      <td id=\"T_22d38_row1_col39\" class=\"data row1 col39\" >0.01</td>\n",
       "      <td id=\"T_22d38_row1_col40\" class=\"data row1 col40\" >0.00</td>\n",
       "      <td id=\"T_22d38_row1_col41\" class=\"data row1 col41\" >0.02</td>\n",
       "      <td id=\"T_22d38_row1_col42\" class=\"data row1 col42\" >0.01</td>\n",
       "      <td id=\"T_22d38_row1_col43\" class=\"data row1 col43\" >0.01</td>\n",
       "      <td id=\"T_22d38_row1_col44\" class=\"data row1 col44\" >0.02</td>\n",
       "      <td id=\"T_22d38_row1_col45\" class=\"data row1 col45\" >0.05</td>\n",
       "      <td id=\"T_22d38_row1_col46\" class=\"data row1 col46\" >0.01</td>\n",
       "      <td id=\"T_22d38_row1_col47\" class=\"data row1 col47\" >0.01</td>\n",
       "      <td id=\"T_22d38_row1_col48\" class=\"data row1 col48\" >0.00</td>\n",
       "      <td id=\"T_22d38_row1_col49\" class=\"data row1 col49\" >0.01</td>\n",
       "      <td id=\"T_22d38_row1_col50\" class=\"data row1 col50\" >0.00</td>\n",
       "      <td id=\"T_22d38_row1_col51\" class=\"data row1 col51\" >0.01</td>\n",
       "      <td id=\"T_22d38_row1_col52\" class=\"data row1 col52\" >0.00</td>\n",
       "      <td id=\"T_22d38_row1_col53\" class=\"data row1 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22d38_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_22d38_row2_col0\" class=\"data row2 col0\" >0.24</td>\n",
       "      <td id=\"T_22d38_row2_col1\" class=\"data row2 col1\" >25.24</td>\n",
       "      <td id=\"T_22d38_row2_col2\" class=\"data row2 col2\" >76.66</td>\n",
       "      <td id=\"T_22d38_row2_col3\" class=\"data row2 col3\" >0.96</td>\n",
       "      <td id=\"T_22d38_row2_col4\" class=\"data row2 col4\" >5.48</td>\n",
       "      <td id=\"T_22d38_row2_col5\" class=\"data row2 col5\" >0.25</td>\n",
       "      <td id=\"T_22d38_row2_col6\" class=\"data row2 col6\" >0.16</td>\n",
       "      <td id=\"T_22d38_row2_col7\" class=\"data row2 col7\" >0.20</td>\n",
       "      <td id=\"T_22d38_row2_col8\" class=\"data row2 col8\" >0.39</td>\n",
       "      <td id=\"T_22d38_row2_col9\" class=\"data row2 col9\" >0.10</td>\n",
       "      <td id=\"T_22d38_row2_col10\" class=\"data row2 col10\" >0.28</td>\n",
       "      <td id=\"T_22d38_row2_col11\" class=\"data row2 col11\" >0.06</td>\n",
       "      <td id=\"T_22d38_row2_col12\" class=\"data row2 col12\" >0.06</td>\n",
       "      <td id=\"T_22d38_row2_col13\" class=\"data row2 col13\" >0.10</td>\n",
       "      <td id=\"T_22d38_row2_col14\" class=\"data row2 col14\" >0.42</td>\n",
       "      <td id=\"T_22d38_row2_col15\" class=\"data row2 col15\" >0.08</td>\n",
       "      <td id=\"T_22d38_row2_col16\" class=\"data row2 col16\" >0.07</td>\n",
       "      <td id=\"T_22d38_row2_col17\" class=\"data row2 col17\" >0.11</td>\n",
       "      <td id=\"T_22d38_row2_col18\" class=\"data row2 col18\" >0.29</td>\n",
       "      <td id=\"T_22d38_row2_col19\" class=\"data row2 col19\" >0.06</td>\n",
       "      <td id=\"T_22d38_row2_col20\" class=\"data row2 col20\" >0.07</td>\n",
       "      <td id=\"T_22d38_row2_col21\" class=\"data row2 col21\" >0.09</td>\n",
       "      <td id=\"T_22d38_row2_col22\" class=\"data row2 col22\" >0.06</td>\n",
       "      <td id=\"T_22d38_row2_col23\" class=\"data row2 col23\" >0.16</td>\n",
       "      <td id=\"T_22d38_row2_col24\" class=\"data row2 col24\" >0.23</td>\n",
       "      <td id=\"T_22d38_row2_col25\" class=\"data row2 col25\" >0.06</td>\n",
       "      <td id=\"T_22d38_row2_col26\" class=\"data row2 col26\" >0.31</td>\n",
       "      <td id=\"T_22d38_row2_col27\" class=\"data row2 col27\" >0.12</td>\n",
       "      <td id=\"T_22d38_row2_col28\" class=\"data row2 col28\" >0.15</td>\n",
       "      <td id=\"T_22d38_row2_col29\" class=\"data row2 col29\" >0.12</td>\n",
       "      <td id=\"T_22d38_row2_col30\" class=\"data row2 col30\" >0.31</td>\n",
       "      <td id=\"T_22d38_row2_col31\" class=\"data row2 col31\" >0.14</td>\n",
       "      <td id=\"T_22d38_row2_col32\" class=\"data row2 col32\" >0.08</td>\n",
       "      <td id=\"T_22d38_row2_col33\" class=\"data row2 col33\" >0.26</td>\n",
       "      <td id=\"T_22d38_row2_col34\" class=\"data row2 col34\" >0.04</td>\n",
       "      <td id=\"T_22d38_row2_col35\" class=\"data row2 col35\" >0.11</td>\n",
       "      <td id=\"T_22d38_row2_col36\" class=\"data row2 col36\" >0.49</td>\n",
       "      <td id=\"T_22d38_row2_col37\" class=\"data row2 col37\" >0.03</td>\n",
       "      <td id=\"T_22d38_row2_col38\" class=\"data row2 col38\" >0.11</td>\n",
       "      <td id=\"T_22d38_row2_col39\" class=\"data row2 col39\" >0.11</td>\n",
       "      <td id=\"T_22d38_row2_col40\" class=\"data row2 col40\" >0.05</td>\n",
       "      <td id=\"T_22d38_row2_col41\" class=\"data row2 col41\" >0.13</td>\n",
       "      <td id=\"T_22d38_row2_col42\" class=\"data row2 col42\" >0.11</td>\n",
       "      <td id=\"T_22d38_row2_col43\" class=\"data row2 col43\" >0.10</td>\n",
       "      <td id=\"T_22d38_row2_col44\" class=\"data row2 col44\" >0.15</td>\n",
       "      <td id=\"T_22d38_row2_col45\" class=\"data row2 col45\" >0.23</td>\n",
       "      <td id=\"T_22d38_row2_col46\" class=\"data row2 col46\" >0.09</td>\n",
       "      <td id=\"T_22d38_row2_col47\" class=\"data row2 col47\" >0.08</td>\n",
       "      <td id=\"T_22d38_row2_col48\" class=\"data row2 col48\" >0.06</td>\n",
       "      <td id=\"T_22d38_row2_col49\" class=\"data row2 col49\" >0.10</td>\n",
       "      <td id=\"T_22d38_row2_col50\" class=\"data row2 col50\" >0.03</td>\n",
       "      <td id=\"T_22d38_row2_col51\" class=\"data row2 col51\" >0.07</td>\n",
       "      <td id=\"T_22d38_row2_col52\" class=\"data row2 col52\" >0.04</td>\n",
       "      <td id=\"T_22d38_row2_col53\" class=\"data row2 col53\" >0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22d38_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_22d38_row3_col0\" class=\"data row3 col0\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col1\" class=\"data row3 col1\" >243.20</td>\n",
       "      <td id=\"T_22d38_row3_col2\" class=\"data row3 col2\" >0.01</td>\n",
       "      <td id=\"T_22d38_row3_col3\" class=\"data row3 col3\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col4\" class=\"data row3 col4\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col5\" class=\"data row3 col5\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col6\" class=\"data row3 col6\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col7\" class=\"data row3 col7\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col8\" class=\"data row3 col8\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col9\" class=\"data row3 col9\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col10\" class=\"data row3 col10\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col11\" class=\"data row3 col11\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col12\" class=\"data row3 col12\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col13\" class=\"data row3 col13\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col14\" class=\"data row3 col14\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col15\" class=\"data row3 col15\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col16\" class=\"data row3 col16\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col17\" class=\"data row3 col17\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col18\" class=\"data row3 col18\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col19\" class=\"data row3 col19\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col20\" class=\"data row3 col20\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col21\" class=\"data row3 col21\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col22\" class=\"data row3 col22\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col23\" class=\"data row3 col23\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col24\" class=\"data row3 col24\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col25\" class=\"data row3 col25\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col26\" class=\"data row3 col26\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col27\" class=\"data row3 col27\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col28\" class=\"data row3 col28\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col29\" class=\"data row3 col29\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col30\" class=\"data row3 col30\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col31\" class=\"data row3 col31\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col32\" class=\"data row3 col32\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col33\" class=\"data row3 col33\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col34\" class=\"data row3 col34\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col35\" class=\"data row3 col35\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col36\" class=\"data row3 col36\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col37\" class=\"data row3 col37\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col38\" class=\"data row3 col38\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col39\" class=\"data row3 col39\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col40\" class=\"data row3 col40\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col41\" class=\"data row3 col41\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col42\" class=\"data row3 col42\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col43\" class=\"data row3 col43\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col44\" class=\"data row3 col44\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col45\" class=\"data row3 col45\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col46\" class=\"data row3 col46\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col47\" class=\"data row3 col47\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col48\" class=\"data row3 col48\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col49\" class=\"data row3 col49\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col50\" class=\"data row3 col50\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col51\" class=\"data row3 col51\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col52\" class=\"data row3 col52\" >0.00</td>\n",
       "      <td id=\"T_22d38_row3_col53\" class=\"data row3 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22d38_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_22d38_row4_col0\" class=\"data row4 col0\" >0.14</td>\n",
       "      <td id=\"T_22d38_row4_col1\" class=\"data row4 col1\" >308.15</td>\n",
       "      <td id=\"T_22d38_row4_col2\" class=\"data row4 col2\" >10.00</td>\n",
       "      <td id=\"T_22d38_row4_col3\" class=\"data row4 col3\" >1.00</td>\n",
       "      <td id=\"T_22d38_row4_col4\" class=\"data row4 col4\" >3.00</td>\n",
       "      <td id=\"T_22d38_row4_col5\" class=\"data row4 col5\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col6\" class=\"data row4 col6\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col7\" class=\"data row4 col7\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col8\" class=\"data row4 col8\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col9\" class=\"data row4 col9\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col10\" class=\"data row4 col10\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col11\" class=\"data row4 col11\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col12\" class=\"data row4 col12\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col13\" class=\"data row4 col13\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col14\" class=\"data row4 col14\" >1.00</td>\n",
       "      <td id=\"T_22d38_row4_col15\" class=\"data row4 col15\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col16\" class=\"data row4 col16\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col17\" class=\"data row4 col17\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col18\" class=\"data row4 col18\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col19\" class=\"data row4 col19\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col20\" class=\"data row4 col20\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col21\" class=\"data row4 col21\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col22\" class=\"data row4 col22\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col23\" class=\"data row4 col23\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col24\" class=\"data row4 col24\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col25\" class=\"data row4 col25\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col26\" class=\"data row4 col26\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col27\" class=\"data row4 col27\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col28\" class=\"data row4 col28\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col29\" class=\"data row4 col29\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col30\" class=\"data row4 col30\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col31\" class=\"data row4 col31\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col32\" class=\"data row4 col32\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col33\" class=\"data row4 col33\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col34\" class=\"data row4 col34\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col35\" class=\"data row4 col35\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col36\" class=\"data row4 col36\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col37\" class=\"data row4 col37\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col38\" class=\"data row4 col38\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col39\" class=\"data row4 col39\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col40\" class=\"data row4 col40\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col41\" class=\"data row4 col41\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col42\" class=\"data row4 col42\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col43\" class=\"data row4 col43\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col44\" class=\"data row4 col44\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col45\" class=\"data row4 col45\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col46\" class=\"data row4 col46\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col47\" class=\"data row4 col47\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col48\" class=\"data row4 col48\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col49\" class=\"data row4 col49\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col50\" class=\"data row4 col50\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col51\" class=\"data row4 col51\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col52\" class=\"data row4 col52\" >0.00</td>\n",
       "      <td id=\"T_22d38_row4_col53\" class=\"data row4 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22d38_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_22d38_row5_col0\" class=\"data row5 col0\" >0.30</td>\n",
       "      <td id=\"T_22d38_row5_col1\" class=\"data row5 col1\" >323.15</td>\n",
       "      <td id=\"T_22d38_row5_col2\" class=\"data row5 col2\" >26.80</td>\n",
       "      <td id=\"T_22d38_row5_col3\" class=\"data row5 col3\" >1.00</td>\n",
       "      <td id=\"T_22d38_row5_col4\" class=\"data row5 col4\" >3.00</td>\n",
       "      <td id=\"T_22d38_row5_col5\" class=\"data row5 col5\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col6\" class=\"data row5 col6\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col7\" class=\"data row5 col7\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col8\" class=\"data row5 col8\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col9\" class=\"data row5 col9\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col10\" class=\"data row5 col10\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col11\" class=\"data row5 col11\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col12\" class=\"data row5 col12\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col13\" class=\"data row5 col13\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col14\" class=\"data row5 col14\" >1.00</td>\n",
       "      <td id=\"T_22d38_row5_col15\" class=\"data row5 col15\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col16\" class=\"data row5 col16\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col17\" class=\"data row5 col17\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col18\" class=\"data row5 col18\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col19\" class=\"data row5 col19\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col20\" class=\"data row5 col20\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col21\" class=\"data row5 col21\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col22\" class=\"data row5 col22\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col23\" class=\"data row5 col23\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col24\" class=\"data row5 col24\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col25\" class=\"data row5 col25\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col26\" class=\"data row5 col26\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col27\" class=\"data row5 col27\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col28\" class=\"data row5 col28\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col29\" class=\"data row5 col29\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col30\" class=\"data row5 col30\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col31\" class=\"data row5 col31\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col32\" class=\"data row5 col32\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col33\" class=\"data row5 col33\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col34\" class=\"data row5 col34\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col35\" class=\"data row5 col35\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col36\" class=\"data row5 col36\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col37\" class=\"data row5 col37\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col38\" class=\"data row5 col38\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col39\" class=\"data row5 col39\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col40\" class=\"data row5 col40\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col41\" class=\"data row5 col41\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col42\" class=\"data row5 col42\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col43\" class=\"data row5 col43\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col44\" class=\"data row5 col44\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col45\" class=\"data row5 col45\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col46\" class=\"data row5 col46\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col47\" class=\"data row5 col47\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col48\" class=\"data row5 col48\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col49\" class=\"data row5 col49\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col50\" class=\"data row5 col50\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col51\" class=\"data row5 col51\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col52\" class=\"data row5 col52\" >0.00</td>\n",
       "      <td id=\"T_22d38_row5_col53\" class=\"data row5 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22d38_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_22d38_row6_col0\" class=\"data row6 col0\" >0.51</td>\n",
       "      <td id=\"T_22d38_row6_col1\" class=\"data row6 col1\" >342.59</td>\n",
       "      <td id=\"T_22d38_row6_col2\" class=\"data row6 col2\" >64.76</td>\n",
       "      <td id=\"T_22d38_row6_col3\" class=\"data row6 col3\" >1.00</td>\n",
       "      <td id=\"T_22d38_row6_col4\" class=\"data row6 col4\" >5.00</td>\n",
       "      <td id=\"T_22d38_row6_col5\" class=\"data row6 col5\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col6\" class=\"data row6 col6\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col7\" class=\"data row6 col7\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col8\" class=\"data row6 col8\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col9\" class=\"data row6 col9\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col10\" class=\"data row6 col10\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col11\" class=\"data row6 col11\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col12\" class=\"data row6 col12\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col13\" class=\"data row6 col13\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col14\" class=\"data row6 col14\" >1.00</td>\n",
       "      <td id=\"T_22d38_row6_col15\" class=\"data row6 col15\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col16\" class=\"data row6 col16\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col17\" class=\"data row6 col17\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col18\" class=\"data row6 col18\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col19\" class=\"data row6 col19\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col20\" class=\"data row6 col20\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col21\" class=\"data row6 col21\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col22\" class=\"data row6 col22\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col23\" class=\"data row6 col23\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col24\" class=\"data row6 col24\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col25\" class=\"data row6 col25\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col26\" class=\"data row6 col26\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col27\" class=\"data row6 col27\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col28\" class=\"data row6 col28\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col29\" class=\"data row6 col29\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col30\" class=\"data row6 col30\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col31\" class=\"data row6 col31\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col32\" class=\"data row6 col32\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col33\" class=\"data row6 col33\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col34\" class=\"data row6 col34\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col35\" class=\"data row6 col35\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col36\" class=\"data row6 col36\" >1.00</td>\n",
       "      <td id=\"T_22d38_row6_col37\" class=\"data row6 col37\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col38\" class=\"data row6 col38\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col39\" class=\"data row6 col39\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col40\" class=\"data row6 col40\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col41\" class=\"data row6 col41\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col42\" class=\"data row6 col42\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col43\" class=\"data row6 col43\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col44\" class=\"data row6 col44\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col45\" class=\"data row6 col45\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col46\" class=\"data row6 col46\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col47\" class=\"data row6 col47\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col48\" class=\"data row6 col48\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col49\" class=\"data row6 col49\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col50\" class=\"data row6 col50\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col51\" class=\"data row6 col51\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col52\" class=\"data row6 col52\" >0.00</td>\n",
       "      <td id=\"T_22d38_row6_col53\" class=\"data row6 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22d38_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_22d38_row7_col0\" class=\"data row7 col0\" >0.95</td>\n",
       "      <td id=\"T_22d38_row7_col1\" class=\"data row7 col1\" >453.15</td>\n",
       "      <td id=\"T_22d38_row7_col2\" class=\"data row7 col2\" >499.90</td>\n",
       "      <td id=\"T_22d38_row7_col3\" class=\"data row7 col3\" >7.00</td>\n",
       "      <td id=\"T_22d38_row7_col4\" class=\"data row7 col4\" >28.00</td>\n",
       "      <td id=\"T_22d38_row7_col5\" class=\"data row7 col5\" >3.00</td>\n",
       "      <td id=\"T_22d38_row7_col6\" class=\"data row7 col6\" >2.00</td>\n",
       "      <td id=\"T_22d38_row7_col7\" class=\"data row7 col7\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col8\" class=\"data row7 col8\" >5.00</td>\n",
       "      <td id=\"T_22d38_row7_col9\" class=\"data row7 col9\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col10\" class=\"data row7 col10\" >3.00</td>\n",
       "      <td id=\"T_22d38_row7_col11\" class=\"data row7 col11\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col12\" class=\"data row7 col12\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col13\" class=\"data row7 col13\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col14\" class=\"data row7 col14\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col15\" class=\"data row7 col15\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col16\" class=\"data row7 col16\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col17\" class=\"data row7 col17\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col18\" class=\"data row7 col18\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col19\" class=\"data row7 col19\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col20\" class=\"data row7 col20\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col21\" class=\"data row7 col21\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col22\" class=\"data row7 col22\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col23\" class=\"data row7 col23\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col24\" class=\"data row7 col24\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col25\" class=\"data row7 col25\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col26\" class=\"data row7 col26\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col27\" class=\"data row7 col27\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col28\" class=\"data row7 col28\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col29\" class=\"data row7 col29\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col30\" class=\"data row7 col30\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col31\" class=\"data row7 col31\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col32\" class=\"data row7 col32\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col33\" class=\"data row7 col33\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col34\" class=\"data row7 col34\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col35\" class=\"data row7 col35\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col36\" class=\"data row7 col36\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col37\" class=\"data row7 col37\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col38\" class=\"data row7 col38\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col39\" class=\"data row7 col39\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col40\" class=\"data row7 col40\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col41\" class=\"data row7 col41\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col42\" class=\"data row7 col42\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col43\" class=\"data row7 col43\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col44\" class=\"data row7 col44\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col45\" class=\"data row7 col45\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col46\" class=\"data row7 col46\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col47\" class=\"data row7 col47\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col48\" class=\"data row7 col48\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col49\" class=\"data row7 col49\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col50\" class=\"data row7 col50\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col51\" class=\"data row7 col51\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col52\" class=\"data row7 col52\" >1.00</td>\n",
       "      <td id=\"T_22d38_row7_col53\" class=\"data row7 col53\" >1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f670050db80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataCO2f='CO2-data'+'/'+'dataCO2.csv'\n",
    "dataCO2=pd.read_csv(dataCO2f,sep=\";\",header=0)\n",
    "display(dataCO2)\n",
    "# describe() generates descriptive statistics\n",
    "display(dataCO2.describe().style.format(\"{0:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf1e66-0756-47f2-928d-d0d578613bba",
   "metadata": {},
   "source": [
    "## 2. Assessment of the stability of the original ML algorithm of Song *et al*. by *K*-fold cross validation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b3ba1f-2f90-441f-b931-28bc9e64748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# separation of the data set into two subsets: (1) training of the ANN & (2) test of the ANN\n",
    "# library used: pandas\n",
    "xdata = dataCO2.drop(['IL','cation','anion','x_CO2'],axis=1)\n",
    "ydata = dataCO2['x_CO2']\n",
    "\n",
    "#######################################################################################\n",
    "# ANN: 1 input layer (53 neurons) / 2 hidden layers (20 and 7 neurons) / 1 output layer (1 neuron) \n",
    "# library used: keras\n",
    "\n",
    "def defANN(shape,acthL):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name='iLayer'))\n",
    "    model.add(keras.layers.Dense(7, activation=acthL, name='hLayer'))\n",
    "    model.add(keras.layers.Dense(1, name='oLayer'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model\n",
    "\n",
    "acthL='tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a3138b7-5dc7-4804-93cb-caab9887319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[91mFold 0\u001b[0m\n",
      "Epoch 1/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 1.5063 - mae: 1.0003 - mse: 1.5063 - val_loss: 0.0592 - val_mae: 0.2031 - val_mse: 0.0592\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0575 - mae: 0.1978 - mse: 0.0575 - val_loss: 0.0557 - val_mae: 0.1980 - val_mse: 0.0557\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0534 - mae: 0.1933 - mse: 0.0534 - val_loss: 0.0575 - val_mae: 0.1997 - val_mse: 0.0575\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0540 - mae: 0.1933 - mse: 0.0540 - val_loss: 0.0530 - val_mae: 0.1930 - val_mse: 0.0530\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0527 - mae: 0.1897 - mse: 0.0527 - val_loss: 0.0573 - val_mae: 0.1983 - val_mse: 0.0573\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0553 - mae: 0.1950 - mse: 0.0553 - val_loss: 0.0508 - val_mae: 0.1898 - val_mse: 0.0508\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0491 - mae: 0.1852 - mse: 0.0491 - val_loss: 0.0501 - val_mae: 0.1878 - val_mse: 0.0501\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0492 - mae: 0.1857 - mse: 0.0492 - val_loss: 0.0519 - val_mae: 0.1903 - val_mse: 0.0519\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0497 - mae: 0.1862 - mse: 0.0497 - val_loss: 0.0482 - val_mae: 0.1845 - val_mse: 0.0482\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0478 - mae: 0.1821 - mse: 0.0478 - val_loss: 0.0468 - val_mae: 0.1836 - val_mse: 0.0468\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0470 - mae: 0.1803 - mse: 0.0470 - val_loss: 0.0439 - val_mae: 0.1758 - val_mse: 0.0439\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0433 - mae: 0.1721 - mse: 0.0433 - val_loss: 0.0393 - val_mae: 0.1636 - val_mse: 0.0393\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0375 - mae: 0.1590 - mse: 0.0375 - val_loss: 0.0326 - val_mae: 0.1469 - val_mse: 0.0326\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0328 - mae: 0.1453 - mse: 0.0328 - val_loss: 0.0214 - val_mae: 0.1164 - val_mse: 0.0214\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0221 - mae: 0.1186 - mse: 0.0221 - val_loss: 0.0196 - val_mae: 0.1122 - val_mse: 0.0196\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0213 - mae: 0.1166 - mse: 0.0213 - val_loss: 0.0190 - val_mae: 0.1106 - val_mse: 0.0190\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0196 - mae: 0.1127 - mse: 0.0196 - val_loss: 0.0191 - val_mae: 0.1092 - val_mse: 0.0191\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0188 - mae: 0.1076 - mse: 0.0188 - val_loss: 0.0165 - val_mae: 0.1006 - val_mse: 0.0165\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0173 - mae: 0.1019 - mse: 0.0173 - val_loss: 0.0166 - val_mae: 0.0975 - val_mse: 0.0166\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0160 - mae: 0.0962 - mse: 0.0160 - val_loss: 0.0161 - val_mae: 0.0953 - val_mse: 0.0161\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0156 - mae: 0.0951 - mse: 0.0156 - val_loss: 0.0142 - val_mae: 0.0906 - val_mse: 0.0142\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0141 - mae: 0.0902 - mse: 0.0141 - val_loss: 0.0134 - val_mae: 0.0848 - val_mse: 0.0134\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0128 - mae: 0.0837 - mse: 0.0128 - val_loss: 0.0119 - val_mae: 0.0798 - val_mse: 0.0119\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0125 - mae: 0.0824 - mse: 0.0125 - val_loss: 0.0094 - val_mae: 0.0722 - val_mse: 0.0094\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0104 - mae: 0.0763 - mse: 0.0104 - val_loss: 0.0085 - val_mae: 0.0689 - val_mse: 0.0085\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0092 - mae: 0.0723 - mse: 0.0092 - val_loss: 0.0082 - val_mae: 0.0674 - val_mse: 0.0082\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0702 - mse: 0.0085 - val_loss: 0.0082 - val_mae: 0.0678 - val_mse: 0.0082\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0703 - mse: 0.0086 - val_loss: 0.0081 - val_mae: 0.0678 - val_mse: 0.0081\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0692 - mse: 0.0083 - val_loss: 0.0122 - val_mae: 0.0894 - val_mse: 0.0122\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0708 - mse: 0.0086 - val_loss: 0.0078 - val_mae: 0.0655 - val_mse: 0.0078\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0680 - mse: 0.0082 - val_loss: 0.0083 - val_mae: 0.0698 - val_mse: 0.0083\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0079 - mae: 0.0680 - mse: 0.0079 - val_loss: 0.0076 - val_mae: 0.0651 - val_mse: 0.0076\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0091 - mae: 0.0725 - mse: 0.0091 - val_loss: 0.0076 - val_mae: 0.0645 - val_mse: 0.0076\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0081 - mae: 0.0687 - mse: 0.0081 - val_loss: 0.0083 - val_mae: 0.0679 - val_mse: 0.0083\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0080 - mae: 0.0674 - mse: 0.0080 - val_loss: 0.0074 - val_mae: 0.0642 - val_mse: 0.0074\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0079 - mae: 0.0678 - mse: 0.0079 - val_loss: 0.0090 - val_mae: 0.0747 - val_mse: 0.0090\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0693 - mse: 0.0083 - val_loss: 0.0090 - val_mae: 0.0719 - val_mse: 0.0090\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0076 - mae: 0.0660 - mse: 0.0076 - val_loss: 0.0076 - val_mae: 0.0666 - val_mse: 0.0076\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0076 - mae: 0.0660 - mse: 0.0076 - val_loss: 0.0071 - val_mae: 0.0626 - val_mse: 0.0071\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0076 - mae: 0.0657 - mse: 0.0076 - val_loss: 0.0071 - val_mae: 0.0628 - val_mse: 0.0071\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0075 - mae: 0.0654 - mse: 0.0075 - val_loss: 0.0081 - val_mae: 0.0709 - val_mse: 0.0081\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0076 - mae: 0.0663 - mse: 0.0076 - val_loss: 0.0071 - val_mae: 0.0631 - val_mse: 0.0071\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0074 - mae: 0.0644 - mse: 0.0074 - val_loss: 0.0072 - val_mae: 0.0639 - val_mse: 0.0072\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0077 - mae: 0.0667 - mse: 0.0077 - val_loss: 0.0072 - val_mae: 0.0625 - val_mse: 0.0072\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0075 - mae: 0.0652 - mse: 0.0075 - val_loss: 0.0123 - val_mae: 0.0916 - val_mse: 0.0123\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0077 - mae: 0.0671 - mse: 0.0077 - val_loss: 0.0068 - val_mae: 0.0615 - val_mse: 0.0068\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0071 - mae: 0.0639 - mse: 0.0071 - val_loss: 0.0074 - val_mae: 0.0665 - val_mse: 0.0074\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0073 - mae: 0.0647 - mse: 0.0073 - val_loss: 0.0075 - val_mae: 0.0639 - val_mse: 0.0075\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0078 - mae: 0.0674 - mse: 0.0078 - val_loss: 0.0068 - val_mae: 0.0607 - val_mse: 0.0068\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0076 - mae: 0.0664 - mse: 0.0076 - val_loss: 0.0067 - val_mae: 0.0611 - val_mse: 0.0067\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0075 - mae: 0.0657 - mse: 0.0075 - val_loss: 0.0068 - val_mae: 0.0605 - val_mse: 0.0068\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0072 - mae: 0.0643 - mse: 0.0072 - val_loss: 0.0067 - val_mae: 0.0613 - val_mse: 0.0067\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0626 - mse: 0.0069 - val_loss: 0.0074 - val_mae: 0.0634 - val_mse: 0.0074\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0073 - mae: 0.0649 - mse: 0.0073 - val_loss: 0.0072 - val_mae: 0.0654 - val_mse: 0.0072\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0072 - mae: 0.0643 - mse: 0.0072 - val_loss: 0.0068 - val_mae: 0.0610 - val_mse: 0.0068\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0073 - mae: 0.0647 - mse: 0.0073 - val_loss: 0.0076 - val_mae: 0.0684 - val_mse: 0.0076\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0073 - mae: 0.0645 - mse: 0.0073 - val_loss: 0.0070 - val_mae: 0.0615 - val_mse: 0.0070\n",
      "Epoch 58/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0070 - mae: 0.0635 - mse: 0.0070 - val_loss: 0.0065 - val_mae: 0.0595 - val_mse: 0.0065\n",
      "Epoch 59/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0072 - mae: 0.0636 - mse: 0.0072 - val_loss: 0.0065 - val_mae: 0.0594 - val_mse: 0.0065\n",
      "Epoch 60/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0072 - mae: 0.0632 - mse: 0.0072 - val_loss: 0.0091 - val_mae: 0.0734 - val_mse: 0.0091\n",
      "Epoch 61/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0072 - mae: 0.0647 - mse: 0.0072 - val_loss: 0.0066 - val_mae: 0.0613 - val_mse: 0.0066\n",
      "Epoch 62/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0630 - mse: 0.0069 - val_loss: 0.0065 - val_mae: 0.0600 - val_mse: 0.0065\n",
      "Epoch 63/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0627 - mse: 0.0068 - val_loss: 0.0116 - val_mae: 0.0886 - val_mse: 0.0116\n",
      "Epoch 64/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0626 - mse: 0.0068 - val_loss: 0.0064 - val_mae: 0.0591 - val_mse: 0.0064\n",
      "Epoch 65/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0638 - mse: 0.0068 - val_loss: 0.0073 - val_mae: 0.0666 - val_mse: 0.0073\n",
      "Epoch 66/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0071 - mae: 0.0642 - mse: 0.0071 - val_loss: 0.0071 - val_mae: 0.0654 - val_mse: 0.0071\n",
      "Epoch 67/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0623 - mse: 0.0068 - val_loss: 0.0064 - val_mae: 0.0605 - val_mse: 0.0064\n",
      "Epoch 68/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0611 - mse: 0.0065 - val_loss: 0.0062 - val_mae: 0.0588 - val_mse: 0.0062\n",
      "Epoch 69/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0606 - mse: 0.0065 - val_loss: 0.0064 - val_mae: 0.0593 - val_mse: 0.0064\n",
      "Epoch 70/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0624 - mse: 0.0066 - val_loss: 0.0064 - val_mae: 0.0587 - val_mse: 0.0064\n",
      "Epoch 71/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0615 - mse: 0.0066 - val_loss: 0.0072 - val_mae: 0.0667 - val_mse: 0.0072\n",
      "Epoch 72/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0618 - mse: 0.0067 - val_loss: 0.0105 - val_mae: 0.0806 - val_mse: 0.0105\n",
      "Epoch 73/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0610 - mse: 0.0065 - val_loss: 0.0063 - val_mae: 0.0602 - val_mse: 0.0063\n",
      "Epoch 74/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0610 - mse: 0.0065 - val_loss: 0.0061 - val_mae: 0.0581 - val_mse: 0.0061\n",
      "Epoch 75/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0606 - mse: 0.0065 - val_loss: 0.0064 - val_mae: 0.0597 - val_mse: 0.0064\n",
      "Epoch 76/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0635 - mse: 0.0069 - val_loss: 0.0062 - val_mae: 0.0587 - val_mse: 0.0062\n",
      "Epoch 77/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0611 - mse: 0.0065 - val_loss: 0.0068 - val_mae: 0.0638 - val_mse: 0.0068\n",
      "Epoch 78/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0628 - mse: 0.0067 - val_loss: 0.0062 - val_mae: 0.0589 - val_mse: 0.0062\n",
      "Epoch 79/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0609 - mse: 0.0064 - val_loss: 0.0060 - val_mae: 0.0569 - val_mse: 0.0060\n",
      "Epoch 80/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0603 - mse: 0.0063 - val_loss: 0.0061 - val_mae: 0.0573 - val_mse: 0.0061\n",
      "Epoch 81/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0602 - mse: 0.0063 - val_loss: 0.0061 - val_mae: 0.0577 - val_mse: 0.0061\n",
      "Epoch 82/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0598 - mse: 0.0063 - val_loss: 0.0061 - val_mae: 0.0580 - val_mse: 0.0061\n",
      "Epoch 83/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0609 - mse: 0.0065 - val_loss: 0.0077 - val_mae: 0.0677 - val_mse: 0.0077\n",
      "Epoch 84/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0608 - mse: 0.0063 - val_loss: 0.0067 - val_mae: 0.0638 - val_mse: 0.0067\n",
      "Epoch 85/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0629 - mse: 0.0067 - val_loss: 0.0077 - val_mae: 0.0672 - val_mse: 0.0077\n",
      "Epoch 86/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0618 - mse: 0.0065 - val_loss: 0.0060 - val_mae: 0.0572 - val_mse: 0.0060\n",
      "Epoch 87/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0617 - mse: 0.0066 - val_loss: 0.0066 - val_mae: 0.0600 - val_mse: 0.0066\n",
      "Epoch 88/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0609 - mse: 0.0064 - val_loss: 0.0087 - val_mae: 0.0763 - val_mse: 0.0087\n",
      "Epoch 89/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0614 - mse: 0.0065 - val_loss: 0.0067 - val_mae: 0.0606 - val_mse: 0.0067\n",
      "Epoch 90/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0603 - mse: 0.0063 - val_loss: 0.0062 - val_mae: 0.0591 - val_mse: 0.0062\n",
      "Epoch 91/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0613 - mse: 0.0065 - val_loss: 0.0064 - val_mae: 0.0587 - val_mse: 0.0064\n",
      "Epoch 92/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0587 - mse: 0.0060 - val_loss: 0.0059 - val_mae: 0.0571 - val_mse: 0.0059\n",
      "Epoch 93/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0628 - mse: 0.0067 - val_loss: 0.0060 - val_mae: 0.0586 - val_mse: 0.0060\n",
      "Epoch 94/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0595 - mse: 0.0063 - val_loss: 0.0062 - val_mae: 0.0585 - val_mse: 0.0062\n",
      "Epoch 95/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0613 - mse: 0.0064 - val_loss: 0.0068 - val_mae: 0.0650 - val_mse: 0.0068\n",
      "Epoch 96/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0602 - mse: 0.0064 - val_loss: 0.0062 - val_mae: 0.0600 - val_mse: 0.0062\n",
      "Epoch 97/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0615 - mse: 0.0066 - val_loss: 0.0059 - val_mae: 0.0559 - val_mse: 0.0059\n",
      "Epoch 98/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0586 - mse: 0.0059 - val_loss: 0.0090 - val_mae: 0.0760 - val_mse: 0.0090\n",
      "Epoch 99/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0611 - mse: 0.0064 - val_loss: 0.0062 - val_mae: 0.0602 - val_mse: 0.0062\n",
      "Epoch 100/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0622 - mse: 0.0067 - val_loss: 0.0059 - val_mae: 0.0567 - val_mse: 0.0059\n",
      "Epoch 101/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0615 - mse: 0.0065 - val_loss: 0.0058 - val_mae: 0.0569 - val_mse: 0.0058\n",
      "Epoch 102/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0602 - mse: 0.0063 - val_loss: 0.0058 - val_mae: 0.0564 - val_mse: 0.0058\n",
      "Epoch 103/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0600 - mse: 0.0063 - val_loss: 0.0060 - val_mae: 0.0577 - val_mse: 0.0060\n",
      "Epoch 104/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0600 - mse: 0.0063 - val_loss: 0.0092 - val_mae: 0.0772 - val_mse: 0.0092\n",
      "Epoch 105/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0595 - mse: 0.0062 - val_loss: 0.0081 - val_mae: 0.0724 - val_mse: 0.0081\n",
      "Epoch 106/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0609 - mse: 0.0064 - val_loss: 0.0070 - val_mae: 0.0631 - val_mse: 0.0070\n",
      "Epoch 107/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0593 - mse: 0.0061 - val_loss: 0.0070 - val_mae: 0.0662 - val_mse: 0.0070\n",
      "Epoch 108/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0604 - mse: 0.0063 - val_loss: 0.0067 - val_mae: 0.0641 - val_mse: 0.0067\n",
      "Epoch 109/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0609 - mse: 0.0064 - val_loss: 0.0066 - val_mae: 0.0616 - val_mse: 0.0066\n",
      "Epoch 110/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0599 - mse: 0.0062 - val_loss: 0.0059 - val_mae: 0.0567 - val_mse: 0.0059\n",
      "Epoch 111/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0608 - mse: 0.0065 - val_loss: 0.0059 - val_mae: 0.0566 - val_mse: 0.0059\n",
      "Epoch 112/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0593 - mse: 0.0062 - val_loss: 0.0057 - val_mae: 0.0561 - val_mse: 0.0057\n",
      "Epoch 113/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0587 - mse: 0.0060 - val_loss: 0.0063 - val_mae: 0.0610 - val_mse: 0.0063\n",
      "Epoch 114/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0615 - mse: 0.0065 - val_loss: 0.0064 - val_mae: 0.0596 - val_mse: 0.0064\n",
      "Epoch 115/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0622 - mse: 0.0066 - val_loss: 0.0060 - val_mae: 0.0586 - val_mse: 0.0060\n",
      "Epoch 116/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0596 - mse: 0.0062 - val_loss: 0.0080 - val_mae: 0.0721 - val_mse: 0.0080\n",
      "Epoch 117/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0607 - mse: 0.0063 - val_loss: 0.0066 - val_mae: 0.0634 - val_mse: 0.0066\n",
      "Epoch 118/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0066 - mae: 0.0613 - mse: 0.0066 - val_loss: 0.0058 - val_mae: 0.0555 - val_mse: 0.0058\n",
      "Epoch 119/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0601 - mse: 0.0062 - val_loss: 0.0062 - val_mae: 0.0586 - val_mse: 0.0062\n",
      "Epoch 120/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0623 - mse: 0.0067 - val_loss: 0.0058 - val_mae: 0.0560 - val_mse: 0.0058\n",
      "Epoch 121/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0587 - mse: 0.0060 - val_loss: 0.0073 - val_mae: 0.0683 - val_mse: 0.0073\n",
      "Epoch 122/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0591 - mse: 0.0060 - val_loss: 0.0057 - val_mae: 0.0559 - val_mse: 0.0057\n",
      "Epoch 123/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0605 - mse: 0.0064 - val_loss: 0.0063 - val_mae: 0.0614 - val_mse: 0.0063\n",
      "Epoch 124/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0581 - mse: 0.0059 - val_loss: 0.0058 - val_mae: 0.0565 - val_mse: 0.0058\n",
      "Epoch 125/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0601 - mse: 0.0062 - val_loss: 0.0058 - val_mae: 0.0568 - val_mse: 0.0058\n",
      "Epoch 126/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0591 - mse: 0.0061 - val_loss: 0.0064 - val_mae: 0.0595 - val_mse: 0.0064\n",
      "Epoch 127/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0581 - mse: 0.0060 - val_loss: 0.0059 - val_mae: 0.0570 - val_mse: 0.0059\n",
      "Epoch 128/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0587 - mse: 0.0060 - val_loss: 0.0058 - val_mae: 0.0572 - val_mse: 0.0058\n",
      "Epoch 129/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0596 - mse: 0.0062 - val_loss: 0.0062 - val_mae: 0.0607 - val_mse: 0.0062\n",
      "Epoch 130/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0593 - mse: 0.0061 - val_loss: 0.0065 - val_mae: 0.0604 - val_mse: 0.0065\n",
      "Epoch 131/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0596 - mse: 0.0061 - val_loss: 0.0067 - val_mae: 0.0642 - val_mse: 0.0067\n",
      "Epoch 132/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0595 - mse: 0.0061 - val_loss: 0.0063 - val_mae: 0.0611 - val_mse: 0.0063\n",
      "Epoch 133/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0598 - mse: 0.0061 - val_loss: 0.0062 - val_mae: 0.0597 - val_mse: 0.0062\n",
      "Epoch 134/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0597 - mse: 0.0062 - val_loss: 0.0063 - val_mae: 0.0593 - val_mse: 0.0063\n",
      "Epoch 135/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0611 - mse: 0.0064 - val_loss: 0.0065 - val_mae: 0.0599 - val_mse: 0.0065\n",
      "Epoch 136/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0597 - mse: 0.0062 - val_loss: 0.0057 - val_mae: 0.0556 - val_mse: 0.0057\n",
      "Epoch 137/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0586 - mse: 0.0060 - val_loss: 0.0057 - val_mae: 0.0558 - val_mse: 0.0057\n",
      "Epoch 138/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0067 - mae: 0.0620 - mse: 0.0067 - val_loss: 0.0061 - val_mae: 0.0578 - val_mse: 0.0061\n",
      "Epoch 139/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0597 - mse: 0.0061 - val_loss: 0.0062 - val_mae: 0.0586 - val_mse: 0.0062\n",
      "Epoch 140/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0597 - mse: 0.0061 - val_loss: 0.0061 - val_mae: 0.0600 - val_mse: 0.0061\n",
      "Epoch 141/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0594 - mse: 0.0062 - val_loss: 0.0058 - val_mae: 0.0562 - val_mse: 0.0058\n",
      "Epoch 142/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0615 - mse: 0.0065 - val_loss: 0.0062 - val_mae: 0.0585 - val_mse: 0.0062\n",
      "Epoch 143/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0610 - mse: 0.0064 - val_loss: 0.0057 - val_mae: 0.0556 - val_mse: 0.0057\n",
      "Epoch 144/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0578 - mse: 0.0060 - val_loss: 0.0058 - val_mae: 0.0558 - val_mse: 0.0058\n",
      "Epoch 145/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0619 - mse: 0.0065 - val_loss: 0.0061 - val_mae: 0.0596 - val_mse: 0.0061\n",
      "Epoch 146/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0594 - mse: 0.0062 - val_loss: 0.0056 - val_mae: 0.0552 - val_mse: 0.0056\n",
      "Epoch 147/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0595 - mse: 0.0062 - val_loss: 0.0059 - val_mae: 0.0565 - val_mse: 0.0059\n",
      "Epoch 148/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0588 - mse: 0.0061 - val_loss: 0.0060 - val_mae: 0.0569 - val_mse: 0.0060\n",
      "Epoch 149/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0605 - mse: 0.0063 - val_loss: 0.0061 - val_mae: 0.0593 - val_mse: 0.0061\n",
      "Epoch 150/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0603 - mse: 0.0062 - val_loss: 0.0062 - val_mae: 0.0585 - val_mse: 0.0062\n",
      "Epoch 151/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0586 - mse: 0.0060 - val_loss: 0.0059 - val_mae: 0.0577 - val_mse: 0.0059\n",
      "Epoch 152/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0592 - mse: 0.0061 - val_loss: 0.0057 - val_mae: 0.0555 - val_mse: 0.0057\n",
      "Epoch 153/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0592 - mse: 0.0061 - val_loss: 0.0058 - val_mae: 0.0567 - val_mse: 0.0058\n",
      "Epoch 154/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0061 - mae: 0.0584 - mse: 0.0061 - val_loss: 0.0061 - val_mae: 0.0579 - val_mse: 0.0061\n",
      "Epoch 155/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0581 - mse: 0.0060 - val_loss: 0.0060 - val_mae: 0.0595 - val_mse: 0.0060\n",
      "Epoch 156/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0601 - mse: 0.0062 - val_loss: 0.0074 - val_mae: 0.0659 - val_mse: 0.0074\n",
      "Epoch 157/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0605 - mse: 0.0063 - val_loss: 0.0060 - val_mae: 0.0575 - val_mse: 0.0060\n",
      "Epoch 158/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0598 - mse: 0.0063 - val_loss: 0.0063 - val_mae: 0.0593 - val_mse: 0.0063\n",
      "Epoch 159/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0584 - mse: 0.0059 - val_loss: 0.0083 - val_mae: 0.0742 - val_mse: 0.0083\n",
      "Epoch 160/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0064 - mae: 0.0613 - mse: 0.0064 - val_loss: 0.0070 - val_mae: 0.0656 - val_mse: 0.0070\n",
      "Epoch 161/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0600 - mse: 0.0062 - val_loss: 0.0060 - val_mae: 0.0573 - val_mse: 0.0060\n",
      "Epoch 00161: early stopping\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  0.016675356100491697    std:  0.0755457420933079    MAE:  0.057586978163425624     R2:  0.9473267048345263\n",
      "Test. mean:  0.01395966077270808    std:  0.07631070034384568    MAE:  0.057268279776548824     R2:  0.948104261023471\n",
      "\u001b[1m\u001b[91mFold 1\u001b[0m\n",
      "Epoch 1/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.1440 - mae: 0.2717 - mse: 0.1440 - val_loss: 0.0575 - val_mae: 0.2029 - val_mse: 0.0575\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0557 - mae: 0.2016 - mse: 0.0557 - val_loss: 0.0573 - val_mae: 0.2024 - val_mse: 0.0573\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0558 - mae: 0.2012 - mse: 0.0558 - val_loss: 0.0573 - val_mae: 0.1984 - val_mse: 0.0573\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0570 - mae: 0.2023 - mse: 0.0570 - val_loss: 0.0571 - val_mae: 0.1989 - val_mse: 0.0571\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2016 - mse: 0.0563 - val_loss: 0.0570 - val_mae: 0.2007 - val_mse: 0.0570\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0566 - mae: 0.2026 - mse: 0.0566 - val_loss: 0.0573 - val_mae: 0.2021 - val_mse: 0.0573\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2006 - mse: 0.0561 - val_loss: 0.0570 - val_mae: 0.1992 - val_mse: 0.0570\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0558 - mae: 0.2005 - mse: 0.0558 - val_loss: 0.0570 - val_mae: 0.2004 - val_mse: 0.0570\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0552 - mae: 0.1993 - mse: 0.0552 - val_loss: 0.0570 - val_mae: 0.2006 - val_mse: 0.0570\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0574 - mae: 0.2045 - mse: 0.0574 - val_loss: 0.0571 - val_mae: 0.1987 - val_mse: 0.0571\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0571 - mae: 0.2029 - mse: 0.0571 - val_loss: 0.0569 - val_mae: 0.2000 - val_mse: 0.0569\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0569 - mae: 0.2029 - mse: 0.0569 - val_loss: 0.0570 - val_mae: 0.2010 - val_mse: 0.0570\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0570 - mae: 0.2037 - mse: 0.0570 - val_loss: 0.0570 - val_mae: 0.1991 - val_mse: 0.0570\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0557 - mae: 0.2006 - mse: 0.0557 - val_loss: 0.0569 - val_mae: 0.2007 - val_mse: 0.0569\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0556 - mae: 0.2004 - mse: 0.0556 - val_loss: 0.0570 - val_mae: 0.2013 - val_mse: 0.0570\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0567 - mae: 0.2027 - mse: 0.0567 - val_loss: 0.0569 - val_mae: 0.2011 - val_mse: 0.0569\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0567 - mae: 0.2029 - mse: 0.0567 - val_loss: 0.0582 - val_mae: 0.2057 - val_mse: 0.0582\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0548 - mae: 0.1982 - mse: 0.0548 - val_loss: 0.0568 - val_mae: 0.2002 - val_mse: 0.0568\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2010 - mse: 0.0563 - val_loss: 0.0570 - val_mae: 0.2018 - val_mse: 0.0570\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0565 - mae: 0.2021 - mse: 0.0565 - val_loss: 0.0573 - val_mae: 0.1977 - val_mse: 0.0573\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0559 - mae: 0.2001 - mse: 0.0559 - val_loss: 0.0567 - val_mae: 0.2002 - val_mse: 0.0567\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2018 - mse: 0.0563 - val_loss: 0.0566 - val_mae: 0.2001 - val_mse: 0.0566\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0574 - val_mae: 0.2042 - val_mse: 0.0574\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0569 - mae: 0.2034 - mse: 0.0569 - val_loss: 0.0567 - val_mae: 0.2019 - val_mse: 0.0567\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0567 - mae: 0.2025 - mse: 0.0567 - val_loss: 0.0571 - val_mae: 0.2035 - val_mse: 0.0571\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2025 - mse: 0.0563 - val_loss: 0.0572 - val_mae: 0.1973 - val_mse: 0.0572\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0570 - mae: 0.2027 - mse: 0.0570 - val_loss: 0.0563 - val_mae: 0.2009 - val_mse: 0.0563\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0564 - mae: 0.2022 - mse: 0.0564 - val_loss: 0.0562 - val_mae: 0.2008 - val_mse: 0.0562\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0552 - mae: 0.1990 - mse: 0.0552 - val_loss: 0.0561 - val_mae: 0.2003 - val_mse: 0.0561\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0573 - mae: 0.2040 - mse: 0.0573 - val_loss: 0.0563 - val_mae: 0.2020 - val_mse: 0.0563\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0557 - mae: 0.2005 - mse: 0.0557 - val_loss: 0.0561 - val_mae: 0.2015 - val_mse: 0.0561\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0556 - mae: 0.2002 - mse: 0.0556 - val_loss: 0.0566 - val_mae: 0.2035 - val_mse: 0.0566\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0569 - mae: 0.2038 - mse: 0.0569 - val_loss: 0.0558 - val_mae: 0.1994 - val_mse: 0.0558\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0553 - mae: 0.1994 - mse: 0.0553 - val_loss: 0.0563 - val_mae: 0.2028 - val_mse: 0.0563\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0565 - mae: 0.2027 - mse: 0.0565 - val_loss: 0.0558 - val_mae: 0.1983 - val_mse: 0.0558\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0565 - mae: 0.2021 - mse: 0.0565 - val_loss: 0.0556 - val_mae: 0.2002 - val_mse: 0.0556\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0560 - mae: 0.2012 - mse: 0.0560 - val_loss: 0.0556 - val_mae: 0.2000 - val_mse: 0.0556\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0558 - mae: 0.2002 - mse: 0.0558 - val_loss: 0.0558 - val_mae: 0.2015 - val_mse: 0.0558\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0556 - mae: 0.2002 - mse: 0.0556 - val_loss: 0.0557 - val_mae: 0.2012 - val_mse: 0.0557\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0563 - mae: 0.2017 - mse: 0.0563 - val_loss: 0.0561 - val_mae: 0.2031 - val_mse: 0.0561\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0560 - mae: 0.2007 - mse: 0.0560 - val_loss: 0.0557 - val_mae: 0.2015 - val_mse: 0.0557\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0557 - mae: 0.2005 - mse: 0.0557 - val_loss: 0.0554 - val_mae: 0.2001 - val_mse: 0.0554\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0571 - mae: 0.2029 - mse: 0.0571 - val_loss: 0.0554 - val_mae: 0.2003 - val_mse: 0.0554\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0561 - mae: 0.2009 - mse: 0.0561 - val_loss: 0.0563 - val_mae: 0.2040 - val_mse: 0.0563\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0562 - mae: 0.2021 - mse: 0.0562 - val_loss: 0.0553 - val_mae: 0.1992 - val_mse: 0.0553\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0557 - mae: 0.2003 - mse: 0.0557 - val_loss: 0.0553 - val_mae: 0.1997 - val_mse: 0.0553\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0566 - mae: 0.2026 - mse: 0.0566 - val_loss: 0.0555 - val_mae: 0.1980 - val_mse: 0.0555\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0555 - mae: 0.1994 - mse: 0.0555 - val_loss: 0.0555 - val_mae: 0.2013 - val_mse: 0.0555\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0566 - mae: 0.2025 - mse: 0.0566 - val_loss: 0.0561 - val_mae: 0.2033 - val_mse: 0.0561\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0558 - mae: 0.2008 - mse: 0.0558 - val_loss: 0.0553 - val_mae: 0.1999 - val_mse: 0.0553\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0559 - mae: 0.2005 - mse: 0.0559 - val_loss: 0.0553 - val_mae: 0.1991 - val_mse: 0.0553\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0566 - mae: 0.2024 - mse: 0.0566 - val_loss: 0.0553 - val_mae: 0.2000 - val_mse: 0.0553\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0552 - mae: 0.1993 - mse: 0.0552 - val_loss: 0.0553 - val_mae: 0.1994 - val_mse: 0.0553\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0565 - mae: 0.2020 - mse: 0.0565 - val_loss: 0.0553 - val_mae: 0.1994 - val_mse: 0.0553\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.2003 - mse: 0.0555 - val_loss: 0.0553 - val_mae: 0.2003 - val_mse: 0.0553\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2011 - mse: 0.0560 - val_loss: 0.0553 - val_mae: 0.1987 - val_mse: 0.0553\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0556 - mae: 0.2007 - mse: 0.0556 - val_loss: 0.0557 - val_mae: 0.2020 - val_mse: 0.0557\n",
      "Epoch 58/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.2003 - mse: 0.0555 - val_loss: 0.0556 - val_mae: 0.2018 - val_mse: 0.0556\n",
      "Epoch 59/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2017 - mse: 0.0563 - val_loss: 0.0553 - val_mae: 0.2003 - val_mse: 0.0553\n",
      "Epoch 60/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2016 - mse: 0.0562 - val_loss: 0.0553 - val_mae: 0.1989 - val_mse: 0.0553\n",
      "Epoch 61/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0565 - mae: 0.2019 - mse: 0.0565 - val_loss: 0.0558 - val_mae: 0.2025 - val_mse: 0.0558\n",
      "Epoch 62/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0566 - mae: 0.2030 - mse: 0.0566 - val_loss: 0.0553 - val_mae: 0.1984 - val_mse: 0.0553\n",
      "Epoch 63/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0554 - mae: 0.1995 - mse: 0.0554 - val_loss: 0.0555 - val_mae: 0.2011 - val_mse: 0.0555\n",
      "Epoch 64/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2015 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1991 - val_mse: 0.0553\n",
      "Epoch 65/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0555 - mae: 0.2005 - mse: 0.0555 - val_loss: 0.0553 - val_mae: 0.1994 - val_mse: 0.0553\n",
      "Epoch 66/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2023 - mse: 0.0563 - val_loss: 0.0554 - val_mae: 0.1980 - val_mse: 0.0554\n",
      "Epoch 67/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2020 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1998 - val_mse: 0.0553\n",
      "Epoch 68/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2015 - mse: 0.0563 - val_loss: 0.0554 - val_mae: 0.1979 - val_mse: 0.0554\n",
      "Epoch 69/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0552 - mae: 0.1991 - mse: 0.0552 - val_loss: 0.0553 - val_mae: 0.1989 - val_mse: 0.0553\n",
      "Epoch 70/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0576 - mae: 0.2043 - mse: 0.0576 - val_loss: 0.0557 - val_mae: 0.2021 - val_mse: 0.0557\n",
      "Epoch 71/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0570 - mae: 0.2033 - mse: 0.0570 - val_loss: 0.0553 - val_mae: 0.1991 - val_mse: 0.0553\n",
      "Epoch 72/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0568 - mae: 0.2030 - mse: 0.0568 - val_loss: 0.0554 - val_mae: 0.2009 - val_mse: 0.0554\n",
      "Epoch 73/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0559 - mae: 0.2005 - mse: 0.0559 - val_loss: 0.0554 - val_mae: 0.2007 - val_mse: 0.0554\n",
      "Epoch 74/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0565 - mae: 0.2008 - mse: 0.0565 - val_loss: 0.0553 - val_mae: 0.1992 - val_mse: 0.0553\n",
      "Epoch 75/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0569 - mae: 0.2027 - mse: 0.0569 - val_loss: 0.0555 - val_mae: 0.1977 - val_mse: 0.0555\n",
      "Epoch 76/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0564 - mae: 0.2017 - mse: 0.0564 - val_loss: 0.0558 - val_mae: 0.2025 - val_mse: 0.0558\n",
      "Epoch 77/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2016 - mse: 0.0560 - val_loss: 0.0553 - val_mae: 0.1999 - val_mse: 0.0553\n",
      "Epoch 78/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0558 - mae: 0.2006 - mse: 0.0558 - val_loss: 0.0558 - val_mae: 0.2026 - val_mse: 0.0558\n",
      "Epoch 79/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0557 - mae: 0.2012 - mse: 0.0557 - val_loss: 0.0554 - val_mae: 0.2005 - val_mse: 0.0554\n",
      "Epoch 80/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2016 - mse: 0.0560 - val_loss: 0.0556 - val_mae: 0.2019 - val_mse: 0.0556\n",
      "Epoch 81/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0575 - mae: 0.2039 - mse: 0.0575 - val_loss: 0.0554 - val_mae: 0.2005 - val_mse: 0.0554\n",
      "Epoch 82/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2011 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1997 - val_mse: 0.0553\n",
      "Epoch 83/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2019 - mse: 0.0562 - val_loss: 0.0556 - val_mae: 0.2015 - val_mse: 0.0556\n",
      "Epoch 84/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0562 - mae: 0.2016 - mse: 0.0562 - val_loss: 0.0554 - val_mae: 0.1981 - val_mse: 0.0554\n",
      "Epoch 85/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0564 - mae: 0.2014 - mse: 0.0564 - val_loss: 0.0553 - val_mae: 0.2003 - val_mse: 0.0553\n",
      "Epoch 86/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0563 - mae: 0.2021 - mse: 0.0563 - val_loss: 0.0554 - val_mae: 0.2005 - val_mse: 0.0554\n",
      "Epoch 87/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0549 - mae: 0.1980 - mse: 0.0549 - val_loss: 0.0555 - val_mae: 0.2010 - val_mse: 0.0555\n",
      "Epoch 88/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2014 - mse: 0.0560 - val_loss: 0.0554 - val_mae: 0.2008 - val_mse: 0.0554\n",
      "Epoch 89/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0564 - mae: 0.2014 - mse: 0.0564 - val_loss: 0.0554 - val_mae: 0.2007 - val_mse: 0.0554\n",
      "Epoch 00089: early stopping\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  0.004483152922256508    std:  0.2364105518158513    MAE:  0.20146854698702976     R2:  nan\n",
      "Test. mean:  0.011297286835054333    std:  0.23508004592796938    MAE:  0.20067722840124855     R2:  -0.051985668516144924\n",
      "\u001b[1m\u001b[91mFold 2\u001b[0m\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/src/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2691: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/usr/local/src/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2692: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 2s 3ms/step - loss: 0.0860 - mae: 0.2409 - mse: 0.0860 - val_loss: 0.0542 - val_mae: 0.1970 - val_mse: 0.0542\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2021 - mse: 0.0562 - val_loss: 0.0543 - val_mae: 0.1983 - val_mse: 0.0543\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0559 - mae: 0.2004 - mse: 0.0559 - val_loss: 0.0543 - val_mae: 0.1967 - val_mse: 0.0543\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0549 - mae: 0.1987 - mse: 0.0549 - val_loss: 0.0542 - val_mae: 0.1974 - val_mse: 0.0542\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0551 - mae: 0.1992 - mse: 0.0551 - val_loss: 0.0542 - val_mae: 0.1971 - val_mse: 0.0542\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0558 - mae: 0.2005 - mse: 0.0558 - val_loss: 0.0554 - val_mae: 0.1954 - val_mse: 0.0554\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0575 - mae: 0.2031 - mse: 0.0575 - val_loss: 0.0542 - val_mae: 0.1969 - val_mse: 0.0542\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0544 - mae: 0.1968 - mse: 0.0544 - val_loss: 0.0139 - val_mae: 0.0936 - val_mse: 0.0139\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0142 - mae: 0.0926 - mse: 0.0142 - val_loss: 0.0103 - val_mae: 0.0776 - val_mse: 0.0103\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0112 - mae: 0.0816 - mse: 0.0112 - val_loss: 0.0093 - val_mae: 0.0735 - val_mse: 0.0093\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0103 - mae: 0.0785 - mse: 0.0103 - val_loss: 0.0145 - val_mae: 0.0979 - val_mse: 0.0145\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0103 - mae: 0.0781 - mse: 0.0103 - val_loss: 0.0090 - val_mae: 0.0719 - val_mse: 0.0090\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0093 - mae: 0.0737 - mse: 0.0093 - val_loss: 0.0087 - val_mae: 0.0719 - val_mse: 0.0087\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0095 - mae: 0.0748 - mse: 0.0095 - val_loss: 0.0086 - val_mae: 0.0697 - val_mse: 0.0086\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0096 - mae: 0.0747 - mse: 0.0096 - val_loss: 0.0095 - val_mae: 0.0749 - val_mse: 0.0095\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0091 - mae: 0.0731 - mse: 0.0091 - val_loss: 0.0084 - val_mae: 0.0690 - val_mse: 0.0084\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0092 - mae: 0.0731 - mse: 0.0092 - val_loss: 0.0082 - val_mae: 0.0686 - val_mse: 0.0082\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0095 - mae: 0.0750 - mse: 0.0095 - val_loss: 0.0082 - val_mae: 0.0679 - val_mse: 0.0082\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0093 - mae: 0.0743 - mse: 0.0093 - val_loss: 0.0093 - val_mae: 0.0768 - val_mse: 0.0093\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0092 - mae: 0.0733 - mse: 0.0092 - val_loss: 0.0083 - val_mae: 0.0679 - val_mse: 0.0083\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0723 - mse: 0.0090 - val_loss: 0.0080 - val_mae: 0.0676 - val_mse: 0.0080\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0093 - mae: 0.0736 - mse: 0.0093 - val_loss: 0.0079 - val_mae: 0.0675 - val_mse: 0.0079\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0720 - mse: 0.0090 - val_loss: 0.0098 - val_mae: 0.0802 - val_mse: 0.0098\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0725 - mse: 0.0088 - val_loss: 0.0098 - val_mae: 0.0804 - val_mse: 0.0098\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0091 - mae: 0.0725 - mse: 0.0091 - val_loss: 0.0079 - val_mae: 0.0675 - val_mse: 0.0079\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0714 - mse: 0.0087 - val_loss: 0.0101 - val_mae: 0.0767 - val_mse: 0.0101\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0724 - mse: 0.0090 - val_loss: 0.0078 - val_mae: 0.0672 - val_mse: 0.0078\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0712 - mse: 0.0087 - val_loss: 0.0080 - val_mae: 0.0661 - val_mse: 0.0080\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0091 - mae: 0.0729 - mse: 0.0091 - val_loss: 0.0080 - val_mae: 0.0664 - val_mse: 0.0080\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0711 - mse: 0.0087 - val_loss: 0.0084 - val_mae: 0.0680 - val_mse: 0.0084\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0708 - mse: 0.0087 - val_loss: 0.0079 - val_mae: 0.0662 - val_mse: 0.0079\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0713 - mse: 0.0087 - val_loss: 0.0082 - val_mae: 0.0711 - val_mse: 0.0082\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0711 - mse: 0.0088 - val_loss: 0.0082 - val_mae: 0.0715 - val_mse: 0.0082\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0727 - mse: 0.0090 - val_loss: 0.0081 - val_mae: 0.0667 - val_mse: 0.0081\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0716 - mse: 0.0087 - val_loss: 0.0079 - val_mae: 0.0659 - val_mse: 0.0079\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0700 - mse: 0.0084 - val_loss: 0.0090 - val_mae: 0.0716 - val_mse: 0.0090\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0722 - mse: 0.0088 - val_loss: 0.0082 - val_mae: 0.0710 - val_mse: 0.0082\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0723 - mse: 0.0089 - val_loss: 0.0078 - val_mae: 0.0660 - val_mse: 0.0078\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0714 - mse: 0.0086 - val_loss: 0.0077 - val_mae: 0.0666 - val_mse: 0.0077\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0707 - mse: 0.0084 - val_loss: 0.0087 - val_mae: 0.0695 - val_mse: 0.0087\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0703 - mse: 0.0085 - val_loss: 0.0103 - val_mae: 0.0769 - val_mse: 0.0103\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0720 - mse: 0.0088 - val_loss: 0.0077 - val_mae: 0.0670 - val_mse: 0.0077\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0714 - mse: 0.0086 - val_loss: 0.0082 - val_mae: 0.0716 - val_mse: 0.0082\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0706 - mse: 0.0084 - val_loss: 0.0079 - val_mae: 0.0686 - val_mse: 0.0079\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0711 - mse: 0.0086 - val_loss: 0.0079 - val_mae: 0.0694 - val_mse: 0.0079\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0725 - mse: 0.0089 - val_loss: 0.0083 - val_mae: 0.0724 - val_mse: 0.0083\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0726 - mse: 0.0089 - val_loss: 0.0079 - val_mae: 0.0655 - val_mse: 0.0079\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0718 - mse: 0.0089 - val_loss: 0.0083 - val_mae: 0.0677 - val_mse: 0.0083\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0713 - mse: 0.0087 - val_loss: 0.0077 - val_mae: 0.0673 - val_mse: 0.0077\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0697 - mse: 0.0083 - val_loss: 0.0096 - val_mae: 0.0743 - val_mse: 0.0096\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0726 - mse: 0.0089 - val_loss: 0.0079 - val_mae: 0.0656 - val_mse: 0.0079\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0702 - mse: 0.0084 - val_loss: 0.0082 - val_mae: 0.0714 - val_mse: 0.0082\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0729 - mse: 0.0090 - val_loss: 0.0078 - val_mae: 0.0680 - val_mse: 0.0078\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0705 - mse: 0.0084 - val_loss: 0.0081 - val_mae: 0.0671 - val_mse: 0.0081\n",
      "Epoch 00054: early stopping\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  0.016040046002905695    std:  0.09103943272279501    MAE:  0.06974362199471655     R2:  0.9248328762920426\n",
      "Test. mean:  0.01617375654174108    std:  0.08852661289240965    MAE:  0.0670505969011751     R2:  0.9260879281027995\n",
      "\u001b[1m\u001b[91mFold 3\u001b[0m\n",
      "Epoch 1/200\n",
      "324/324 [==============================] - 2s 3ms/step - loss: 1.7672 - mae: 1.2196 - mse: 1.7672 - val_loss: 0.1738 - val_mae: 0.3369 - val_mse: 0.1738\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.1220 - mae: 0.2735 - mse: 0.1220 - val_loss: 0.0592 - val_mae: 0.2066 - val_mse: 0.0592\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2012 - mse: 0.0562 - val_loss: 0.0576 - val_mae: 0.2048 - val_mse: 0.0576\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0557 - mae: 0.1993 - mse: 0.0557 - val_loss: 0.0576 - val_mae: 0.2044 - val_mse: 0.0576\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0548 - mae: 0.1985 - mse: 0.0548 - val_loss: 0.0576 - val_mae: 0.2047 - val_mse: 0.0576\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0566 - mae: 0.2018 - mse: 0.0566 - val_loss: 0.0578 - val_mae: 0.2040 - val_mse: 0.0578\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0554 - mae: 0.1991 - mse: 0.0554 - val_loss: 0.0576 - val_mae: 0.2053 - val_mse: 0.0576\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0550 - mae: 0.1994 - mse: 0.0550 - val_loss: 0.0576 - val_mae: 0.2045 - val_mse: 0.0576\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0570 - mae: 0.2030 - mse: 0.0570 - val_loss: 0.0576 - val_mae: 0.2049 - val_mse: 0.0576\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0549 - mae: 0.1986 - mse: 0.0549 - val_loss: 0.0576 - val_mae: 0.2050 - val_mse: 0.0576\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0551 - mae: 0.1989 - mse: 0.0551 - val_loss: 0.0576 - val_mae: 0.2048 - val_mse: 0.0576\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0558 - mae: 0.2008 - mse: 0.0558 - val_loss: 0.0576 - val_mae: 0.2043 - val_mse: 0.0576\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0556 - mae: 0.1993 - mse: 0.0556 - val_loss: 0.0576 - val_mae: 0.2045 - val_mse: 0.0576\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2016 - mse: 0.0563 - val_loss: 0.0576 - val_mae: 0.2053 - val_mse: 0.0576\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0538 - mae: 0.1966 - mse: 0.0538 - val_loss: 0.0576 - val_mae: 0.2044 - val_mse: 0.0576\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0551 - mae: 0.1991 - mse: 0.0551 - val_loss: 0.0577 - val_mae: 0.2058 - val_mse: 0.0577\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0566 - mae: 0.2026 - mse: 0.0566 - val_loss: 0.0578 - val_mae: 0.2039 - val_mse: 0.0578\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0552 - mae: 0.1989 - mse: 0.0552 - val_loss: 0.0581 - val_mae: 0.2074 - val_mse: 0.0581\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0550 - mae: 0.1993 - mse: 0.0550 - val_loss: 0.0576 - val_mae: 0.2044 - val_mse: 0.0576\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0558 - mae: 0.1995 - mse: 0.0558 - val_loss: 0.0576 - val_mae: 0.2049 - val_mse: 0.0576\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0544 - mae: 0.1976 - mse: 0.0544 - val_loss: 0.0577 - val_mae: 0.2041 - val_mse: 0.0577\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0569 - mae: 0.2027 - mse: 0.0569 - val_loss: 0.0577 - val_mae: 0.2059 - val_mse: 0.0577\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0551 - mae: 0.1991 - mse: 0.0551 - val_loss: 0.0578 - val_mae: 0.2038 - val_mse: 0.0578\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0548 - mae: 0.1985 - mse: 0.0548 - val_loss: 0.0579 - val_mae: 0.2037 - val_mse: 0.0579\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0557 - mae: 0.1998 - mse: 0.0557 - val_loss: 0.0578 - val_mae: 0.2039 - val_mse: 0.0578\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2002 - mse: 0.0561 - val_loss: 0.0578 - val_mae: 0.2039 - val_mse: 0.0578\n",
      "Epoch 00026: early stopping\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  -0.011096684348137686    std:  0.23518807314947263    MAE:  0.19853828918217054     R2:  -0.025754310202623212\n",
      "Test. mean:  -0.012473940175364536    std:  0.2400091645280366    MAE:  0.20393694200663814     R2:  nan\n",
      "\u001b[1m\u001b[91mFold 4\u001b[0m\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/src/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2691: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/usr/local/src/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2692: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 2s 3ms/step - loss: 0.2109 - mae: 0.3497 - mse: 0.2109 - val_loss: 0.0242 - val_mae: 0.1282 - val_mse: 0.0242\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0215 - mae: 0.1213 - mse: 0.0215 - val_loss: 0.0166 - val_mae: 0.1004 - val_mse: 0.0166\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0178 - mae: 0.0997 - mse: 0.0178 - val_loss: 0.0148 - val_mae: 0.0932 - val_mse: 0.0148\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0163 - mae: 0.0965 - mse: 0.0163 - val_loss: 0.0163 - val_mae: 0.0947 - val_mse: 0.0163\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0153 - mae: 0.0945 - mse: 0.0153 - val_loss: 0.0142 - val_mae: 0.0902 - val_mse: 0.0142\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0147 - mae: 0.0918 - mse: 0.0147 - val_loss: 0.0141 - val_mae: 0.0897 - val_mse: 0.0141\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0136 - mae: 0.0886 - mse: 0.0136 - val_loss: 0.0135 - val_mae: 0.0879 - val_mse: 0.0135\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0142 - mae: 0.0903 - mse: 0.0142 - val_loss: 0.0131 - val_mae: 0.0863 - val_mse: 0.0131\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0125 - mae: 0.0849 - mse: 0.0125 - val_loss: 0.0125 - val_mae: 0.0846 - val_mse: 0.0125\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0115 - mae: 0.0806 - mse: 0.0115 - val_loss: 0.0112 - val_mae: 0.0764 - val_mse: 0.0112\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0100 - mae: 0.0741 - mse: 0.0100 - val_loss: 0.0096 - val_mae: 0.0712 - val_mse: 0.0096\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0093 - mae: 0.0706 - mse: 0.0093 - val_loss: 0.0085 - val_mae: 0.0663 - val_mse: 0.0085\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0674 - mse: 0.0087 - val_loss: 0.0081 - val_mae: 0.0641 - val_mse: 0.0081\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0651 - mse: 0.0082 - val_loss: 0.0079 - val_mae: 0.0627 - val_mse: 0.0079\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0081 - mae: 0.0642 - mse: 0.0081 - val_loss: 0.0078 - val_mae: 0.0642 - val_mse: 0.0078\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0074 - mae: 0.0626 - mse: 0.0074 - val_loss: 0.0078 - val_mae: 0.0653 - val_mse: 0.0078\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0071 - mae: 0.0614 - mse: 0.0071 - val_loss: 0.0069 - val_mae: 0.0597 - val_mse: 0.0069\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0604 - mse: 0.0068 - val_loss: 0.0062 - val_mae: 0.0554 - val_mse: 0.0062\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0584 - mse: 0.0065 - val_loss: 0.0059 - val_mae: 0.0536 - val_mse: 0.0059\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0562 - mse: 0.0062 - val_loss: 0.0058 - val_mae: 0.0531 - val_mse: 0.0058\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0062 - mae: 0.0562 - mse: 0.0062 - val_loss: 0.0057 - val_mae: 0.0534 - val_mse: 0.0057\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0060 - mae: 0.0548 - mse: 0.0060 - val_loss: 0.0055 - val_mae: 0.0516 - val_mse: 0.0055\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0058 - mae: 0.0548 - mse: 0.0058 - val_loss: 0.0058 - val_mae: 0.0538 - val_mse: 0.0058\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0549 - mse: 0.0059 - val_loss: 0.0074 - val_mae: 0.0692 - val_mse: 0.0074\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0052 - mae: 0.0529 - mse: 0.0052 - val_loss: 0.0047 - val_mae: 0.0505 - val_mse: 0.0047\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0049 - mae: 0.0518 - mse: 0.0049 - val_loss: 0.0047 - val_mae: 0.0490 - val_mse: 0.0047\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0046 - mae: 0.0503 - mse: 0.0046 - val_loss: 0.0048 - val_mae: 0.0495 - val_mse: 0.0048\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0505 - mse: 0.0047 - val_loss: 0.0045 - val_mae: 0.0483 - val_mse: 0.0045\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0493 - mse: 0.0045 - val_loss: 0.0043 - val_mae: 0.0476 - val_mse: 0.0043\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0049 - mae: 0.0519 - mse: 0.0049 - val_loss: 0.0053 - val_mae: 0.0562 - val_mse: 0.0053\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0044 - mae: 0.0487 - mse: 0.0044 - val_loss: 0.0046 - val_mae: 0.0502 - val_mse: 0.0046\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0499 - mse: 0.0045 - val_loss: 0.0042 - val_mae: 0.0464 - val_mse: 0.0042\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0044 - mae: 0.0488 - mse: 0.0044 - val_loss: 0.0047 - val_mae: 0.0523 - val_mse: 0.0047\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0044 - mae: 0.0487 - mse: 0.0044 - val_loss: 0.0040 - val_mae: 0.0464 - val_mse: 0.0040\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0499 - mse: 0.0045 - val_loss: 0.0044 - val_mae: 0.0493 - val_mse: 0.0044\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0496 - mse: 0.0045 - val_loss: 0.0044 - val_mae: 0.0500 - val_mse: 0.0044\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0043 - mae: 0.0492 - mse: 0.0043 - val_loss: 0.0041 - val_mae: 0.0469 - val_mse: 0.0041\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0044 - mae: 0.0488 - mse: 0.0044 - val_loss: 0.0062 - val_mae: 0.0634 - val_mse: 0.0062\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0045 - mae: 0.0504 - mse: 0.0045 - val_loss: 0.0038 - val_mae: 0.0455 - val_mse: 0.0038\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0042 - mae: 0.0487 - mse: 0.0042 - val_loss: 0.0042 - val_mae: 0.0478 - val_mse: 0.0042\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0041 - mae: 0.0478 - mse: 0.0041 - val_loss: 0.0041 - val_mae: 0.0488 - val_mse: 0.0041\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0042 - mae: 0.0494 - mse: 0.0042 - val_loss: 0.0037 - val_mae: 0.0448 - val_mse: 0.0037\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0043 - mae: 0.0493 - mse: 0.0043 - val_loss: 0.0038 - val_mae: 0.0451 - val_mse: 0.0038\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0044 - mae: 0.0506 - mse: 0.0044 - val_loss: 0.0040 - val_mae: 0.0475 - val_mse: 0.0040\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0042 - mae: 0.0490 - mse: 0.0042 - val_loss: 0.0038 - val_mae: 0.0471 - val_mse: 0.0038\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0039 - mae: 0.0475 - mse: 0.0039 - val_loss: 0.0034 - val_mae: 0.0432 - val_mse: 0.0034\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0039 - mae: 0.0474 - mse: 0.0039 - val_loss: 0.0038 - val_mae: 0.0470 - val_mse: 0.0038\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0043 - mae: 0.0497 - mse: 0.0043 - val_loss: 0.0050 - val_mae: 0.0572 - val_mse: 0.0050\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0461 - mse: 0.0037 - val_loss: 0.0036 - val_mae: 0.0454 - val_mse: 0.0036\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0041 - mae: 0.0482 - mse: 0.0041 - val_loss: 0.0033 - val_mae: 0.0424 - val_mse: 0.0033\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0039 - mae: 0.0474 - mse: 0.0039 - val_loss: 0.0037 - val_mae: 0.0469 - val_mse: 0.0037\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0041 - mae: 0.0487 - mse: 0.0041 - val_loss: 0.0035 - val_mae: 0.0440 - val_mse: 0.0035\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0461 - mse: 0.0037 - val_loss: 0.0035 - val_mae: 0.0441 - val_mse: 0.0035\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0040 - mae: 0.0481 - mse: 0.0040 - val_loss: 0.0066 - val_mae: 0.0684 - val_mse: 0.0066\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0038 - mae: 0.0467 - mse: 0.0038 - val_loss: 0.0037 - val_mae: 0.0464 - val_mse: 0.0037\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0040 - mae: 0.0478 - mse: 0.0040 - val_loss: 0.0038 - val_mae: 0.0482 - val_mse: 0.0038\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0464 - mse: 0.0037 - val_loss: 0.0033 - val_mae: 0.0428 - val_mse: 0.0033\n",
      "Epoch 58/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0448 - mse: 0.0035 - val_loss: 0.0042 - val_mae: 0.0504 - val_mse: 0.0042\n",
      "Epoch 59/200\n",
      "324/324 [==============================] - 3s 10ms/step - loss: 0.0039 - mae: 0.0482 - mse: 0.0039 - val_loss: 0.0034 - val_mae: 0.0443 - val_mse: 0.0034\n",
      "Epoch 60/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0038 - mae: 0.0471 - mse: 0.0038 - val_loss: 0.0037 - val_mae: 0.0467 - val_mse: 0.0037\n",
      "Epoch 61/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0457 - mse: 0.0036 - val_loss: 0.0038 - val_mae: 0.0469 - val_mse: 0.0038\n",
      "Epoch 62/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0441 - mse: 0.0035 - val_loss: 0.0034 - val_mae: 0.0432 - val_mse: 0.0034\n",
      "Epoch 63/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0040 - mae: 0.0482 - mse: 0.0040 - val_loss: 0.0030 - val_mae: 0.0407 - val_mse: 0.0030\n",
      "Epoch 64/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0039 - mae: 0.0477 - mse: 0.0039 - val_loss: 0.0054 - val_mae: 0.0597 - val_mse: 0.0054\n",
      "Epoch 65/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0461 - mse: 0.0037 - val_loss: 0.0030 - val_mae: 0.0407 - val_mse: 0.0030\n",
      "Epoch 66/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0459 - mse: 0.0037 - val_loss: 0.0046 - val_mae: 0.0544 - val_mse: 0.0046\n",
      "Epoch 67/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0039 - mae: 0.0482 - mse: 0.0039 - val_loss: 0.0030 - val_mae: 0.0405 - val_mse: 0.0030\n",
      "Epoch 68/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0041 - mae: 0.0489 - mse: 0.0041 - val_loss: 0.0032 - val_mae: 0.0423 - val_mse: 0.0032\n",
      "Epoch 69/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0447 - mse: 0.0035 - val_loss: 0.0032 - val_mae: 0.0425 - val_mse: 0.0032\n",
      "Epoch 70/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0439 - mse: 0.0033 - val_loss: 0.0037 - val_mae: 0.0478 - val_mse: 0.0037\n",
      "Epoch 71/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0444 - mse: 0.0035 - val_loss: 0.0030 - val_mae: 0.0398 - val_mse: 0.0030\n",
      "Epoch 72/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0436 - mse: 0.0033 - val_loss: 0.0034 - val_mae: 0.0436 - val_mse: 0.0034\n",
      "Epoch 73/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0451 - mse: 0.0036 - val_loss: 0.0041 - val_mae: 0.0501 - val_mse: 0.0041\n",
      "Epoch 74/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0453 - mse: 0.0036 - val_loss: 0.0036 - val_mae: 0.0451 - val_mse: 0.0036\n",
      "Epoch 75/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0425 - mse: 0.0032 - val_loss: 0.0038 - val_mae: 0.0475 - val_mse: 0.0038\n",
      "Epoch 76/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0438 - mse: 0.0033 - val_loss: 0.0030 - val_mae: 0.0408 - val_mse: 0.0030\n",
      "Epoch 77/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0426 - mse: 0.0033 - val_loss: 0.0029 - val_mae: 0.0389 - val_mse: 0.0029\n",
      "Epoch 78/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0434 - mse: 0.0033 - val_loss: 0.0029 - val_mae: 0.0399 - val_mse: 0.0029\n",
      "Epoch 79/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0451 - mse: 0.0035 - val_loss: 0.0030 - val_mae: 0.0401 - val_mse: 0.0030\n",
      "Epoch 80/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0439 - mse: 0.0033 - val_loss: 0.0033 - val_mae: 0.0435 - val_mse: 0.0033\n",
      "Epoch 81/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0434 - mse: 0.0033 - val_loss: 0.0036 - val_mae: 0.0468 - val_mse: 0.0036\n",
      "Epoch 82/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0432 - mse: 0.0033 - val_loss: 0.0032 - val_mae: 0.0421 - val_mse: 0.0032\n",
      "Epoch 83/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0455 - mse: 0.0037 - val_loss: 0.0040 - val_mae: 0.0497 - val_mse: 0.0040\n",
      "Epoch 84/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0433 - mse: 0.0033 - val_loss: 0.0029 - val_mae: 0.0393 - val_mse: 0.0029\n",
      "Epoch 85/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0446 - mse: 0.0034 - val_loss: 0.0028 - val_mae: 0.0393 - val_mse: 0.0028\n",
      "Epoch 86/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0032 - mae: 0.0432 - mse: 0.0032 - val_loss: 0.0029 - val_mae: 0.0391 - val_mse: 0.0029\n",
      "Epoch 87/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0433 - mse: 0.0032 - val_loss: 0.0034 - val_mae: 0.0438 - val_mse: 0.0034\n",
      "Epoch 88/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0450 - mse: 0.0035 - val_loss: 0.0029 - val_mae: 0.0401 - val_mse: 0.0029\n",
      "Epoch 89/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0437 - mse: 0.0033 - val_loss: 0.0028 - val_mae: 0.0383 - val_mse: 0.0028\n",
      "Epoch 90/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0417 - mse: 0.0031 - val_loss: 0.0029 - val_mae: 0.0403 - val_mse: 0.0029\n",
      "Epoch 91/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0438 - mse: 0.0033 - val_loss: 0.0038 - val_mae: 0.0497 - val_mse: 0.0038\n",
      "Epoch 92/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0427 - mse: 0.0032 - val_loss: 0.0037 - val_mae: 0.0471 - val_mse: 0.0037\n",
      "Epoch 93/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0455 - mse: 0.0036 - val_loss: 0.0028 - val_mae: 0.0390 - val_mse: 0.0028\n",
      "Epoch 94/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0447 - mse: 0.0034 - val_loss: 0.0027 - val_mae: 0.0381 - val_mse: 0.0027\n",
      "Epoch 95/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0423 - mse: 0.0032 - val_loss: 0.0029 - val_mae: 0.0408 - val_mse: 0.0029\n",
      "Epoch 96/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0419 - mse: 0.0031 - val_loss: 0.0030 - val_mae: 0.0403 - val_mse: 0.0030\n",
      "Epoch 97/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0432 - mse: 0.0033 - val_loss: 0.0036 - val_mae: 0.0471 - val_mse: 0.0036\n",
      "Epoch 98/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0425 - mse: 0.0032 - val_loss: 0.0067 - val_mae: 0.0696 - val_mse: 0.0067\n",
      "Epoch 99/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0453 - mse: 0.0035 - val_loss: 0.0034 - val_mae: 0.0436 - val_mse: 0.0034\n",
      "Epoch 100/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0429 - mse: 0.0033 - val_loss: 0.0054 - val_mae: 0.0602 - val_mse: 0.0054\n",
      "Epoch 101/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0451 - mse: 0.0035 - val_loss: 0.0036 - val_mae: 0.0468 - val_mse: 0.0036\n",
      "Epoch 102/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0425 - mse: 0.0031 - val_loss: 0.0040 - val_mae: 0.0498 - val_mse: 0.0040\n",
      "Epoch 103/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0421 - mse: 0.0031 - val_loss: 0.0029 - val_mae: 0.0396 - val_mse: 0.0029\n",
      "Epoch 104/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0419 - mse: 0.0031 - val_loss: 0.0035 - val_mae: 0.0455 - val_mse: 0.0035\n",
      "Epoch 105/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0432 - mse: 0.0033 - val_loss: 0.0037 - val_mae: 0.0473 - val_mse: 0.0037\n",
      "Epoch 106/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0435 - mse: 0.0033 - val_loss: 0.0027 - val_mae: 0.0372 - val_mse: 0.0027\n",
      "Epoch 107/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0438 - mse: 0.0033 - val_loss: 0.0032 - val_mae: 0.0434 - val_mse: 0.0032\n",
      "Epoch 108/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0427 - mse: 0.0032 - val_loss: 0.0034 - val_mae: 0.0447 - val_mse: 0.0034\n",
      "Epoch 109/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0439 - mse: 0.0034 - val_loss: 0.0033 - val_mae: 0.0454 - val_mse: 0.0033\n",
      "Epoch 110/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0423 - mse: 0.0031 - val_loss: 0.0032 - val_mae: 0.0439 - val_mse: 0.0032\n",
      "Epoch 111/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0434 - mse: 0.0033 - val_loss: 0.0027 - val_mae: 0.0382 - val_mse: 0.0027\n",
      "Epoch 112/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0429 - mse: 0.0032 - val_loss: 0.0028 - val_mae: 0.0401 - val_mse: 0.0028\n",
      "Epoch 113/200\n",
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0030 - mae: 0.0414 - mse: 0.0030 - val_loss: 0.0029 - val_mae: 0.0391 - val_mse: 0.0029\n",
      "Epoch 114/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0038 - mae: 0.0470 - mse: 0.0038 - val_loss: 0.0028 - val_mae: 0.0390 - val_mse: 0.0028\n",
      "Epoch 115/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0443 - mse: 0.0034 - val_loss: 0.0032 - val_mae: 0.0443 - val_mse: 0.0032\n",
      "Epoch 116/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0449 - mse: 0.0035 - val_loss: 0.0039 - val_mae: 0.0505 - val_mse: 0.0039\n",
      "Epoch 117/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0038 - mae: 0.0476 - mse: 0.0038 - val_loss: 0.0027 - val_mae: 0.0380 - val_mse: 0.0027\n",
      "Epoch 118/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0409 - mse: 0.0030 - val_loss: 0.0032 - val_mae: 0.0443 - val_mse: 0.0032\n",
      "Epoch 119/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0465 - mse: 0.0036 - val_loss: 0.0030 - val_mae: 0.0406 - val_mse: 0.0030\n",
      "Epoch 120/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0409 - mse: 0.0030 - val_loss: 0.0027 - val_mae: 0.0377 - val_mse: 0.0027\n",
      "Epoch 121/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0431 - mse: 0.0033 - val_loss: 0.0027 - val_mae: 0.0392 - val_mse: 0.0027\n",
      "Epoch 122/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0431 - mse: 0.0033 - val_loss: 0.0034 - val_mae: 0.0446 - val_mse: 0.0034\n",
      "Epoch 123/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0433 - mse: 0.0032 - val_loss: 0.0036 - val_mae: 0.0463 - val_mse: 0.0036\n",
      "Epoch 124/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0413 - mse: 0.0030 - val_loss: 0.0030 - val_mae: 0.0413 - val_mse: 0.0030\n",
      "Epoch 125/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0441 - mse: 0.0032 - val_loss: 0.0029 - val_mae: 0.0410 - val_mse: 0.0029\n",
      "Epoch 126/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0420 - mse: 0.0032 - val_loss: 0.0047 - val_mae: 0.0550 - val_mse: 0.0047\n",
      "Epoch 127/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0419 - mse: 0.0031 - val_loss: 0.0033 - val_mae: 0.0454 - val_mse: 0.0033\n",
      "Epoch 128/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0427 - mse: 0.0032 - val_loss: 0.0029 - val_mae: 0.0402 - val_mse: 0.0029\n",
      "Epoch 129/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0462 - mse: 0.0036 - val_loss: 0.0037 - val_mae: 0.0452 - val_mse: 0.0037\n",
      "Epoch 130/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0413 - mse: 0.0030 - val_loss: 0.0027 - val_mae: 0.0383 - val_mse: 0.0027\n",
      "Epoch 131/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0445 - mse: 0.0034 - val_loss: 0.0029 - val_mae: 0.0399 - val_mse: 0.0029\n",
      "Epoch 132/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0415 - mse: 0.0030 - val_loss: 0.0027 - val_mae: 0.0388 - val_mse: 0.0027\n",
      "Epoch 133/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0418 - mse: 0.0031 - val_loss: 0.0031 - val_mae: 0.0399 - val_mse: 0.0031\n",
      "Epoch 134/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0419 - mse: 0.0031 - val_loss: 0.0032 - val_mae: 0.0406 - val_mse: 0.0032\n",
      "Epoch 135/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0431 - mse: 0.0033 - val_loss: 0.0039 - val_mae: 0.0491 - val_mse: 0.0039\n",
      "Epoch 00135: early stopping\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  0.03521061980246375    std:  0.0518904595045858    MAE:  0.04953439931297689     R2:  0.9758041732541212\n",
      "Test. mean:  0.034539314459070714    std:  0.05187746690498466    MAE:  0.04910765802854196     R2:  0.9751751963355441\n",
      "\n",
      "Duration :  00:05:21 854ms\n",
      "\u001b[1maverage MAE of the training set:\u001b[0m   0.12 +/- 0.07\n",
      "\u001b[1maverage MAE of the validation set:\u001b[0m 0.12 +/- 0.07\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#######################################################################################\n",
    "# optimization of the ANN\n",
    "# library used: keras and scikit learn for the KFold cross-validator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "VERBOSE = 1\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 25\n",
    "N_SPLIT = 5\n",
    "\n",
    "vID.chrono_start()\n",
    "\n",
    "# variables created to save at each iteration of the KFold process: the man error, the standard deviation, MAE, R2\n",
    "meantT=list()\n",
    "stdtT=list()\n",
    "MAEtT=list()\n",
    "R2tT=list()\n",
    "meanvT=list()\n",
    "stdvT=list()\n",
    "MAEvT=list()\n",
    "R2vT=list()\n",
    "\n",
    "kfold = KFold(n_splits=N_SPLIT,shuffle=True,random_state=42) # k-fold is here!\n",
    "#print(list(kfold.split(x_train,y_train)))\n",
    "\n",
    "j = 0 # Variable for keeping count of split we are executing\n",
    "# The KFold cv provides train/test indices to split data in train/test sets\n",
    "for train_idx, val_idx in list(kfold.split(xdata,ydata)):\n",
    "\n",
    "    x_train_cv = xdata.iloc[train_idx]\n",
    "    x_valid_cv = xdata.iloc[val_idx]\n",
    "    y_train_cv = ydata.iloc[train_idx]\n",
    "    y_valid_cv = ydata.iloc[val_idx]\n",
    "#    display(x_train_cv,x_valid_cv)\n",
    "# This part has been commented with respect to the original script\n",
    "    # scaler = preprocessing.StandardScaler()\n",
    "    # scaler.fit(x_train_cv.values)\n",
    "    # xt_scaled = scaler.transform(x_train_cv.values) #returns a numpy array\n",
    "    # xv_scaled = scaler.transform(x_valid_cv.values) #returns a numpy array\n",
    "    # x_train_cv = pd.DataFrame(xt_scaled, index=x_train_cv.index, columns=x_train_cv.columns)\n",
    "    # x_valid_cv = pd.DataFrame(xv_scaled, index=x_valid_cv.index, columns=x_valid_cv.columns)\n",
    "    # del xt_scaled, xv_scaled\n",
    "##############\n",
    "#    display(x_train_cv.describe().style.format(\"{0:.2f}\").set_caption(\"Training set after normalization (with scikit-learn):\"))\n",
    "#    display(x_valid_cv.describe().style.format(\"{0:.2f}\").set_caption(\"Validation set after normalization (with scikit-learn):\"))\n",
    "    print(f\"{color.BOLD}{color.RED}Fold {j}{color.OFF}\")\n",
    "    j+=1\n",
    "    ANNmodel=defANN( (53,), acthL )\n",
    "    ANNhistory = ANNmodel.fit(x_train_cv,\n",
    "                        y_train_cv,\n",
    "                        epochs          = EPOCHS,\n",
    "                        batch_size      = BATCH_SIZE,\n",
    "                        verbose         = VERBOSE,\n",
    "                        validation_data = (x_valid_cv, y_valid_cv),\n",
    "                        callbacks=[es])\n",
    "    ytrain_hat=ANNmodel.predict(x_train_cv)\n",
    "    yvalid_hat=ANNmodel.predict(x_valid_cv)\n",
    "    diffyt = ytrain_hat.ravel() - y_train_cv.ravel()\n",
    "    diffyv = yvalid_hat.ravel() - y_valid_cv.ravel()\n",
    "\n",
    "    print()\n",
    "    print(\"xCO2(predicted) - xCO2(actual)\")\n",
    "    print(\n",
    "          \"Train.\",\"mean: \", np.mean(diffyt),\n",
    "          \"   std: \", np.std(diffyt),\n",
    "          \"   MAE: \", np.average(abs(diffyt)),\n",
    "          \"    R2: \", np.corrcoef(y_train_cv.ravel(),ytrain_hat.ravel())[0,1]\n",
    "         )\n",
    "    print(\n",
    "          \"Test.\",\"mean: \", np.mean(diffyv),\n",
    "          \"   std: \", np.std(diffyv),\n",
    "          \"   MAE: \", np.average(abs(diffyv)),\n",
    "          \"    R2: \", np.corrcoef(y_valid_cv.ravel(),yvalid_hat.ravel())[0,1]\n",
    "         )\n",
    "    meantT.append(np.mean(diffyt))\n",
    "    meanvT.append(np.mean(diffyv))\n",
    "    stdtT.append(np.std(diffyt))\n",
    "    stdvT.append(np.std(diffyv))\n",
    "    MAEtT.append(np.average(abs(diffyt)))\n",
    "    MAEvT.append(np.average(abs(diffyv)))\n",
    "    R2tT.append(np.corrcoef(y_train_cv.ravel(),ytrain_hat.ravel())[0,1])\n",
    "    R2vT.append(np.corrcoef(y_valid_cv.ravel(),yvalid_hat.ravel())[0,1])\n",
    "    \n",
    "vID.chrono_show()\n",
    "\n",
    "#######################################################################################\n",
    "# accuracy of the ANN?\n",
    "# library used: numpy\n",
    "print(f\"{color.BOLD}average MAE of the training set:{color.OFF}   {np.mean(MAEtT):.2f} +/- {np.std(MAEtT):.2f}\")\n",
    "print(f\"{color.BOLD}average MAE of the validation set:{color.OFF} {np.mean(MAEvT):.2f} +/- {np.std(MAEvT):.2f}\")\n",
    "\n",
    "figCV, axCV = plt.subplots(1, 1)\n",
    "figCV.set_size_inches(12,5)\n",
    "axCV.errorbar(x=np.arange(len(meantT)), y=meantT, yerr=MAEtT, label='training sets', fmt='o-', capsize=10)\n",
    "axCV.errorbar(x=np.arange(len(meanvT))+0.1, y=meanvT, yerr=MAEvT, label='validation sets', fmt='o-', capsize=10)\n",
    "axCV.legend(loc='lower left', shadow=True, fontsize='14')\n",
    "axCV.set_xlabel('cross-validation k-values ',fontdict={'fontsize':16})\n",
    "axCV.set_ylabel('$\\hat{y}-y_{\\mathrm{actual}}$',fontdict={'fontsize':16})\n",
    "axCV.tick_params(labelsize = 14)\n",
    "plt.savefig('./CO2-images/KFold-cv-AppliedToSong_etal.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d639724-2e98-43dd-b6e5-7f0767cfb27d",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "You will probably find results close to those reported in the following error plot:\n",
    "<p style=\"text-align: center\"><img width=\"650px\" src=\"./CO2-images/KFold-cv-AppliedToSong_etalK.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_ResultsSong\"></p>\n",
    "    <b>This error plot shows a bad performance of the original ML algorithm of Song <i>et al</i>. (<i>i.e.</i> without standardization of the data), with a strong variation of error bars.</b>\n",
    "    \n",
    "Either the authors did actually apply a standardization preprocessing and they forgot to mention it in the article, or they ran several optimization algorithms of the ANN until they found a seemingly performant one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1998a632",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**End at:** Sunday 19 June 2022, 20:32:42  \n",
       "**Duration:** 01:40:02 152ms"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p style=\"text-align: center\"><img width=\"800px\" src=\"./svg/logoEnd.svg\" style=\"margin-left:auto; margin-right:auto\"/></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vID.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb92c04-23d1-4c52-885d-fa8e5eee43c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
