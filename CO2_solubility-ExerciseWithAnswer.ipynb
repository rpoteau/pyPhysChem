{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10024ca9-7e94-4f5c-a982-b4537b919d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "  font-family: Verdana, \"DejaVu Sans\", \"Bitstream Vera Sans\", Geneva, sans-serif;\n",
       "  font-weight: bold;\n",
       "}\n",
       "body {\n",
       "  font-family: Verdana, \"DejaVu Sans\", \"Bitstream Vera Sans\", Geneva, sans-serif;\n",
       "  font-weight: 200;\n",
       "}\n",
       "h1 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 30px ;\n",
       "  color: white;\n",
       "  background: #b11d01;\n",
       "  text-align: center;\n",
       "}\n",
       "h2 {\n",
       "  border: 3px solid #333;\n",
       "  padding: 18px ;\n",
       "  color: #b11d01;\n",
       "  background: #ffffff;\n",
       "  text-align: center;\n",
       "}\n",
       "h3 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 12px ;\n",
       "  color: #000000;\n",
       "  background: #c1c1c1;\n",
       "  text-align: left;\n",
       "}\n",
       "h4 {\n",
       "  border: 0 solid #333;\n",
       "  padding: 2px ;\n",
       "  color: #000000;\n",
       "  background: #d9fffc;\n",
       "  text-align: left;\n",
       "}\n",
       "h5 {\n",
       "  border: 1px solid #333;\n",
       "  padding: 2px ;\n",
       "  color: #000000;\n",
       "  background: #ffffff;\n",
       "  text-align: left;\n",
       "}\n",
       ".warn {    \n",
       "    background-color: #fcf2f2;\n",
       "    border-color: #dFb5b4;\n",
       "    border-left: 5px solid #dfb5b4;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       ".rq {    \n",
       "    background-color: #e2e2e2;\n",
       "    border-color: #969696;\n",
       "    border-left: 5px solid #969696;\n",
       "    padding: 0.5em;\n",
       "    font-weight: 200;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Start at:** Friday 19 August 2022, 11:41:58  \n",
       "**Hostname:** localhost.localdomain (Linux)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p style=\"text-align: center\"><img width=\"800px\" src=\"./svg/logoPytChem.svg\" style=\"margin-left:auto; margin-right:auto\"/></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import visualID_Eng as vID\n",
    "vID.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e4b9b",
   "metadata": {},
   "source": [
    "# Prediction by an artificial neural network of the solubility of CO<sub>2</sub> in ionic liquids\n",
    "\n",
    "<div class=\"rq\">\n",
    "<b>Reference</b>: \n",
    "Z. Song, H. Shi, X. Zhang & T. Zhou (**2020**), Prediction of CO<sub>2</sub> solubility in ionic liquids using machine learning methods, [<i>Chem. Eng. Sci.</i> <b>223</b>: 115752](https://www.doi.org/10.1016/j.ces.2020.115752) \n",
    "<br>\n",
    "<p style=\"text-align: center\"><img width=\"650px\" src=\"./CO2-images/AbstractANNCO2-SongEtal.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_AbstractSong\"></p>\n",
    "<br>\n",
    "The main results are graphically reported below.\n",
    "<br>\n",
    "<p style=\"text-align: center\"><img width=\"900px\" src=\"./CO2-images/ANNCO2-SongEtal-Results.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_ResultsSong\"></p>\n",
    "<br>\n",
    "Yet, it seems sthat no standardization process of the data has been applied. \n",
    "    \n",
    "<span style=\"color:red\">Moreover, a spurious separation of the data between training and test sets has been applied: \"<i>Instead of performing random selection, we employ a hybrid artificial-random strategy to decompose the dataset. Specifically, the data points consisting of the least frequently used groups are equally divided into five folders\"</i></span> \n",
    "<br><br>\n",
    "<b>It raises doubts about the stability of the algorithm developped in this paper (*unless the authors forgot to mention that data were standardized*).</b>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84860c4-3fcf-45f5-80a3-607ead68da80",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "<span style=\"font-weight:bold\">The goal of this exercise is to apply the <i>K</i>-fold cross-validation the ANN part of this article, <i>i.e.</i> without standardized data. </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af39c092-f4b7-460b-b698-32e0f2c55461",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os,sys\n",
    "from IPython.display import display\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   OFF = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89a89d-fb12-4ba1-8382-1609a0c4f848",
   "metadata": {},
   "source": [
    "<a id=\"data-read\"></a>\n",
    "## **1.** Database reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7d2251b-1f22-42f5-b930-e6032fe55170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IL</th>\n",
       "      <th>cation</th>\n",
       "      <th>anion</th>\n",
       "      <th>x_CO2</th>\n",
       "      <th>T (K)</th>\n",
       "      <th>P (bar)</th>\n",
       "      <th>[CH3]</th>\n",
       "      <th>[CH2]</th>\n",
       "      <th>[CH]</th>\n",
       "      <th>[OCH2]</th>\n",
       "      <th>...</th>\n",
       "      <th>[MeSO3]</th>\n",
       "      <th>[TfO]</th>\n",
       "      <th>[NfO]</th>\n",
       "      <th>[TDfO]</th>\n",
       "      <th>[TOS]</th>\n",
       "      <th>[C12PhSO3]</th>\n",
       "      <th>[DMPO4]</th>\n",
       "      <th>[DEPO4]</th>\n",
       "      <th>[DBPO4]</th>\n",
       "      <th>[methide]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.610</td>\n",
       "      <td>363.15</td>\n",
       "      <td>246.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.500</td>\n",
       "      <td>383.15</td>\n",
       "      <td>235.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.610</td>\n",
       "      <td>353.15</td>\n",
       "      <td>223.30</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.500</td>\n",
       "      <td>373.15</td>\n",
       "      <td>198.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[BMIM][BF4]</td>\n",
       "      <td>[BMIM]</td>\n",
       "      <td>[BF4]</td>\n",
       "      <td>0.610</td>\n",
       "      <td>343.15</td>\n",
       "      <td>188.50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10111</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.592</td>\n",
       "      <td>298.15</td>\n",
       "      <td>35.86</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10112</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.239</td>\n",
       "      <td>343.15</td>\n",
       "      <td>27.54</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10113</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.396</td>\n",
       "      <td>298.15</td>\n",
       "      <td>20.15</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10114</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.140</td>\n",
       "      <td>343.15</td>\n",
       "      <td>17.93</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10115</th>\n",
       "      <td>[HMIM][Tf2N]</td>\n",
       "      <td>[HMIM]</td>\n",
       "      <td>[Tf2N]</td>\n",
       "      <td>0.139</td>\n",
       "      <td>323.15</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10116 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 IL  cation   anion  x_CO2   T (K)  P (bar)  [CH3]  [CH2]  \\\n",
       "0       [BMIM][BF4]  [BMIM]   [BF4]  0.610  363.15   246.00      1      3   \n",
       "1       [BMIM][BF4]  [BMIM]   [BF4]  0.500  383.15   235.00      1      3   \n",
       "2       [BMIM][BF4]  [BMIM]   [BF4]  0.610  353.15   223.30      1      3   \n",
       "3       [BMIM][BF4]  [BMIM]   [BF4]  0.500  373.15   198.00      1      3   \n",
       "4       [BMIM][BF4]  [BMIM]   [BF4]  0.610  343.15   188.50      1      3   \n",
       "...             ...     ...     ...    ...     ...      ...    ...    ...   \n",
       "10111  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.592  298.15    35.86      1      5   \n",
       "10112  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.239  343.15    27.54      1      5   \n",
       "10113  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.396  298.15    20.15      1      5   \n",
       "10114  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.140  343.15    17.93      1      5   \n",
       "10115  [HMIM][Tf2N]  [HMIM]  [Tf2N]  0.139  323.15     8.00      1      5   \n",
       "\n",
       "       [CH]  [OCH2]  ...  [MeSO3]  [TfO]  [NfO]  [TDfO]  [TOS]  [C12PhSO3]  \\\n",
       "0         0       0  ...        0      0      0       0      0           0   \n",
       "1         0       0  ...        0      0      0       0      0           0   \n",
       "2         0       0  ...        0      0      0       0      0           0   \n",
       "3         0       0  ...        0      0      0       0      0           0   \n",
       "4         0       0  ...        0      0      0       0      0           0   \n",
       "...     ...     ...  ...      ...    ...    ...     ...    ...         ...   \n",
       "10111     0       0  ...        0      0      0       0      0           0   \n",
       "10112     0       0  ...        0      0      0       0      0           0   \n",
       "10113     0       0  ...        0      0      0       0      0           0   \n",
       "10114     0       0  ...        0      0      0       0      0           0   \n",
       "10115     0       0  ...        0      0      0       0      0           0   \n",
       "\n",
       "       [DMPO4]  [DEPO4]  [DBPO4]  [methide]  \n",
       "0            0        0        0          0  \n",
       "1            0        0        0          0  \n",
       "2            0        0        0          0  \n",
       "3            0        0        0          0  \n",
       "4            0        0        0          0  \n",
       "...        ...      ...      ...        ...  \n",
       "10111        0        0        0          0  \n",
       "10112        0        0        0          0  \n",
       "10113        0        0        0          0  \n",
       "10114        0        0        0          0  \n",
       "10115        0        0        0          0  \n",
       "\n",
       "[10116 rows x 57 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_668c8\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_668c8_level0_col0\" class=\"col_heading level0 col0\" >x_CO2</th>\n",
       "      <th id=\"T_668c8_level0_col1\" class=\"col_heading level0 col1\" >T (K)</th>\n",
       "      <th id=\"T_668c8_level0_col2\" class=\"col_heading level0 col2\" >P (bar)</th>\n",
       "      <th id=\"T_668c8_level0_col3\" class=\"col_heading level0 col3\" >[CH3]</th>\n",
       "      <th id=\"T_668c8_level0_col4\" class=\"col_heading level0 col4\" >[CH2]</th>\n",
       "      <th id=\"T_668c8_level0_col5\" class=\"col_heading level0 col5\" >[CH]</th>\n",
       "      <th id=\"T_668c8_level0_col6\" class=\"col_heading level0 col6\" >[OCH2]</th>\n",
       "      <th id=\"T_668c8_level0_col7\" class=\"col_heading level0 col7\" >[OCH3]</th>\n",
       "      <th id=\"T_668c8_level0_col8\" class=\"col_heading level0 col8\" >[CF2]</th>\n",
       "      <th id=\"T_668c8_level0_col9\" class=\"col_heading level0 col9\" >[CF3]</th>\n",
       "      <th id=\"T_668c8_level0_col10\" class=\"col_heading level0 col10\" >[OH]</th>\n",
       "      <th id=\"T_668c8_level0_col11\" class=\"col_heading level0 col11\" >CH=CH</th>\n",
       "      <th id=\"T_668c8_level0_col12\" class=\"col_heading level0 col12\" >CH=CH2</th>\n",
       "      <th id=\"T_668c8_level0_col13\" class=\"col_heading level0 col13\" >[Im13]</th>\n",
       "      <th id=\"T_668c8_level0_col14\" class=\"col_heading level0 col14\" >[MIm]</th>\n",
       "      <th id=\"T_668c8_level0_col15\" class=\"col_heading level0 col15\" >[MMIM]</th>\n",
       "      <th id=\"T_668c8_level0_col16\" class=\"col_heading level0 col16\" >[Py]</th>\n",
       "      <th id=\"T_668c8_level0_col17\" class=\"col_heading level0 col17\" >[MPy]</th>\n",
       "      <th id=\"T_668c8_level0_col18\" class=\"col_heading level0 col18\" >[MPyrro]</th>\n",
       "      <th id=\"T_668c8_level0_col19\" class=\"col_heading level0 col19\" >[MPip]</th>\n",
       "      <th id=\"T_668c8_level0_col20\" class=\"col_heading level0 col20\" >[NH3]</th>\n",
       "      <th id=\"T_668c8_level0_col21\" class=\"col_heading level0 col21\" >[NH2]</th>\n",
       "      <th id=\"T_668c8_level0_col22\" class=\"col_heading level0 col22\" >[NH]</th>\n",
       "      <th id=\"T_668c8_level0_col23\" class=\"col_heading level0 col23\" >[N]</th>\n",
       "      <th id=\"T_668c8_level0_col24\" class=\"col_heading level0 col24\" >[P]</th>\n",
       "      <th id=\"T_668c8_level0_col25\" class=\"col_heading level0 col25\" >[S]</th>\n",
       "      <th id=\"T_668c8_level0_col26\" class=\"col_heading level0 col26\" >[BF4]</th>\n",
       "      <th id=\"T_668c8_level0_col27\" class=\"col_heading level0 col27\" >[Cl]</th>\n",
       "      <th id=\"T_668c8_level0_col28\" class=\"col_heading level0 col28\" >[DCA]</th>\n",
       "      <th id=\"T_668c8_level0_col29\" class=\"col_heading level0 col29\" >[NO3]</th>\n",
       "      <th id=\"T_668c8_level0_col30\" class=\"col_heading level0 col30\" >[PF6]</th>\n",
       "      <th id=\"T_668c8_level0_col31\" class=\"col_heading level0 col31\" >[SCN]</th>\n",
       "      <th id=\"T_668c8_level0_col32\" class=\"col_heading level0 col32\" >[TCB]</th>\n",
       "      <th id=\"T_668c8_level0_col33\" class=\"col_heading level0 col33\" >[C(CN)3]</th>\n",
       "      <th id=\"T_668c8_level0_col34\" class=\"col_heading level0 col34\" >[HSO4]</th>\n",
       "      <th id=\"T_668c8_level0_col35\" class=\"col_heading level0 col35\" >[FSA]</th>\n",
       "      <th id=\"T_668c8_level0_col36\" class=\"col_heading level0 col36\" >[Tf2N]</th>\n",
       "      <th id=\"T_668c8_level0_col37\" class=\"col_heading level0 col37\" >[BETA]</th>\n",
       "      <th id=\"T_668c8_level0_col38\" class=\"col_heading level0 col38\" >[FOR]</th>\n",
       "      <th id=\"T_668c8_level0_col39\" class=\"col_heading level0 col39\" >[TFA]</th>\n",
       "      <th id=\"T_668c8_level0_col40\" class=\"col_heading level0 col40\" >[C3F7CO2]</th>\n",
       "      <th id=\"T_668c8_level0_col41\" class=\"col_heading level0 col41\" >[MeSO4]</th>\n",
       "      <th id=\"T_668c8_level0_col42\" class=\"col_heading level0 col42\" >[EtSO4]</th>\n",
       "      <th id=\"T_668c8_level0_col43\" class=\"col_heading level0 col43\" >[MDEGSO4]</th>\n",
       "      <th id=\"T_668c8_level0_col44\" class=\"col_heading level0 col44\" >[MeSO3]</th>\n",
       "      <th id=\"T_668c8_level0_col45\" class=\"col_heading level0 col45\" >[TfO]</th>\n",
       "      <th id=\"T_668c8_level0_col46\" class=\"col_heading level0 col46\" >[NfO]</th>\n",
       "      <th id=\"T_668c8_level0_col47\" class=\"col_heading level0 col47\" >[TDfO]</th>\n",
       "      <th id=\"T_668c8_level0_col48\" class=\"col_heading level0 col48\" >[TOS]</th>\n",
       "      <th id=\"T_668c8_level0_col49\" class=\"col_heading level0 col49\" >[C12PhSO3]</th>\n",
       "      <th id=\"T_668c8_level0_col50\" class=\"col_heading level0 col50\" >[DMPO4]</th>\n",
       "      <th id=\"T_668c8_level0_col51\" class=\"col_heading level0 col51\" >[DEPO4]</th>\n",
       "      <th id=\"T_668c8_level0_col52\" class=\"col_heading level0 col52\" >[DBPO4]</th>\n",
       "      <th id=\"T_668c8_level0_col53\" class=\"col_heading level0 col53\" >[methide]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_668c8_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_668c8_row0_col0\" class=\"data row0 col0\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col1\" class=\"data row0 col1\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col2\" class=\"data row0 col2\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col3\" class=\"data row0 col3\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col4\" class=\"data row0 col4\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col5\" class=\"data row0 col5\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col6\" class=\"data row0 col6\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col7\" class=\"data row0 col7\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col8\" class=\"data row0 col8\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col9\" class=\"data row0 col9\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col10\" class=\"data row0 col10\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col11\" class=\"data row0 col11\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col12\" class=\"data row0 col12\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col13\" class=\"data row0 col13\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col14\" class=\"data row0 col14\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col15\" class=\"data row0 col15\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col16\" class=\"data row0 col16\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col17\" class=\"data row0 col17\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col18\" class=\"data row0 col18\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col19\" class=\"data row0 col19\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col20\" class=\"data row0 col20\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col21\" class=\"data row0 col21\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col22\" class=\"data row0 col22\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col23\" class=\"data row0 col23\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col24\" class=\"data row0 col24\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col25\" class=\"data row0 col25\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col26\" class=\"data row0 col26\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col27\" class=\"data row0 col27\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col28\" class=\"data row0 col28\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col29\" class=\"data row0 col29\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col30\" class=\"data row0 col30\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col31\" class=\"data row0 col31\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col32\" class=\"data row0 col32\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col33\" class=\"data row0 col33\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col34\" class=\"data row0 col34\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col35\" class=\"data row0 col35\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col36\" class=\"data row0 col36\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col37\" class=\"data row0 col37\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col38\" class=\"data row0 col38\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col39\" class=\"data row0 col39\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col40\" class=\"data row0 col40\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col41\" class=\"data row0 col41\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col42\" class=\"data row0 col42\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col43\" class=\"data row0 col43\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col44\" class=\"data row0 col44\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col45\" class=\"data row0 col45\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col46\" class=\"data row0 col46\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col47\" class=\"data row0 col47\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col48\" class=\"data row0 col48\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col49\" class=\"data row0 col49\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col50\" class=\"data row0 col50\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col51\" class=\"data row0 col51\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col52\" class=\"data row0 col52\" >10116.00</td>\n",
       "      <td id=\"T_668c8_row0_col53\" class=\"data row0 col53\" >10116.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_668c8_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_668c8_row1_col0\" class=\"data row1 col0\" >0.33</td>\n",
       "      <td id=\"T_668c8_row1_col1\" class=\"data row1 col1\" >325.27</td>\n",
       "      <td id=\"T_668c8_row1_col2\" class=\"data row1 col2\" >54.21</td>\n",
       "      <td id=\"T_668c8_row1_col3\" class=\"data row1 col3\" >1.18</td>\n",
       "      <td id=\"T_668c8_row1_col4\" class=\"data row1 col4\" >4.72</td>\n",
       "      <td id=\"T_668c8_row1_col5\" class=\"data row1 col5\" >0.02</td>\n",
       "      <td id=\"T_668c8_row1_col6\" class=\"data row1 col6\" >0.02</td>\n",
       "      <td id=\"T_668c8_row1_col7\" class=\"data row1 col7\" >0.04</td>\n",
       "      <td id=\"T_668c8_row1_col8\" class=\"data row1 col8\" >0.04</td>\n",
       "      <td id=\"T_668c8_row1_col9\" class=\"data row1 col9\" >0.01</td>\n",
       "      <td id=\"T_668c8_row1_col10\" class=\"data row1 col10\" >0.06</td>\n",
       "      <td id=\"T_668c8_row1_col11\" class=\"data row1 col11\" >0.00</td>\n",
       "      <td id=\"T_668c8_row1_col12\" class=\"data row1 col12\" >0.00</td>\n",
       "      <td id=\"T_668c8_row1_col13\" class=\"data row1 col13\" >0.01</td>\n",
       "      <td id=\"T_668c8_row1_col14\" class=\"data row1 col14\" >0.77</td>\n",
       "      <td id=\"T_668c8_row1_col15\" class=\"data row1 col15\" >0.01</td>\n",
       "      <td id=\"T_668c8_row1_col16\" class=\"data row1 col16\" >0.00</td>\n",
       "      <td id=\"T_668c8_row1_col17\" class=\"data row1 col17\" >0.01</td>\n",
       "      <td id=\"T_668c8_row1_col18\" class=\"data row1 col18\" >0.09</td>\n",
       "      <td id=\"T_668c8_row1_col19\" class=\"data row1 col19\" >0.00</td>\n",
       "      <td id=\"T_668c8_row1_col20\" class=\"data row1 col20\" >0.00</td>\n",
       "      <td id=\"T_668c8_row1_col21\" class=\"data row1 col21\" >0.01</td>\n",
       "      <td id=\"T_668c8_row1_col22\" class=\"data row1 col22\" >0.00</td>\n",
       "      <td id=\"T_668c8_row1_col23\" class=\"data row1 col23\" >0.02</td>\n",
       "      <td id=\"T_668c8_row1_col24\" class=\"data row1 col24\" >0.05</td>\n",
       "      <td id=\"T_668c8_row1_col25\" class=\"data row1 col25\" >0.00</td>\n",
       "      <td id=\"T_668c8_row1_col26\" class=\"data row1 col26\" >0.11</td>\n",
       "      <td id=\"T_668c8_row1_col27\" class=\"data row1 col27\" >0.02</td>\n",
       "      <td id=\"T_668c8_row1_col28\" class=\"data row1 col28\" >0.02</td>\n",
       "      <td id=\"T_668c8_row1_col29\" class=\"data row1 col29\" >0.02</td>\n",
       "      <td id=\"T_668c8_row1_col30\" class=\"data row1 col30\" >0.11</td>\n",
       "      <td id=\"T_668c8_row1_col31\" class=\"data row1 col31\" >0.02</td>\n",
       "      <td id=\"T_668c8_row1_col32\" class=\"data row1 col32\" >0.01</td>\n",
       "      <td id=\"T_668c8_row1_col33\" class=\"data row1 col33\" >0.07</td>\n",
       "      <td id=\"T_668c8_row1_col34\" class=\"data row1 col34\" >0.00</td>\n",
       "      <td id=\"T_668c8_row1_col35\" class=\"data row1 col35\" >0.01</td>\n",
       "      <td id=\"T_668c8_row1_col36\" class=\"data row1 col36\" >0.43</td>\n",
       "      <td id=\"T_668c8_row1_col37\" class=\"data row1 col37\" >0.00</td>\n",
       "      <td id=\"T_668c8_row1_col38\" class=\"data row1 col38\" >0.01</td>\n",
       "      <td id=\"T_668c8_row1_col39\" class=\"data row1 col39\" >0.01</td>\n",
       "      <td id=\"T_668c8_row1_col40\" class=\"data row1 col40\" >0.00</td>\n",
       "      <td id=\"T_668c8_row1_col41\" class=\"data row1 col41\" >0.02</td>\n",
       "      <td id=\"T_668c8_row1_col42\" class=\"data row1 col42\" >0.01</td>\n",
       "      <td id=\"T_668c8_row1_col43\" class=\"data row1 col43\" >0.01</td>\n",
       "      <td id=\"T_668c8_row1_col44\" class=\"data row1 col44\" >0.02</td>\n",
       "      <td id=\"T_668c8_row1_col45\" class=\"data row1 col45\" >0.05</td>\n",
       "      <td id=\"T_668c8_row1_col46\" class=\"data row1 col46\" >0.01</td>\n",
       "      <td id=\"T_668c8_row1_col47\" class=\"data row1 col47\" >0.01</td>\n",
       "      <td id=\"T_668c8_row1_col48\" class=\"data row1 col48\" >0.00</td>\n",
       "      <td id=\"T_668c8_row1_col49\" class=\"data row1 col49\" >0.01</td>\n",
       "      <td id=\"T_668c8_row1_col50\" class=\"data row1 col50\" >0.00</td>\n",
       "      <td id=\"T_668c8_row1_col51\" class=\"data row1 col51\" >0.01</td>\n",
       "      <td id=\"T_668c8_row1_col52\" class=\"data row1 col52\" >0.00</td>\n",
       "      <td id=\"T_668c8_row1_col53\" class=\"data row1 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_668c8_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_668c8_row2_col0\" class=\"data row2 col0\" >0.24</td>\n",
       "      <td id=\"T_668c8_row2_col1\" class=\"data row2 col1\" >25.24</td>\n",
       "      <td id=\"T_668c8_row2_col2\" class=\"data row2 col2\" >76.66</td>\n",
       "      <td id=\"T_668c8_row2_col3\" class=\"data row2 col3\" >0.96</td>\n",
       "      <td id=\"T_668c8_row2_col4\" class=\"data row2 col4\" >5.48</td>\n",
       "      <td id=\"T_668c8_row2_col5\" class=\"data row2 col5\" >0.25</td>\n",
       "      <td id=\"T_668c8_row2_col6\" class=\"data row2 col6\" >0.16</td>\n",
       "      <td id=\"T_668c8_row2_col7\" class=\"data row2 col7\" >0.20</td>\n",
       "      <td id=\"T_668c8_row2_col8\" class=\"data row2 col8\" >0.39</td>\n",
       "      <td id=\"T_668c8_row2_col9\" class=\"data row2 col9\" >0.10</td>\n",
       "      <td id=\"T_668c8_row2_col10\" class=\"data row2 col10\" >0.28</td>\n",
       "      <td id=\"T_668c8_row2_col11\" class=\"data row2 col11\" >0.06</td>\n",
       "      <td id=\"T_668c8_row2_col12\" class=\"data row2 col12\" >0.06</td>\n",
       "      <td id=\"T_668c8_row2_col13\" class=\"data row2 col13\" >0.10</td>\n",
       "      <td id=\"T_668c8_row2_col14\" class=\"data row2 col14\" >0.42</td>\n",
       "      <td id=\"T_668c8_row2_col15\" class=\"data row2 col15\" >0.08</td>\n",
       "      <td id=\"T_668c8_row2_col16\" class=\"data row2 col16\" >0.07</td>\n",
       "      <td id=\"T_668c8_row2_col17\" class=\"data row2 col17\" >0.11</td>\n",
       "      <td id=\"T_668c8_row2_col18\" class=\"data row2 col18\" >0.29</td>\n",
       "      <td id=\"T_668c8_row2_col19\" class=\"data row2 col19\" >0.06</td>\n",
       "      <td id=\"T_668c8_row2_col20\" class=\"data row2 col20\" >0.07</td>\n",
       "      <td id=\"T_668c8_row2_col21\" class=\"data row2 col21\" >0.09</td>\n",
       "      <td id=\"T_668c8_row2_col22\" class=\"data row2 col22\" >0.06</td>\n",
       "      <td id=\"T_668c8_row2_col23\" class=\"data row2 col23\" >0.16</td>\n",
       "      <td id=\"T_668c8_row2_col24\" class=\"data row2 col24\" >0.23</td>\n",
       "      <td id=\"T_668c8_row2_col25\" class=\"data row2 col25\" >0.06</td>\n",
       "      <td id=\"T_668c8_row2_col26\" class=\"data row2 col26\" >0.31</td>\n",
       "      <td id=\"T_668c8_row2_col27\" class=\"data row2 col27\" >0.12</td>\n",
       "      <td id=\"T_668c8_row2_col28\" class=\"data row2 col28\" >0.15</td>\n",
       "      <td id=\"T_668c8_row2_col29\" class=\"data row2 col29\" >0.12</td>\n",
       "      <td id=\"T_668c8_row2_col30\" class=\"data row2 col30\" >0.31</td>\n",
       "      <td id=\"T_668c8_row2_col31\" class=\"data row2 col31\" >0.14</td>\n",
       "      <td id=\"T_668c8_row2_col32\" class=\"data row2 col32\" >0.08</td>\n",
       "      <td id=\"T_668c8_row2_col33\" class=\"data row2 col33\" >0.26</td>\n",
       "      <td id=\"T_668c8_row2_col34\" class=\"data row2 col34\" >0.04</td>\n",
       "      <td id=\"T_668c8_row2_col35\" class=\"data row2 col35\" >0.11</td>\n",
       "      <td id=\"T_668c8_row2_col36\" class=\"data row2 col36\" >0.49</td>\n",
       "      <td id=\"T_668c8_row2_col37\" class=\"data row2 col37\" >0.03</td>\n",
       "      <td id=\"T_668c8_row2_col38\" class=\"data row2 col38\" >0.11</td>\n",
       "      <td id=\"T_668c8_row2_col39\" class=\"data row2 col39\" >0.11</td>\n",
       "      <td id=\"T_668c8_row2_col40\" class=\"data row2 col40\" >0.05</td>\n",
       "      <td id=\"T_668c8_row2_col41\" class=\"data row2 col41\" >0.13</td>\n",
       "      <td id=\"T_668c8_row2_col42\" class=\"data row2 col42\" >0.11</td>\n",
       "      <td id=\"T_668c8_row2_col43\" class=\"data row2 col43\" >0.10</td>\n",
       "      <td id=\"T_668c8_row2_col44\" class=\"data row2 col44\" >0.15</td>\n",
       "      <td id=\"T_668c8_row2_col45\" class=\"data row2 col45\" >0.23</td>\n",
       "      <td id=\"T_668c8_row2_col46\" class=\"data row2 col46\" >0.09</td>\n",
       "      <td id=\"T_668c8_row2_col47\" class=\"data row2 col47\" >0.08</td>\n",
       "      <td id=\"T_668c8_row2_col48\" class=\"data row2 col48\" >0.06</td>\n",
       "      <td id=\"T_668c8_row2_col49\" class=\"data row2 col49\" >0.10</td>\n",
       "      <td id=\"T_668c8_row2_col50\" class=\"data row2 col50\" >0.03</td>\n",
       "      <td id=\"T_668c8_row2_col51\" class=\"data row2 col51\" >0.07</td>\n",
       "      <td id=\"T_668c8_row2_col52\" class=\"data row2 col52\" >0.04</td>\n",
       "      <td id=\"T_668c8_row2_col53\" class=\"data row2 col53\" >0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_668c8_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_668c8_row3_col0\" class=\"data row3 col0\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col1\" class=\"data row3 col1\" >243.20</td>\n",
       "      <td id=\"T_668c8_row3_col2\" class=\"data row3 col2\" >0.01</td>\n",
       "      <td id=\"T_668c8_row3_col3\" class=\"data row3 col3\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col4\" class=\"data row3 col4\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col5\" class=\"data row3 col5\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col6\" class=\"data row3 col6\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col7\" class=\"data row3 col7\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col8\" class=\"data row3 col8\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col9\" class=\"data row3 col9\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col10\" class=\"data row3 col10\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col11\" class=\"data row3 col11\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col12\" class=\"data row3 col12\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col13\" class=\"data row3 col13\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col14\" class=\"data row3 col14\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col15\" class=\"data row3 col15\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col16\" class=\"data row3 col16\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col17\" class=\"data row3 col17\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col18\" class=\"data row3 col18\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col19\" class=\"data row3 col19\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col20\" class=\"data row3 col20\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col21\" class=\"data row3 col21\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col22\" class=\"data row3 col22\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col23\" class=\"data row3 col23\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col24\" class=\"data row3 col24\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col25\" class=\"data row3 col25\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col26\" class=\"data row3 col26\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col27\" class=\"data row3 col27\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col28\" class=\"data row3 col28\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col29\" class=\"data row3 col29\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col30\" class=\"data row3 col30\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col31\" class=\"data row3 col31\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col32\" class=\"data row3 col32\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col33\" class=\"data row3 col33\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col34\" class=\"data row3 col34\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col35\" class=\"data row3 col35\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col36\" class=\"data row3 col36\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col37\" class=\"data row3 col37\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col38\" class=\"data row3 col38\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col39\" class=\"data row3 col39\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col40\" class=\"data row3 col40\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col41\" class=\"data row3 col41\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col42\" class=\"data row3 col42\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col43\" class=\"data row3 col43\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col44\" class=\"data row3 col44\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col45\" class=\"data row3 col45\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col46\" class=\"data row3 col46\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col47\" class=\"data row3 col47\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col48\" class=\"data row3 col48\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col49\" class=\"data row3 col49\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col50\" class=\"data row3 col50\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col51\" class=\"data row3 col51\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col52\" class=\"data row3 col52\" >0.00</td>\n",
       "      <td id=\"T_668c8_row3_col53\" class=\"data row3 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_668c8_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_668c8_row4_col0\" class=\"data row4 col0\" >0.14</td>\n",
       "      <td id=\"T_668c8_row4_col1\" class=\"data row4 col1\" >308.15</td>\n",
       "      <td id=\"T_668c8_row4_col2\" class=\"data row4 col2\" >10.00</td>\n",
       "      <td id=\"T_668c8_row4_col3\" class=\"data row4 col3\" >1.00</td>\n",
       "      <td id=\"T_668c8_row4_col4\" class=\"data row4 col4\" >3.00</td>\n",
       "      <td id=\"T_668c8_row4_col5\" class=\"data row4 col5\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col6\" class=\"data row4 col6\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col7\" class=\"data row4 col7\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col8\" class=\"data row4 col8\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col9\" class=\"data row4 col9\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col10\" class=\"data row4 col10\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col11\" class=\"data row4 col11\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col12\" class=\"data row4 col12\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col13\" class=\"data row4 col13\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col14\" class=\"data row4 col14\" >1.00</td>\n",
       "      <td id=\"T_668c8_row4_col15\" class=\"data row4 col15\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col16\" class=\"data row4 col16\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col17\" class=\"data row4 col17\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col18\" class=\"data row4 col18\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col19\" class=\"data row4 col19\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col20\" class=\"data row4 col20\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col21\" class=\"data row4 col21\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col22\" class=\"data row4 col22\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col23\" class=\"data row4 col23\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col24\" class=\"data row4 col24\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col25\" class=\"data row4 col25\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col26\" class=\"data row4 col26\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col27\" class=\"data row4 col27\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col28\" class=\"data row4 col28\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col29\" class=\"data row4 col29\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col30\" class=\"data row4 col30\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col31\" class=\"data row4 col31\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col32\" class=\"data row4 col32\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col33\" class=\"data row4 col33\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col34\" class=\"data row4 col34\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col35\" class=\"data row4 col35\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col36\" class=\"data row4 col36\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col37\" class=\"data row4 col37\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col38\" class=\"data row4 col38\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col39\" class=\"data row4 col39\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col40\" class=\"data row4 col40\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col41\" class=\"data row4 col41\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col42\" class=\"data row4 col42\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col43\" class=\"data row4 col43\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col44\" class=\"data row4 col44\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col45\" class=\"data row4 col45\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col46\" class=\"data row4 col46\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col47\" class=\"data row4 col47\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col48\" class=\"data row4 col48\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col49\" class=\"data row4 col49\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col50\" class=\"data row4 col50\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col51\" class=\"data row4 col51\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col52\" class=\"data row4 col52\" >0.00</td>\n",
       "      <td id=\"T_668c8_row4_col53\" class=\"data row4 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_668c8_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_668c8_row5_col0\" class=\"data row5 col0\" >0.30</td>\n",
       "      <td id=\"T_668c8_row5_col1\" class=\"data row5 col1\" >323.15</td>\n",
       "      <td id=\"T_668c8_row5_col2\" class=\"data row5 col2\" >26.80</td>\n",
       "      <td id=\"T_668c8_row5_col3\" class=\"data row5 col3\" >1.00</td>\n",
       "      <td id=\"T_668c8_row5_col4\" class=\"data row5 col4\" >3.00</td>\n",
       "      <td id=\"T_668c8_row5_col5\" class=\"data row5 col5\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col6\" class=\"data row5 col6\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col7\" class=\"data row5 col7\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col8\" class=\"data row5 col8\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col9\" class=\"data row5 col9\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col10\" class=\"data row5 col10\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col11\" class=\"data row5 col11\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col12\" class=\"data row5 col12\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col13\" class=\"data row5 col13\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col14\" class=\"data row5 col14\" >1.00</td>\n",
       "      <td id=\"T_668c8_row5_col15\" class=\"data row5 col15\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col16\" class=\"data row5 col16\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col17\" class=\"data row5 col17\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col18\" class=\"data row5 col18\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col19\" class=\"data row5 col19\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col20\" class=\"data row5 col20\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col21\" class=\"data row5 col21\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col22\" class=\"data row5 col22\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col23\" class=\"data row5 col23\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col24\" class=\"data row5 col24\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col25\" class=\"data row5 col25\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col26\" class=\"data row5 col26\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col27\" class=\"data row5 col27\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col28\" class=\"data row5 col28\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col29\" class=\"data row5 col29\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col30\" class=\"data row5 col30\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col31\" class=\"data row5 col31\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col32\" class=\"data row5 col32\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col33\" class=\"data row5 col33\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col34\" class=\"data row5 col34\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col35\" class=\"data row5 col35\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col36\" class=\"data row5 col36\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col37\" class=\"data row5 col37\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col38\" class=\"data row5 col38\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col39\" class=\"data row5 col39\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col40\" class=\"data row5 col40\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col41\" class=\"data row5 col41\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col42\" class=\"data row5 col42\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col43\" class=\"data row5 col43\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col44\" class=\"data row5 col44\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col45\" class=\"data row5 col45\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col46\" class=\"data row5 col46\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col47\" class=\"data row5 col47\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col48\" class=\"data row5 col48\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col49\" class=\"data row5 col49\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col50\" class=\"data row5 col50\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col51\" class=\"data row5 col51\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col52\" class=\"data row5 col52\" >0.00</td>\n",
       "      <td id=\"T_668c8_row5_col53\" class=\"data row5 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_668c8_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_668c8_row6_col0\" class=\"data row6 col0\" >0.51</td>\n",
       "      <td id=\"T_668c8_row6_col1\" class=\"data row6 col1\" >342.59</td>\n",
       "      <td id=\"T_668c8_row6_col2\" class=\"data row6 col2\" >64.76</td>\n",
       "      <td id=\"T_668c8_row6_col3\" class=\"data row6 col3\" >1.00</td>\n",
       "      <td id=\"T_668c8_row6_col4\" class=\"data row6 col4\" >5.00</td>\n",
       "      <td id=\"T_668c8_row6_col5\" class=\"data row6 col5\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col6\" class=\"data row6 col6\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col7\" class=\"data row6 col7\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col8\" class=\"data row6 col8\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col9\" class=\"data row6 col9\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col10\" class=\"data row6 col10\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col11\" class=\"data row6 col11\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col12\" class=\"data row6 col12\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col13\" class=\"data row6 col13\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col14\" class=\"data row6 col14\" >1.00</td>\n",
       "      <td id=\"T_668c8_row6_col15\" class=\"data row6 col15\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col16\" class=\"data row6 col16\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col17\" class=\"data row6 col17\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col18\" class=\"data row6 col18\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col19\" class=\"data row6 col19\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col20\" class=\"data row6 col20\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col21\" class=\"data row6 col21\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col22\" class=\"data row6 col22\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col23\" class=\"data row6 col23\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col24\" class=\"data row6 col24\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col25\" class=\"data row6 col25\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col26\" class=\"data row6 col26\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col27\" class=\"data row6 col27\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col28\" class=\"data row6 col28\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col29\" class=\"data row6 col29\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col30\" class=\"data row6 col30\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col31\" class=\"data row6 col31\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col32\" class=\"data row6 col32\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col33\" class=\"data row6 col33\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col34\" class=\"data row6 col34\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col35\" class=\"data row6 col35\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col36\" class=\"data row6 col36\" >1.00</td>\n",
       "      <td id=\"T_668c8_row6_col37\" class=\"data row6 col37\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col38\" class=\"data row6 col38\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col39\" class=\"data row6 col39\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col40\" class=\"data row6 col40\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col41\" class=\"data row6 col41\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col42\" class=\"data row6 col42\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col43\" class=\"data row6 col43\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col44\" class=\"data row6 col44\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col45\" class=\"data row6 col45\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col46\" class=\"data row6 col46\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col47\" class=\"data row6 col47\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col48\" class=\"data row6 col48\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col49\" class=\"data row6 col49\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col50\" class=\"data row6 col50\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col51\" class=\"data row6 col51\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col52\" class=\"data row6 col52\" >0.00</td>\n",
       "      <td id=\"T_668c8_row6_col53\" class=\"data row6 col53\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_668c8_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_668c8_row7_col0\" class=\"data row7 col0\" >0.95</td>\n",
       "      <td id=\"T_668c8_row7_col1\" class=\"data row7 col1\" >453.15</td>\n",
       "      <td id=\"T_668c8_row7_col2\" class=\"data row7 col2\" >499.90</td>\n",
       "      <td id=\"T_668c8_row7_col3\" class=\"data row7 col3\" >7.00</td>\n",
       "      <td id=\"T_668c8_row7_col4\" class=\"data row7 col4\" >28.00</td>\n",
       "      <td id=\"T_668c8_row7_col5\" class=\"data row7 col5\" >3.00</td>\n",
       "      <td id=\"T_668c8_row7_col6\" class=\"data row7 col6\" >2.00</td>\n",
       "      <td id=\"T_668c8_row7_col7\" class=\"data row7 col7\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col8\" class=\"data row7 col8\" >5.00</td>\n",
       "      <td id=\"T_668c8_row7_col9\" class=\"data row7 col9\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col10\" class=\"data row7 col10\" >3.00</td>\n",
       "      <td id=\"T_668c8_row7_col11\" class=\"data row7 col11\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col12\" class=\"data row7 col12\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col13\" class=\"data row7 col13\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col14\" class=\"data row7 col14\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col15\" class=\"data row7 col15\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col16\" class=\"data row7 col16\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col17\" class=\"data row7 col17\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col18\" class=\"data row7 col18\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col19\" class=\"data row7 col19\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col20\" class=\"data row7 col20\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col21\" class=\"data row7 col21\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col22\" class=\"data row7 col22\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col23\" class=\"data row7 col23\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col24\" class=\"data row7 col24\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col25\" class=\"data row7 col25\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col26\" class=\"data row7 col26\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col27\" class=\"data row7 col27\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col28\" class=\"data row7 col28\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col29\" class=\"data row7 col29\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col30\" class=\"data row7 col30\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col31\" class=\"data row7 col31\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col32\" class=\"data row7 col32\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col33\" class=\"data row7 col33\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col34\" class=\"data row7 col34\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col35\" class=\"data row7 col35\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col36\" class=\"data row7 col36\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col37\" class=\"data row7 col37\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col38\" class=\"data row7 col38\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col39\" class=\"data row7 col39\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col40\" class=\"data row7 col40\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col41\" class=\"data row7 col41\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col42\" class=\"data row7 col42\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col43\" class=\"data row7 col43\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col44\" class=\"data row7 col44\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col45\" class=\"data row7 col45\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col46\" class=\"data row7 col46\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col47\" class=\"data row7 col47\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col48\" class=\"data row7 col48\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col49\" class=\"data row7 col49\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col50\" class=\"data row7 col50\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col51\" class=\"data row7 col51\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col52\" class=\"data row7 col52\" >1.00</td>\n",
       "      <td id=\"T_668c8_row7_col53\" class=\"data row7 col53\" >1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f630c790eb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataCO2f='CO2-data'+'/'+'dataCO2.csv'\n",
    "dataCO2=pd.read_csv(dataCO2f,sep=\";\",header=0)\n",
    "display(dataCO2)\n",
    "# describe() generates descriptive statistics\n",
    "display(dataCO2.describe().style.format(\"{0:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf1e66-0756-47f2-928d-d0d578613bba",
   "metadata": {},
   "source": [
    "## 2. Assessment of the stability of the original ML algorithm of Song *et al*. by *K*-fold cross validation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b3ba1f-2f90-441f-b931-28bc9e64748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# separation of the data set into two subsets: (1) training of the ANN & (2) test of the ANN\n",
    "# library used: pandas\n",
    "xdata = dataCO2.drop(['IL','cation','anion','x_CO2'],axis=1)\n",
    "ydata = dataCO2['x_CO2']\n",
    "\n",
    "#######################################################################################\n",
    "# ANN: 1 input layer (53 neurons) / 2 hidden layers (20 and 7 neurons) / 1 output layer (1 neuron) \n",
    "# library used: keras\n",
    "\n",
    "def defANN(shape,acthL):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name='iLayer'))\n",
    "    model.add(keras.layers.Dense(7, activation=acthL, name='hLayer'))\n",
    "    model.add(keras.layers.Dense(1, name='oLayer'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model\n",
    "\n",
    "acthL='tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a3138b7-5dc7-4804-93cb-caab9887319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[91mFold 0\u001b[0m\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-19 11:42:08.805539: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 1s 3ms/step - loss: 0.0601 - mae: 0.1820 - mse: 0.0601 - val_loss: 0.0281 - val_mae: 0.1304 - val_mse: 0.0281\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0241 - mae: 0.1210 - mse: 0.0241 - val_loss: 0.0214 - val_mae: 0.1160 - val_mse: 0.0214\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0208 - mae: 0.1150 - mse: 0.0208 - val_loss: 0.0199 - val_mae: 0.1112 - val_mse: 0.0199\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0163 - mae: 0.1007 - mse: 0.0163 - val_loss: 0.0131 - val_mae: 0.0889 - val_mse: 0.0131\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0110 - mae: 0.0790 - mse: 0.0110 - val_loss: 0.0097 - val_mae: 0.0740 - val_mse: 0.0097\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0095 - mae: 0.0733 - mse: 0.0095 - val_loss: 0.0090 - val_mae: 0.0709 - val_mse: 0.0090\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0687 - mse: 0.0086 - val_loss: 0.0087 - val_mae: 0.0709 - val_mse: 0.0087\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0079 - mae: 0.0655 - mse: 0.0079 - val_loss: 0.0082 - val_mae: 0.0671 - val_mse: 0.0082\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0075 - mae: 0.0639 - mse: 0.0075 - val_loss: 0.0078 - val_mae: 0.0653 - val_mse: 0.0078\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0068 - mae: 0.0606 - mse: 0.0068 - val_loss: 0.0074 - val_mae: 0.0614 - val_mse: 0.0074\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0063 - mae: 0.0583 - mse: 0.0063 - val_loss: 0.0061 - val_mae: 0.0558 - val_mse: 0.0061\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0059 - mae: 0.0565 - mse: 0.0059 - val_loss: 0.0061 - val_mae: 0.0556 - val_mse: 0.0061\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0550 - mse: 0.0056 - val_loss: 0.0070 - val_mae: 0.0638 - val_mse: 0.0070\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0056 - mae: 0.0560 - mse: 0.0056 - val_loss: 0.0051 - val_mae: 0.0519 - val_mse: 0.0051\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0047 - mae: 0.0513 - mse: 0.0047 - val_loss: 0.0040 - val_mae: 0.0464 - val_mse: 0.0040\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0046 - mae: 0.0508 - mse: 0.0046 - val_loss: 0.0041 - val_mae: 0.0492 - val_mse: 0.0041\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0048 - mae: 0.0528 - mse: 0.0048 - val_loss: 0.0051 - val_mae: 0.0558 - val_mse: 0.0051\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0039 - mae: 0.0470 - mse: 0.0039 - val_loss: 0.0032 - val_mae: 0.0420 - val_mse: 0.0032\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0038 - mae: 0.0466 - mse: 0.0038 - val_loss: 0.0034 - val_mae: 0.0440 - val_mse: 0.0034\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0447 - mse: 0.0035 - val_loss: 0.0037 - val_mae: 0.0480 - val_mse: 0.0037\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0467 - mse: 0.0037 - val_loss: 0.0038 - val_mae: 0.0490 - val_mse: 0.0038\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0037 - mae: 0.0465 - mse: 0.0037 - val_loss: 0.0052 - val_mae: 0.0573 - val_mse: 0.0052\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0038 - mae: 0.0471 - mse: 0.0038 - val_loss: 0.0060 - val_mae: 0.0642 - val_mse: 0.0060\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0460 - mse: 0.0036 - val_loss: 0.0031 - val_mae: 0.0407 - val_mse: 0.0031\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0437 - mse: 0.0033 - val_loss: 0.0029 - val_mae: 0.0398 - val_mse: 0.0029\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0442 - mse: 0.0034 - val_loss: 0.0036 - val_mae: 0.0463 - val_mse: 0.0036\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0452 - mse: 0.0035 - val_loss: 0.0085 - val_mae: 0.0784 - val_mse: 0.0085\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0447 - mse: 0.0034 - val_loss: 0.0031 - val_mae: 0.0425 - val_mse: 0.0031\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0432 - mse: 0.0032 - val_loss: 0.0035 - val_mae: 0.0477 - val_mse: 0.0035\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0442 - mse: 0.0034 - val_loss: 0.0028 - val_mae: 0.0400 - val_mse: 0.0028\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0451 - mse: 0.0035 - val_loss: 0.0043 - val_mae: 0.0536 - val_mse: 0.0043\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0432 - mse: 0.0032 - val_loss: 0.0028 - val_mae: 0.0388 - val_mse: 0.0028\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0035 - mae: 0.0452 - mse: 0.0035 - val_loss: 0.0048 - val_mae: 0.0551 - val_mse: 0.0048\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0429 - mse: 0.0032 - val_loss: 0.0028 - val_mae: 0.0405 - val_mse: 0.0028\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0438 - mse: 0.0033 - val_loss: 0.0037 - val_mae: 0.0497 - val_mse: 0.0037\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0436 - mse: 0.0033 - val_loss: 0.0036 - val_mae: 0.0481 - val_mse: 0.0036\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0431 - mse: 0.0032 - val_loss: 0.0032 - val_mae: 0.0431 - val_mse: 0.0032\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0430 - mse: 0.0032 - val_loss: 0.0028 - val_mae: 0.0404 - val_mse: 0.0028\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0429 - mse: 0.0032 - val_loss: 0.0026 - val_mae: 0.0380 - val_mse: 0.0026\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0435 - mse: 0.0033 - val_loss: 0.0032 - val_mae: 0.0422 - val_mse: 0.0032\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0432 - mse: 0.0032 - val_loss: 0.0054 - val_mae: 0.0586 - val_mse: 0.0054\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0428 - mse: 0.0032 - val_loss: 0.0027 - val_mae: 0.0380 - val_mse: 0.0027\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0418 - mse: 0.0030 - val_loss: 0.0028 - val_mae: 0.0385 - val_mse: 0.0028\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0428 - mse: 0.0032 - val_loss: 0.0033 - val_mae: 0.0428 - val_mse: 0.0033\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0033 - mae: 0.0434 - mse: 0.0033 - val_loss: 0.0031 - val_mae: 0.0410 - val_mse: 0.0031\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0445 - mse: 0.0034 - val_loss: 0.0028 - val_mae: 0.0406 - val_mse: 0.0028\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0031 - mae: 0.0427 - mse: 0.0031 - val_loss: 0.0035 - val_mae: 0.0438 - val_mse: 0.0035\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0411 - mse: 0.0030 - val_loss: 0.0032 - val_mae: 0.0423 - val_mse: 0.0032\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0032 - mae: 0.0431 - mse: 0.0032 - val_loss: 0.0045 - val_mae: 0.0549 - val_mse: 0.0045\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0034 - mae: 0.0444 - mse: 0.0034 - val_loss: 0.0032 - val_mae: 0.0415 - val_mse: 0.0032\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0036 - mae: 0.0453 - mse: 0.0036 - val_loss: 0.0030 - val_mae: 0.0402 - val_mse: 0.0030\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0416 - mse: 0.0030 - val_loss: 0.0050 - val_mae: 0.0565 - val_mse: 0.0050\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0029 - mae: 0.0403 - mse: 0.0029 - val_loss: 0.0027 - val_mae: 0.0385 - val_mse: 0.0027\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0030 - mae: 0.0413 - mse: 0.0030 - val_loss: 0.0029 - val_mae: 0.0415 - val_mse: 0.0029\n",
      "Epoch 54: early stopping\n",
      "253/253 [==============================] - 0s 1ms/step\n",
      "64/64 [==============================] - 0s 1ms/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  0.008349336265144076    std:  0.05194799021838361    MAE:  0.040170277584926826     R2:  0.9770606017186803\n",
      "Test. mean:  0.006657489523868538    std:  0.053433636869045495    MAE:  0.04148075706505125     R2:  0.9764926667028677\n",
      "\u001b[1m\u001b[91mFold 1\u001b[0m\n",
      "Epoch 1/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0901 - mae: 0.2200 - mse: 0.0901 - val_loss: 0.0565 - val_mae: 0.2043 - val_mse: 0.0565\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0555 - val_mae: 0.2010 - val_mse: 0.0555\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2014 - mse: 0.0562 - val_loss: 0.0553 - val_mae: 0.1992 - val_mse: 0.0553\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0559 - mae: 0.2009 - mse: 0.0559 - val_loss: 0.0562 - val_mae: 0.2039 - val_mse: 0.0562\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2014 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.1988 - val_mse: 0.0553\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.2002 - val_mse: 0.0553\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0554 - val_mae: 0.1982 - val_mse: 0.0554\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.2003 - val_mse: 0.0553\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0554 - val_mae: 0.2006 - val_mse: 0.0554\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2010 - mse: 0.0560 - val_loss: 0.0559 - val_mae: 0.2028 - val_mse: 0.0559\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0554 - val_mae: 0.2008 - val_mse: 0.0554\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2012 - mse: 0.0560 - val_loss: 0.0554 - val_mae: 0.1982 - val_mse: 0.0554\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2011 - mse: 0.0561 - val_loss: 0.0554 - val_mae: 0.2006 - val_mse: 0.0554\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2013 - mse: 0.0561 - val_loss: 0.0557 - val_mae: 0.2022 - val_mse: 0.0557\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0556 - val_mae: 0.2018 - val_mse: 0.0556\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0561 - mae: 0.2012 - mse: 0.0561 - val_loss: 0.0553 - val_mae: 0.2000 - val_mse: 0.0553\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2010 - mse: 0.0560 - val_loss: 0.0569 - val_mae: 0.2058 - val_mse: 0.0569\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0560 - mae: 0.2012 - mse: 0.0560 - val_loss: 0.0553 - val_mae: 0.1984 - val_mse: 0.0553\n",
      "Epoch 18: early stopping\n",
      "253/253 [==============================] - 0s 1ms/step\n",
      "64/64 [==============================] - 0s 1ms/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  -0.014777712386978092    std:  0.23641063388624725    MAE:  0.199733478524057     R2:  -0.02315808110513313\n",
      "Test. mean:  -0.00796515588103411    std:  0.23508384329857276    MAE:  0.19839801175483252     R2:  -0.05201540086297599\n",
      "\u001b[1m\u001b[91mFold 2\u001b[0m\n",
      "Epoch 1/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.3282 - mae: 0.4519 - mse: 0.3282 - val_loss: 0.0544 - val_mae: 0.1994 - val_mse: 0.0544\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2018 - mse: 0.0562 - val_loss: 0.0542 - val_mae: 0.1977 - val_mse: 0.0542\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2016 - mse: 0.0562 - val_loss: 0.0542 - val_mae: 0.1969 - val_mse: 0.0542\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2016 - mse: 0.0562 - val_loss: 0.0542 - val_mae: 0.1969 - val_mse: 0.0542\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2015 - mse: 0.0562 - val_loss: 0.0542 - val_mae: 0.1970 - val_mse: 0.0542\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2015 - mse: 0.0562 - val_loss: 0.0542 - val_mae: 0.1969 - val_mse: 0.0542\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2015 - mse: 0.0562 - val_loss: 0.0544 - val_mae: 0.1964 - val_mse: 0.0544\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2016 - mse: 0.0563 - val_loss: 0.0542 - val_mae: 0.1978 - val_mse: 0.0542\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2016 - mse: 0.0563 - val_loss: 0.0542 - val_mae: 0.1975 - val_mse: 0.0542\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2015 - mse: 0.0562 - val_loss: 0.0545 - val_mae: 0.1961 - val_mse: 0.0545\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2016 - mse: 0.0563 - val_loss: 0.0542 - val_mae: 0.1976 - val_mse: 0.0542\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0562 - mae: 0.2017 - mse: 0.0562 - val_loss: 0.0547 - val_mae: 0.1957 - val_mse: 0.0547\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2015 - mse: 0.0563 - val_loss: 0.0544 - val_mae: 0.1962 - val_mse: 0.0544\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2016 - mse: 0.0563 - val_loss: 0.0542 - val_mae: 0.1974 - val_mse: 0.0542\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2017 - mse: 0.0563 - val_loss: 0.0543 - val_mae: 0.1967 - val_mse: 0.0543\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2017 - mse: 0.0563 - val_loss: 0.0542 - val_mae: 0.1973 - val_mse: 0.0542\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2016 - mse: 0.0563 - val_loss: 0.0543 - val_mae: 0.1984 - val_mse: 0.0543\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0564 - mae: 0.2018 - mse: 0.0564 - val_loss: 0.0548 - val_mae: 0.1957 - val_mse: 0.0548\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2016 - mse: 0.0563 - val_loss: 0.0543 - val_mae: 0.1985 - val_mse: 0.0543\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2017 - mse: 0.0563 - val_loss: 0.0546 - val_mae: 0.1959 - val_mse: 0.0546\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0565 - mae: 0.2020 - mse: 0.0565 - val_loss: 0.0543 - val_mae: 0.1983 - val_mse: 0.0543\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2016 - mse: 0.0563 - val_loss: 0.0543 - val_mae: 0.1982 - val_mse: 0.0543\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0564 - mae: 0.2018 - mse: 0.0564 - val_loss: 0.0545 - val_mae: 0.1960 - val_mse: 0.0545\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0563 - mae: 0.2015 - mse: 0.0563 - val_loss: 0.0544 - val_mae: 0.1962 - val_mse: 0.0544\n",
      "Epoch 24: early stopping\n",
      "253/253 [==============================] - 0s 1ms/step\n",
      "64/64 [==============================] - 0s 1ms/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  -0.009013340602912794    std:  0.2369730512419525    MAE:  0.2005994097839987     R2:  nan\n",
      "Test. mean:  -0.01385280806945367    std:  0.23284230935614172    MAE:  0.1962378201955791     R2:  nan\n",
      "\u001b[1m\u001b[91mFold 3\u001b[0m\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romuald/anaconda3/envs/ML/lib/python3.9/site-packages/numpy/lib/function_base.py:2853: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/home/romuald/anaconda3/envs/ML/lib/python3.9/site-packages/numpy/lib/function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 1s 2ms/step - loss: 0.2137 - mae: 0.2971 - mse: 0.2137 - val_loss: 0.0578 - val_mae: 0.2047 - val_mse: 0.0578\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0548 - mae: 0.1984 - mse: 0.0548 - val_loss: 0.0535 - val_mae: 0.1985 - val_mse: 0.0535\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0325 - mae: 0.1484 - mse: 0.0325 - val_loss: 0.0240 - val_mae: 0.1281 - val_mse: 0.0240\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0221 - mae: 0.1204 - mse: 0.0221 - val_loss: 0.0218 - val_mae: 0.1198 - val_mse: 0.0218\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0215 - mae: 0.1183 - mse: 0.0215 - val_loss: 0.0213 - val_mae: 0.1178 - val_mse: 0.0213\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0210 - mae: 0.1166 - mse: 0.0210 - val_loss: 0.0206 - val_mae: 0.1157 - val_mse: 0.0206\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0205 - mae: 0.1149 - mse: 0.0205 - val_loss: 0.0196 - val_mae: 0.1121 - val_mse: 0.0196\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0193 - mae: 0.1096 - mse: 0.0193 - val_loss: 0.0180 - val_mae: 0.1042 - val_mse: 0.0180\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0175 - mae: 0.1027 - mse: 0.0175 - val_loss: 0.0161 - val_mae: 0.0976 - val_mse: 0.0161\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0154 - mae: 0.0955 - mse: 0.0154 - val_loss: 0.0152 - val_mae: 0.0949 - val_mse: 0.0152\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0124 - mae: 0.0858 - mse: 0.0124 - val_loss: 0.0118 - val_mae: 0.0853 - val_mse: 0.0118\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0107 - mae: 0.0804 - mse: 0.0107 - val_loss: 0.0104 - val_mae: 0.0777 - val_mse: 0.0104\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0099 - mae: 0.0772 - mse: 0.0099 - val_loss: 0.0100 - val_mae: 0.0759 - val_mse: 0.0100\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0096 - mae: 0.0760 - mse: 0.0096 - val_loss: 0.0112 - val_mae: 0.0816 - val_mse: 0.0112\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0093 - mae: 0.0745 - mse: 0.0093 - val_loss: 0.0095 - val_mae: 0.0743 - val_mse: 0.0095\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0095 - mae: 0.0753 - mse: 0.0095 - val_loss: 0.0094 - val_mae: 0.0732 - val_mse: 0.0094\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0091 - mae: 0.0737 - mse: 0.0091 - val_loss: 0.0094 - val_mae: 0.0751 - val_mse: 0.0094\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0726 - mse: 0.0090 - val_loss: 0.0101 - val_mae: 0.0758 - val_mse: 0.0101\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0721 - mse: 0.0088 - val_loss: 0.0091 - val_mae: 0.0739 - val_mse: 0.0091\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0725 - mse: 0.0089 - val_loss: 0.0090 - val_mae: 0.0711 - val_mse: 0.0090\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0730 - mse: 0.0090 - val_loss: 0.0096 - val_mae: 0.0735 - val_mse: 0.0096\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0718 - mse: 0.0087 - val_loss: 0.0095 - val_mae: 0.0780 - val_mse: 0.0095\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0720 - mse: 0.0088 - val_loss: 0.0087 - val_mae: 0.0714 - val_mse: 0.0087\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0721 - mse: 0.0088 - val_loss: 0.0094 - val_mae: 0.0727 - val_mse: 0.0094\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0709 - mse: 0.0086 - val_loss: 0.0094 - val_mae: 0.0773 - val_mse: 0.0094\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0711 - mse: 0.0086 - val_loss: 0.0093 - val_mae: 0.0769 - val_mse: 0.0093\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0711 - mse: 0.0086 - val_loss: 0.0086 - val_mae: 0.0707 - val_mse: 0.0086\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0701 - mse: 0.0084 - val_loss: 0.0087 - val_mae: 0.0699 - val_mse: 0.0087\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0697 - mse: 0.0084 - val_loss: 0.0087 - val_mae: 0.0704 - val_mse: 0.0087\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0709 - mse: 0.0086 - val_loss: 0.0094 - val_mae: 0.0713 - val_mse: 0.0094\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0696 - mse: 0.0083 - val_loss: 0.0086 - val_mae: 0.0697 - val_mse: 0.0086\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0705 - mse: 0.0085 - val_loss: 0.0090 - val_mae: 0.0752 - val_mse: 0.0090\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0696 - mse: 0.0083 - val_loss: 0.0090 - val_mae: 0.0738 - val_mse: 0.0090\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0694 - mse: 0.0083 - val_loss: 0.0086 - val_mae: 0.0694 - val_mse: 0.0086\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0704 - mse: 0.0085 - val_loss: 0.0086 - val_mae: 0.0708 - val_mse: 0.0086\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0695 - mse: 0.0083 - val_loss: 0.0087 - val_mae: 0.0725 - val_mse: 0.0087\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0705 - mse: 0.0085 - val_loss: 0.0085 - val_mae: 0.0696 - val_mse: 0.0085\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0703 - mse: 0.0084 - val_loss: 0.0090 - val_mae: 0.0707 - val_mse: 0.0090\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0699 - mse: 0.0084 - val_loss: 0.0089 - val_mae: 0.0710 - val_mse: 0.0089\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0696 - mse: 0.0083 - val_loss: 0.0088 - val_mae: 0.0740 - val_mse: 0.0088\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0701 - mse: 0.0084 - val_loss: 0.0095 - val_mae: 0.0726 - val_mse: 0.0095\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0699 - mse: 0.0083 - val_loss: 0.0154 - val_mae: 0.1034 - val_mse: 0.0154\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0704 - mse: 0.0085 - val_loss: 0.0090 - val_mae: 0.0705 - val_mse: 0.0090\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0702 - mse: 0.0084 - val_loss: 0.0087 - val_mae: 0.0699 - val_mse: 0.0087\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0703 - mse: 0.0083 - val_loss: 0.0086 - val_mae: 0.0698 - val_mse: 0.0086\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0709 - mse: 0.0085 - val_loss: 0.0091 - val_mae: 0.0707 - val_mse: 0.0091\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0727 - mse: 0.0089 - val_loss: 0.0098 - val_mae: 0.0734 - val_mse: 0.0098\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0697 - mse: 0.0083 - val_loss: 0.0092 - val_mae: 0.0774 - val_mse: 0.0092\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0712 - mse: 0.0086 - val_loss: 0.0092 - val_mae: 0.0713 - val_mse: 0.0092\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0696 - mse: 0.0083 - val_loss: 0.0086 - val_mae: 0.0712 - val_mse: 0.0086\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0699 - mse: 0.0084 - val_loss: 0.0098 - val_mae: 0.0734 - val_mse: 0.0098\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0699 - mse: 0.0083 - val_loss: 0.0093 - val_mae: 0.0717 - val_mse: 0.0093\n",
      "Epoch 52: early stopping\n",
      "253/253 [==============================] - 0s 1ms/step\n",
      "64/64 [==============================] - 0s 1ms/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  0.025145954692258384    std:  0.08940704699643269    MAE:  0.06939060467180895     R2:  0.9254799345025001\n",
      "Test. mean:  0.026128925451187585    std:  0.09292855904732372    MAE:  0.0717198823221656     R2:  0.9225487909461826\n",
      "\u001b[1m\u001b[91mFold 4\u001b[0m\n",
      "Epoch 1/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.5566 - mae: 0.5770 - mse: 0.5566 - val_loss: 0.0537 - val_mae: 0.1876 - val_mse: 0.0537\n",
      "Epoch 2/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0494 - mae: 0.1847 - mse: 0.0494 - val_loss: 0.0471 - val_mae: 0.1796 - val_mse: 0.0471\n",
      "Epoch 3/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0464 - mae: 0.1786 - mse: 0.0464 - val_loss: 0.0426 - val_mae: 0.1699 - val_mse: 0.0426\n",
      "Epoch 4/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0376 - mae: 0.1582 - mse: 0.0376 - val_loss: 0.0299 - val_mae: 0.1391 - val_mse: 0.0299\n",
      "Epoch 5/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0251 - mae: 0.1278 - mse: 0.0251 - val_loss: 0.0222 - val_mae: 0.1203 - val_mse: 0.0222\n",
      "Epoch 6/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0223 - mae: 0.1209 - mse: 0.0223 - val_loss: 0.0218 - val_mae: 0.1198 - val_mse: 0.0218\n",
      "Epoch 7/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0218 - mae: 0.1201 - mse: 0.0218 - val_loss: 0.0217 - val_mae: 0.1190 - val_mse: 0.0217\n",
      "Epoch 8/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0217 - mae: 0.1194 - mse: 0.0217 - val_loss: 0.0214 - val_mae: 0.1185 - val_mse: 0.0214\n",
      "Epoch 9/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0215 - mae: 0.1185 - mse: 0.0215 - val_loss: 0.0217 - val_mae: 0.1183 - val_mse: 0.0217\n",
      "Epoch 10/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0214 - mae: 0.1186 - mse: 0.0214 - val_loss: 0.0212 - val_mae: 0.1178 - val_mse: 0.0212\n",
      "Epoch 11/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0213 - mae: 0.1185 - mse: 0.0213 - val_loss: 0.0215 - val_mae: 0.1191 - val_mse: 0.0215\n",
      "Epoch 12/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0211 - mae: 0.1174 - mse: 0.0211 - val_loss: 0.0209 - val_mae: 0.1173 - val_mse: 0.0209\n",
      "Epoch 13/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0210 - mae: 0.1175 - mse: 0.0210 - val_loss: 0.0204 - val_mae: 0.1162 - val_mse: 0.0204\n",
      "Epoch 14/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0208 - mae: 0.1167 - mse: 0.0208 - val_loss: 0.0205 - val_mae: 0.1161 - val_mse: 0.0205\n",
      "Epoch 15/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0203 - mae: 0.1150 - mse: 0.0203 - val_loss: 0.0206 - val_mae: 0.1156 - val_mse: 0.0206\n",
      "Epoch 16/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0199 - mae: 0.1140 - mse: 0.0199 - val_loss: 0.0197 - val_mae: 0.1141 - val_mse: 0.0197\n",
      "Epoch 17/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0194 - mae: 0.1127 - mse: 0.0194 - val_loss: 0.0190 - val_mae: 0.1108 - val_mse: 0.0190\n",
      "Epoch 18/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0185 - mae: 0.1093 - mse: 0.0185 - val_loss: 0.0177 - val_mae: 0.1065 - val_mse: 0.0177\n",
      "Epoch 19/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0171 - mae: 0.1047 - mse: 0.0171 - val_loss: 0.0154 - val_mae: 0.0993 - val_mse: 0.0154\n",
      "Epoch 20/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0156 - mae: 0.0996 - mse: 0.0156 - val_loss: 0.0148 - val_mae: 0.0961 - val_mse: 0.0148\n",
      "Epoch 21/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0143 - mae: 0.0950 - mse: 0.0143 - val_loss: 0.0136 - val_mae: 0.0916 - val_mse: 0.0136\n",
      "Epoch 22/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0133 - mae: 0.0916 - mse: 0.0133 - val_loss: 0.0136 - val_mae: 0.0927 - val_mse: 0.0136\n",
      "Epoch 23/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0123 - mae: 0.0875 - mse: 0.0123 - val_loss: 0.0114 - val_mae: 0.0826 - val_mse: 0.0114\n",
      "Epoch 24/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0115 - mae: 0.0844 - mse: 0.0115 - val_loss: 0.0110 - val_mae: 0.0812 - val_mse: 0.0110\n",
      "Epoch 25/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0110 - mae: 0.0823 - mse: 0.0110 - val_loss: 0.0109 - val_mae: 0.0826 - val_mse: 0.0109\n",
      "Epoch 26/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0103 - mae: 0.0791 - mse: 0.0103 - val_loss: 0.0096 - val_mae: 0.0754 - val_mse: 0.0096\n",
      "Epoch 27/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0099 - mae: 0.0773 - mse: 0.0099 - val_loss: 0.0094 - val_mae: 0.0741 - val_mse: 0.0094\n",
      "Epoch 28/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0098 - mae: 0.0767 - mse: 0.0098 - val_loss: 0.0093 - val_mae: 0.0731 - val_mse: 0.0093\n",
      "Epoch 29/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0097 - mae: 0.0764 - mse: 0.0097 - val_loss: 0.0118 - val_mae: 0.0867 - val_mse: 0.0118\n",
      "Epoch 30/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0096 - mae: 0.0763 - mse: 0.0096 - val_loss: 0.0089 - val_mae: 0.0725 - val_mse: 0.0089\n",
      "Epoch 31/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0093 - mae: 0.0748 - mse: 0.0093 - val_loss: 0.0088 - val_mae: 0.0724 - val_mse: 0.0088\n",
      "Epoch 32/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0091 - mae: 0.0737 - mse: 0.0091 - val_loss: 0.0095 - val_mae: 0.0745 - val_mse: 0.0095\n",
      "Epoch 33/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0092 - mae: 0.0747 - mse: 0.0092 - val_loss: 0.0087 - val_mae: 0.0720 - val_mse: 0.0087\n",
      "Epoch 34/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0733 - mse: 0.0090 - val_loss: 0.0090 - val_mae: 0.0751 - val_mse: 0.0090\n",
      "Epoch 35/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0089 - mae: 0.0731 - mse: 0.0089 - val_loss: 0.0086 - val_mae: 0.0703 - val_mse: 0.0086\n",
      "Epoch 36/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0731 - mse: 0.0090 - val_loss: 0.0090 - val_mae: 0.0749 - val_mse: 0.0090\n",
      "Epoch 37/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0719 - mse: 0.0087 - val_loss: 0.0088 - val_mae: 0.0740 - val_mse: 0.0088\n",
      "Epoch 38/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0090 - mae: 0.0733 - mse: 0.0090 - val_loss: 0.0085 - val_mae: 0.0708 - val_mse: 0.0085\n",
      "Epoch 39/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0719 - mse: 0.0087 - val_loss: 0.0087 - val_mae: 0.0725 - val_mse: 0.0087\n",
      "Epoch 40/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0716 - mse: 0.0087 - val_loss: 0.0086 - val_mae: 0.0699 - val_mse: 0.0086\n",
      "Epoch 41/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0715 - mse: 0.0086 - val_loss: 0.0086 - val_mae: 0.0700 - val_mse: 0.0086\n",
      "Epoch 42/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0716 - mse: 0.0087 - val_loss: 0.0086 - val_mae: 0.0700 - val_mse: 0.0086\n",
      "Epoch 43/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0717 - mse: 0.0087 - val_loss: 0.0086 - val_mae: 0.0721 - val_mse: 0.0086\n",
      "Epoch 44/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0716 - mse: 0.0086 - val_loss: 0.0083 - val_mae: 0.0699 - val_mse: 0.0083\n",
      "Epoch 45/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0710 - mse: 0.0086 - val_loss: 0.0086 - val_mae: 0.0724 - val_mse: 0.0086\n",
      "Epoch 46/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0716 - mse: 0.0086 - val_loss: 0.0083 - val_mae: 0.0697 - val_mse: 0.0083\n",
      "Epoch 47/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0716 - mse: 0.0086 - val_loss: 0.0083 - val_mae: 0.0700 - val_mse: 0.0083\n",
      "Epoch 48/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0716 - mse: 0.0086 - val_loss: 0.0090 - val_mae: 0.0706 - val_mse: 0.0090\n",
      "Epoch 49/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0710 - mse: 0.0086 - val_loss: 0.0095 - val_mae: 0.0734 - val_mse: 0.0095\n",
      "Epoch 50/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0713 - mse: 0.0086 - val_loss: 0.0094 - val_mae: 0.0724 - val_mse: 0.0094\n",
      "Epoch 51/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0707 - mse: 0.0084 - val_loss: 0.0084 - val_mae: 0.0692 - val_mse: 0.0084\n",
      "Epoch 52/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0707 - mse: 0.0085 - val_loss: 0.0092 - val_mae: 0.0767 - val_mse: 0.0092\n",
      "Epoch 53/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0710 - mse: 0.0085 - val_loss: 0.0084 - val_mae: 0.0684 - val_mse: 0.0084\n",
      "Epoch 54/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0711 - mse: 0.0086 - val_loss: 0.0086 - val_mae: 0.0724 - val_mse: 0.0086\n",
      "Epoch 55/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0716 - mse: 0.0087 - val_loss: 0.0089 - val_mae: 0.0748 - val_mse: 0.0089\n",
      "Epoch 56/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0703 - mse: 0.0084 - val_loss: 0.0085 - val_mae: 0.0720 - val_mse: 0.0085\n",
      "Epoch 57/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0708 - mse: 0.0085 - val_loss: 0.0086 - val_mae: 0.0691 - val_mse: 0.0086\n",
      "Epoch 58/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0716 - mse: 0.0087 - val_loss: 0.0082 - val_mae: 0.0692 - val_mse: 0.0082\n",
      "Epoch 59/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0709 - mse: 0.0085 - val_loss: 0.0085 - val_mae: 0.0717 - val_mse: 0.0085\n",
      "Epoch 60/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0715 - mse: 0.0086 - val_loss: 0.0090 - val_mae: 0.0711 - val_mse: 0.0090\n",
      "Epoch 61/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0712 - mse: 0.0085 - val_loss: 0.0088 - val_mae: 0.0705 - val_mse: 0.0088\n",
      "Epoch 62/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0712 - mse: 0.0085 - val_loss: 0.0095 - val_mae: 0.0788 - val_mse: 0.0095\n",
      "Epoch 63/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0707 - mse: 0.0085 - val_loss: 0.0090 - val_mae: 0.0710 - val_mse: 0.0090\n",
      "Epoch 64/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0704 - mse: 0.0084 - val_loss: 0.0084 - val_mae: 0.0713 - val_mse: 0.0084\n",
      "Epoch 65/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0702 - mse: 0.0083 - val_loss: 0.0084 - val_mae: 0.0684 - val_mse: 0.0084\n",
      "Epoch 66/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0726 - mse: 0.0088 - val_loss: 0.0084 - val_mae: 0.0708 - val_mse: 0.0084\n",
      "Epoch 67/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0699 - mse: 0.0083 - val_loss: 0.0091 - val_mae: 0.0767 - val_mse: 0.0091\n",
      "Epoch 68/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0704 - mse: 0.0084 - val_loss: 0.0083 - val_mae: 0.0687 - val_mse: 0.0083\n",
      "Epoch 69/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0701 - mse: 0.0084 - val_loss: 0.0083 - val_mae: 0.0690 - val_mse: 0.0083\n",
      "Epoch 70/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0705 - mse: 0.0085 - val_loss: 0.0087 - val_mae: 0.0700 - val_mse: 0.0087\n",
      "Epoch 71/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0087 - mae: 0.0718 - mse: 0.0087 - val_loss: 0.0082 - val_mae: 0.0688 - val_mse: 0.0082\n",
      "Epoch 72/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0083 - mae: 0.0697 - mse: 0.0083 - val_loss: 0.0082 - val_mae: 0.0689 - val_mse: 0.0082\n",
      "Epoch 73/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0711 - mse: 0.0086 - val_loss: 0.0083 - val_mae: 0.0682 - val_mse: 0.0083\n",
      "Epoch 74/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0709 - mse: 0.0085 - val_loss: 0.0106 - val_mae: 0.0778 - val_mse: 0.0106\n",
      "Epoch 75/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0705 - mse: 0.0085 - val_loss: 0.0085 - val_mae: 0.0727 - val_mse: 0.0085\n",
      "Epoch 76/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0706 - mse: 0.0085 - val_loss: 0.0084 - val_mae: 0.0683 - val_mse: 0.0084\n",
      "Epoch 77/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0706 - mse: 0.0084 - val_loss: 0.0088 - val_mae: 0.0705 - val_mse: 0.0088\n",
      "Epoch 78/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0712 - mse: 0.0085 - val_loss: 0.0094 - val_mae: 0.0777 - val_mse: 0.0094\n",
      "Epoch 79/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0713 - mse: 0.0086 - val_loss: 0.0083 - val_mae: 0.0709 - val_mse: 0.0083\n",
      "Epoch 80/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0703 - mse: 0.0084 - val_loss: 0.0084 - val_mae: 0.0684 - val_mse: 0.0084\n",
      "Epoch 81/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0086 - mae: 0.0717 - mse: 0.0086 - val_loss: 0.0084 - val_mae: 0.0713 - val_mse: 0.0084\n",
      "Epoch 82/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0722 - mse: 0.0088 - val_loss: 0.0083 - val_mae: 0.0687 - val_mse: 0.0083\n",
      "Epoch 83/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0702 - mse: 0.0084 - val_loss: 0.0084 - val_mae: 0.0721 - val_mse: 0.0084\n",
      "Epoch 84/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0706 - mse: 0.0085 - val_loss: 0.0088 - val_mae: 0.0698 - val_mse: 0.0088\n",
      "Epoch 85/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0703 - mse: 0.0084 - val_loss: 0.0082 - val_mae: 0.0690 - val_mse: 0.0082\n",
      "Epoch 86/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0084 - mae: 0.0701 - mse: 0.0084 - val_loss: 0.0084 - val_mae: 0.0684 - val_mse: 0.0084\n",
      "Epoch 87/200\n",
      "324/324 [==============================] - 1s 2ms/step - loss: 0.0085 - mae: 0.0710 - mse: 0.0085 - val_loss: 0.0083 - val_mae: 0.0707 - val_mse: 0.0083\n",
      "Epoch 87: early stopping\n",
      "253/253 [==============================] - 0s 1ms/step\n",
      "64/64 [==============================] - 0s 1ms/step\n",
      "\n",
      "xCO2(predicted) - xCO2(actual)\n",
      "Train. mean:  -0.008255975250494648    std:  0.08948008082712083    MAE:  0.0700084828587737     R2:  0.9258428467517378\n",
      "Test. mean:  -0.0067702744482775    std:  0.09098247623394293    MAE:  0.07065150470475401     R2:  0.9212878460463413\n",
      "\n",
      "Duration :  00:02:35 594ms\n",
      "\u001b[1maverage MAE of the training set:\u001b[0m   0.12 +/- 0.07\n",
      "\u001b[1maverage MAE of the validation set:\u001b[0m 0.12 +/- 0.07\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#######################################################################################\n",
    "# optimization of the ANN\n",
    "# library used: keras and scikit learn for the KFold cross-validator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "VERBOSE = 1\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 25\n",
    "N_SPLIT = 5\n",
    "\n",
    "vID.chrono_start()\n",
    "\n",
    "# variables created to save at each iteration of the KFold process: the man error, the standard deviation, MAE, R2\n",
    "meantT=list()\n",
    "stdtT=list()\n",
    "MAEtT=list()\n",
    "R2tT=list()\n",
    "meanvT=list()\n",
    "stdvT=list()\n",
    "MAEvT=list()\n",
    "R2vT=list()\n",
    "\n",
    "kfold = KFold(n_splits=N_SPLIT,shuffle=True,random_state=42) # k-fold is here!\n",
    "#print(list(kfold.split(x_train,y_train)))\n",
    "\n",
    "j = 0 # Variable for keeping count of split we are executing\n",
    "# The KFold cv provides train/test indices to split data in train/test sets\n",
    "for train_idx, val_idx in list(kfold.split(xdata,ydata)):\n",
    "\n",
    "    x_train_cv = xdata.iloc[train_idx]\n",
    "    x_valid_cv = xdata.iloc[val_idx]\n",
    "    y_train_cv = ydata.iloc[train_idx]\n",
    "    y_valid_cv = ydata.iloc[val_idx]\n",
    "#    display(x_train_cv,x_valid_cv)\n",
    "# This part has been commented with respect to the original script\n",
    "    # scaler = preprocessing.StandardScaler()\n",
    "    # scaler.fit(x_train_cv.values)\n",
    "    # xt_scaled = scaler.transform(x_train_cv.values) #returns a numpy array\n",
    "    # xv_scaled = scaler.transform(x_valid_cv.values) #returns a numpy array\n",
    "    # x_train_cv = pd.DataFrame(xt_scaled, index=x_train_cv.index, columns=x_train_cv.columns)\n",
    "    # x_valid_cv = pd.DataFrame(xv_scaled, index=x_valid_cv.index, columns=x_valid_cv.columns)\n",
    "    # del xt_scaled, xv_scaled\n",
    "##############\n",
    "#    display(x_train_cv.describe().style.format(\"{0:.2f}\").set_caption(\"Training set after normalization (with scikit-learn):\"))\n",
    "#    display(x_valid_cv.describe().style.format(\"{0:.2f}\").set_caption(\"Validation set after normalization (with scikit-learn):\"))\n",
    "    print(f\"{color.BOLD}{color.RED}Fold {j}{color.OFF}\")\n",
    "    j+=1\n",
    "    ANNmodel=defANN( (53,), acthL )\n",
    "    ANNhistory = ANNmodel.fit(x_train_cv,\n",
    "                        y_train_cv,\n",
    "                        epochs          = EPOCHS,\n",
    "                        batch_size      = BATCH_SIZE,\n",
    "                        verbose         = VERBOSE,\n",
    "                        validation_data = (x_valid_cv, y_valid_cv),\n",
    "                        callbacks=[es])\n",
    "    ytrain_hat=ANNmodel.predict(x_train_cv)\n",
    "    yvalid_hat=ANNmodel.predict(x_valid_cv)\n",
    "    diffyt = ytrain_hat.ravel() - y_train_cv.ravel()\n",
    "    diffyv = yvalid_hat.ravel() - y_valid_cv.ravel()\n",
    "\n",
    "    print()\n",
    "    print(\"xCO2(predicted) - xCO2(actual)\")\n",
    "    print(\n",
    "          \"Train.\",\"mean: \", np.mean(diffyt),\n",
    "          \"   std: \", np.std(diffyt),\n",
    "          \"   MAE: \", np.average(abs(diffyt)),\n",
    "          \"    R2: \", np.corrcoef(y_train_cv.ravel(),ytrain_hat.ravel())[0,1]\n",
    "         )\n",
    "    print(\n",
    "          \"Test.\",\"mean: \", np.mean(diffyv),\n",
    "          \"   std: \", np.std(diffyv),\n",
    "          \"   MAE: \", np.average(abs(diffyv)),\n",
    "          \"    R2: \", np.corrcoef(y_valid_cv.ravel(),yvalid_hat.ravel())[0,1]\n",
    "         )\n",
    "    meantT.append(np.mean(diffyt))\n",
    "    meanvT.append(np.mean(diffyv))\n",
    "    stdtT.append(np.std(diffyt))\n",
    "    stdvT.append(np.std(diffyv))\n",
    "    MAEtT.append(np.average(abs(diffyt)))\n",
    "    MAEvT.append(np.average(abs(diffyv)))\n",
    "    R2tT.append(np.corrcoef(y_train_cv.ravel(),ytrain_hat.ravel())[0,1])\n",
    "    R2vT.append(np.corrcoef(y_valid_cv.ravel(),yvalid_hat.ravel())[0,1])\n",
    "    \n",
    "vID.chrono_show()\n",
    "\n",
    "#######################################################################################\n",
    "# accuracy of the ANN?\n",
    "# library used: numpy\n",
    "print(f\"{color.BOLD}average MAE of the training set:{color.OFF}   {np.mean(MAEtT):.2f} +/- {np.std(MAEtT):.2f}\")\n",
    "print(f\"{color.BOLD}average MAE of the validation set:{color.OFF} {np.mean(MAEvT):.2f} +/- {np.std(MAEvT):.2f}\")\n",
    "\n",
    "figCV, axCV = plt.subplots(1, 1)\n",
    "figCV.set_size_inches(12,5)\n",
    "axCV.errorbar(x=np.arange(len(meantT)), y=meantT, yerr=MAEtT, label='training sets', fmt='o-', capsize=10)\n",
    "axCV.errorbar(x=np.arange(len(meanvT))+0.1, y=meanvT, yerr=MAEvT, label='validation sets', fmt='o-', capsize=10)\n",
    "axCV.legend(loc='lower left', shadow=True, fontsize='14')\n",
    "axCV.set_xlabel('cross-validation k-values ',fontdict={'fontsize':16})\n",
    "axCV.set_ylabel('$\\hat{y}-y_{\\mathrm{actual}}$',fontdict={'fontsize':16})\n",
    "axCV.tick_params(labelsize = 14)\n",
    "plt.savefig('./CO2-images/KFold-cv-AppliedToSong_etal.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d639724-2e98-43dd-b6e5-7f0767cfb27d",
   "metadata": {},
   "source": [
    "<div class=\"warn\">\n",
    "You have probably found results similar to those reported in the following error plot:\n",
    "<p style=\"text-align: center\"><img width=\"650px\" src=\"./CO2-images/KFold-cv-AppliedToSong_etalK.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_ResultsSong\"></p>\n",
    "    <b>This error plot shows a bad performance of the original ML algorithm of Song <i>et al</i>. (<i>i.e.</i> without standardization of the data), with a strong variation of error bars.</b>\n",
    "    \n",
    "Either the authors did actually apply a standardization preprocessing and they forgot to mention it in the article, or they ran several optimization algorithms of the ANN until they found a seemingly performant one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1998a632",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**End at:** Thursday 30 June 2022, 12:35:06  \n",
       "**Duration:** 00:05:04 872ms"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<p style=\"text-align: center\"><img width=\"800px\" src=\"./svg/logoEnd.svg\" style=\"margin-left:auto; margin-right:auto\"/></p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vID.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb92c04-23d1-4c52-885d-fa8e5eee43c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
