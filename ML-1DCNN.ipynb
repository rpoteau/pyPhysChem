{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10024ca9-7e94-4f5c-a982-b4537b919d76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "cwd0 = './config/'\n",
    "sys.path.append(cwd0)\n",
    "\n",
    "import visualID_Eng as vID\n",
    "from visualID_Eng import color, fg, bg, hl\n",
    "vID.init(cwd0)\n",
    "import tools4pyPhysChem as t4pPC\n",
    "\n",
    "#cancel the \"last show-up\" behaviour of Jupyter notebooks\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "__author__ = \"romuald.poteau@utoulouse.fr\"\n",
    "__version__ = \"20250929\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e4b9b",
   "metadata": {},
   "source": [
    "# 1D Convolutional Neural Networks (CNN)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Convolution in mathematics and physics\n",
    "\n",
    "<div class=\"intro\">\n",
    "\n",
    "Convolution is an important operation in signal and image processing. Convolution operates on two signals (in 1D) or two images (in 2D): you can think of one as the “input” signal (or image), and the other (called the **kernel**) as a “filter” on the input image, producing an output image. So convolution takes two images as input and produces a third as output.\n",
    "\n",
    "Actually, [convolution is a mathematical process](https://en.wikipedia.org/wiki/Convolution) that \"blends\" two functions. If you take two functions *f* and *g* , there are a number of ways you can combine them. All basic operations can do this (addition, subtraction, multiplication, and division), but there are also special operations that only work with functions and do not work on standard variables or numbers. For example, $f \\circ g$ is a composition of the two functions, where you plug  $g(x)$ into $f$. A convolution is another function-related operation, and is often notated with a star (∗) operator, where\n",
    "\n",
    "$f∗g=h$\n",
    " \n",
    "provides a third function, *h*, that is a blended version of two seemingly unrelated functions *f*  and *g*. it might be better to think of a **convolution as a method to apply a filter to a signal or image**.\n",
    "\n",
    "In 1D, convolution is defined as the integral of the product of the two functions after one is reflected about the *y*-axis and shifted. As such, it is a particular kind of integral transform:\n",
    "\n",
    "$$(f*g)(t)=\\int_{-\\infty}^{\\infty}f(t)g(t-\\tau)d\\tau$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2efb4b2-852c-4fae-8f41-1a411cfb2080",
   "metadata": {},
   "source": [
    "#### Simple examples with numpy\n",
    "\n",
    "##### Basic 1D arrays\n",
    "\n",
    "<div class=\"intro\">\n",
    "\n",
    "Let's apply the `convolve` function of numpy to two 1D arrays, `f = [1,2,3,2,1]` and `g = [1,2,2]`, the **kernel**.\n",
    "\n",
    "We want the resulting array, `h`, to have the same dimension as the largest array. The convolution operation first flips the first array before “sliding” the two across one another. Results in the green boxes correspond to the product of the *f* and *g* vectors term by term, followed by the sum of resulting values - *i.e.* it is the application of $\\int_{-\\infty}^{\\infty}f(t)g(t-\\tau)d\\tau$:\n",
    "\n",
    "<img width=\"60%\" src=\"./ML-Figures/convol1Darrays-same.svg\" style=\"display:block; margin-left:auto; margin-right:auto\" id=\"CNN\"/>  \n",
    "</div>\n",
    "<br>\n",
    "<div class=\"rqE\">\n",
    "\n",
    "We want the output to have the same size as the largest array, *i.e.* *f*. This can be obtained with the `same` option of the [`convolve`function of NumPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve.html). In practice, it consists in adding as many zeros as necessary  at the begining and at the end of the `f` array, as illustrated in the figure above. This is called **padding**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a677de45-b374-463a-a31e-17e87221b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "f = [1,2,3,2,1]\n",
    "g = [1,2,2]\n",
    "\n",
    "h = np.convolve(f,g,'same')\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ddfa03-3081-4cf7-b61f-7f1f4c0f77fd",
   "metadata": {},
   "source": [
    "##### Convolution used to smooth a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d53f4a2-ede7-4b40-a9bb-c679217e58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "x = np.linspace(0,2*np.pi,N)\n",
    "f = np.sin(x) + np.random.random(N)\n",
    "g = 1/5*np.array([1,1,1,1,1])\n",
    "h = np.convolve(f,g,'same')\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(x,f,marker='o',label='f')\n",
    "plt.plot(x,h,marker='v',label='g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d67ad9d-68b1-42fe-b951-c32492392748",
   "metadata": {},
   "source": [
    "##### Convolution used to calculate the derivative of a function\n",
    "\n",
    "<div class=\"intro\">\n",
    "    \n",
    "The definition of the first [derivative](https://en.wikipedia.org/wiki/Derivative) of a function is:\n",
    "\n",
    "$$f'(x)=\\frac{df}{dx}=\\lim_{h\\rightarrow0}\\frac{f(x)-f(x+h)}{h}$$\n",
    "\n",
    "A \"central\" derivative can also be defined:\n",
    "\n",
    "$$f'(x)=\\frac{df}{dx}=\\lim_{h\\rightarrow0}\\frac{f(x+h)-f(x-h)}{2h}$$\n",
    "\n",
    "If you have a discretized function, *i.e.* $[... f(x_{k-2}),f(x_{k-1}),f(x_{k}), f(x_{k+1}), f(x_{k+2}) ...]$, with a regular step (*i.e.* (*x*<sub>k+1</sub> - *x*<sub>k</sub>) = constant), the first derivative can be approximated as\n",
    "\n",
    "$$f'(x_{k})\\approx\\frac{\\left(f(x_{k+1})-f(x_{k-1})\\right)}{2\\delta x}$$\n",
    "\n",
    "where $\\delta x = x_1 - x_0$\n",
    "\n",
    "It can be calculated by a convolution, using the **[1,0,-1]/(2&delta;x) kernel** (or **filter**)\n",
    "\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"app\">\n",
    "\n",
    "Let's consider the sin(*x*) function, which first derivative is cos(*x*)\n",
    "\n",
    "**1.** Plot the sin(x) and cos(x) functions, between -2$\\pi$ and 2$\\pi$\n",
    "\n",
    "**2.** Use the convolve operation with the [1,0,-1]/(2&delta;x) kernel to calculate the first derivative of sin(*x*). Plot the resulting array. Add two horizontal lines, using the `plt.axhline(1,linestyle='--',color='r',linewidth=0.5)` and `plt.axhline(-1,linestyle='--',color='r',linewidth=0.5)` commands. Comment?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89631b04-3530-481b-8ca3-0e41906dd511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Q1: insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7611ef35-cfbe-4eee-8eb1-0412674a95c8",
   "metadata": {},
   "source": [
    "<div class=\"sol\">\n",
    "\n",
    "Want to see a possible answer to Q1? Uncomment the `# %load ./SolutionsToExercises/...` command below\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8eecb4-0446-45df-b7af-ec77d6c71a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load ./SolutionsToExercises/ML/numDerivative_with_convolveQ1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810e1044-e212-48a6-8ef1-91b8e0b8df98",
   "metadata": {},
   "source": [
    "<div class=\"sol\">\n",
    "\n",
    "Want to see a possible answer to Q2? Uncomment the `# %load ./SolutionsToExercises/...` command below\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a214d26-1695-4992-95d1-55f770ad1fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: insert your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99067e0-9bb1-4018-965c-f70b429a737b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load ./SolutionsToExercises/ML/numDerivative_with_convolveQ2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4e8bde-571e-4f2b-8ce5-2a912f7ae447",
   "metadata": {},
   "source": [
    "<div class=\"rqE\">\n",
    "\n",
    "There is a boundary effect, due to the `same` option. \n",
    "\n",
    "Let's apply the `np.convolve` operation, but with the `valid` and `full` options. **Check the lengths of the resulting functions before plotting!**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b20c7dd-5aa2-4f00-8d24-43aba479d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsame = np.convolve(f,K,'same')\n",
    "hvalid = np.convolve(f,K,'valid')\n",
    "hfull = np.convolve(f,K,'full')\n",
    "plt.plot(x[:], hsame,marker='o',linestyle='--', label='same')\n",
    "plt.plot(x[1:-1], hvalid,marker='x',linestyle='-.', label='valid')\n",
    "plt.plot(x[:], hfull[1:-1],marker='v',linestyle=':', label='full')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b82a580-3304-4039-8a40-00e0dbaf7d25",
   "metadata": {},
   "source": [
    "##### Application of a pattern to a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e768d8-922e-4fce-a6c3-ed9ddadac3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 40\n",
    "f = np.zeros(N) + (np.random.random(N))/10\n",
    "f[3*N//4-1] = 1.2\n",
    "f[3*N//4] = 1\n",
    "f[3*N//4+1] = 1.3\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(f,marker='o',label='f')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "def gaussian(x, mu, sigma):\n",
    "    return np.exp(-(x - mu)**2 / (2 * sigma**2.))/(sigma*np.sqrt(2*np.pi))\n",
    "mu = 3\n",
    "fwhm = 2\n",
    "sigma = fwhm/(2*np.sqrt(2*np.log(2)))\n",
    "x = np.arange(mu-3,mu+4,1)\n",
    "g = gaussian(x,mu,sigma)\n",
    "plt.plot(x,g,marker='v',label=\"gaussian filter\",color='C1')\n",
    "plt.xlim(0,25)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6b20f-a6ab-4324-b9a3-0c1fa6129e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "filtered_f = gaussian_filter1d(f,sigma)\n",
    "\n",
    "filtered_f2 = np.convolve(f,g,mode='same')\n",
    "print(filtered_f2)\n",
    "\n",
    "plt.plot(f,marker='o',label=\"f\")\n",
    "plt.plot(g,marker='v',label=\"gaussian filter\")\n",
    "plt.plot(filtered_f, marker='x', label=\"filtered f\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cb7a7c-1e82-4579-a370-34a4c41c687f",
   "metadata": {},
   "source": [
    "##### Find a pattern by a correlation study: application to radar detection\n",
    "\n",
    "<div class=\"intro\">\n",
    "\n",
    "Another application is radar detection. Here's how it works: the radar emits a signal, the waves reach an object (an aircraft, for example) and are reflected back, and the radar receives this reflected signal. The time taken for the waves to make the round trip is used to calculate the distance to the object.\n",
    "\n",
    "<img width=\"300px\" src=\"./ML-Figures/RadarAntenna-DallE.png\" style=\"display:block; margin-left:auto; margin-right:auto\" id=\"radar\"/>  \n",
    "\n",
    "\n",
    "In practice, it's more complicated: the transmitted signal is well known, but the received signal, or echo, is made up of the reflected wave superimposed with random noise from the environment (clouds, birds, etc.).\n",
    "\n",
    "The application of *correlation* is a good method to find the radar signal in the echo. Correlation is a similar operation to except that the kernel is not inverted. \n",
    "\n",
    "Let's first simulate the pulsed radar signal, *i.e.* the wave emitted by the radar. It will be the **kernel** \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac67fc-12d9-4bb3-970b-609f83ff9e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def correlation(f,g):\n",
    "    import numpy as np\n",
    "    gg = np.flip(g)\n",
    "    h = np.convolve(f,gg,'same')\n",
    "    return h\n",
    "\n",
    "# Generating a signal emitted by a radar\n",
    "Nradar = 100\n",
    "Xradar = np.linspace(-3.14,3.14,Nradar)\n",
    "Yradar = np.sinc(Xradar) - 0.5\n",
    "plt.plot(Yradar,color='C1',marker='v')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e68599-b7d8-4f26-8274-828d5cbbf503",
   "metadata": {},
   "source": [
    "<div class=\"intro\">\n",
    "\n",
    "Let's now simulate a noisy signal, in order to generate the echo received by the radar.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff77d6a-fada-4970-b5dd-9a369f6db33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, Nin = 0, 1000, 1000\n",
    "Xin = np.linspace(a, b, Nin)\n",
    "Yrandom = np.random.normal(0,1,Nin)/2\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(Yrandom)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fb922e-7af9-4d22-b97b-fdc5601c513b",
   "metadata": {},
   "source": [
    "<div class=\"intro\">\n",
    "\n",
    "Add now the pulsed radar single signal to the noisy signal, after a delay of 350 time steps. For that, we will use the [`pad` function of NumPy](https://numpy.org/doc/stable/reference/generated/numpy.pad.html).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42041e8-3b73-476a-99e9-a76216698dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "delay = 350\n",
    "Yin = Yrandom + np.pad(Yradar,(delay,Nin-Nradar-delay),'constant', constant_values=(0, 0))\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(Yin,color='C0')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd2b033-5330-41d3-80b7-4aa080777ae9",
   "metadata": {},
   "source": [
    "<div class=\"rqT\" title=\"Compare the two signals\">\n",
    "\n",
    "At first sight, the difference between the two signals is not immediately noticeable. The radar echo, between t=350 and t=450, is well hidden in the noise..\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"intro\">\n",
    "\n",
    "Let's now filter the received signal by searching a correlation with the radar echo.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b188f-94c1-4d13-a217-929757249382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affichage_correlation(f,g):\n",
    "    gg = np.flip(g)\n",
    "    h = np.convolve(f,gg,'same')\n",
    "\n",
    "    ax = plt.subplot(2,1,1)\n",
    "    ax.set_title(\"Signal received by the radar\")\n",
    "    plt.plot(f,color='C0')\n",
    "\n",
    "    ax = plt.subplot(2,1,2)\n",
    "    ax.set_title(\"Signal filtered by searching a correlation with the radar emitting signal\")\n",
    "    plt.plot(h,color='C2')\n",
    "\n",
    "    plt.subplots_adjust(top=0.9, bottom=0.1, left=0.1, right=0.95, hspace=1.0,wspace=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "affichage_correlation(Yin,Yradar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376e7452-1d65-42ab-9e39-d5da548f9616",
   "metadata": {},
   "source": [
    "<div class=\"rqE\">\n",
    "\n",
    "The observed peak corresponds to the signal returned by the aircraft. In the example, this signal is detected at position t=400. This makes possible the calculation of the distance from the radar to the aircraft\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45747eae-2cf3-417e-bedf-5290206a52c6",
   "metadata": {},
   "source": [
    "### 1D convolutional neural networks\n",
    "\n",
    "<div class=\"intro\">\n",
    "\n",
    "**1D Convolutional Neural Networks** (**CNNs**) have recently become the state-of-the-art technique for crucial signal processing applications such as patient-specific ECG classification, structural health monitoring, anomaly detection in power electronics circuitry and motor-fault detection. Here is a general scheme:\n",
    "\n",
    "<img width=\"40%\" src=\"./DS4B-Slides/pngs/ZooNN/CNNwithLegend.svg\" style=\"display:block; margin-left:auto; margin-right:auto\" id=\"CNN\"/>  \n",
    "\n",
    "Convolutional layers are generally followed by \"classical\" dense layers.\n",
    "\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"rqE\">\n",
    "\n",
    "As you can see in the above scheme, the connectivity is not the same in the CNN part as in dense layers. Let's consider a 1D convolution window, *i.e.* the **kernel**, of size 3:\n",
    "\n",
    "<img width=\"17%\" src=\"./ML-Figures/CNNschematic.svg\" style=\"display:block; margin-left:auto; margin-right:auto\" id=\"CNNwithKernel\"/>  \n",
    "\n",
    "The content of a convolutional neuron is simply calculated as \n",
    "\n",
    "$$c=\\sum_{i=m}^{m+K-1}w_{i}x_{i}$$\n",
    "\n",
    "where $i$ runs over the kernel, of size $K$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd77a32-4e5a-41ce-9c7a-ea577df1ffca",
   "metadata": {},
   "source": [
    "## Application to the classification of nanoparticles, on the basis of their RDF profile\n",
    "\n",
    "### Context\n",
    "\n",
    "<div class=\"introT\" title=\"Introduction\">\n",
    "\n",
    "<img width=\"400px\" src=\"./ML-Figures/RDFs/GTA_AI.png\" style=\"display:block; margin-left:auto; margin-right:auto\" id=\"GA\"/>  \n",
    "<br><br>\n",
    "\n",
    "The **structure of nanomaterials** can directly influence their physical and chemical properties, that can be of interest for applications in various fields ranging from biology, medicine, optoelectronics, catalysis, energy, etc... Among nano-objects, colloidal transition metal nanoparticles (TMNPs) exhibit unique properties, often located between those of bulk materials and small clusters, and related to their size, shape, surface composition, surface or core defects. Thanks to the art of chemical synthesis, the metal core of TMNPs exhibit a fascinating variety of shapes, most of them being in fact Platonic, Archimedean, or Catalan solids, or even concave or convex polyhedra.\n",
    "\n",
    "The **Pair Distribution Function** (PDF) $g(r)$ is another good fingerprint for materials science that encodes information about the whole atomic structure. It is experimentally obtained from **high energy X-ray diffraction**, a technique called Wide-Angle X-ray Scattering (**WAXS**). It is particularly well suited to discriminate in situ different crystalline structures and different shapes of NPs (see [*this article*](https://doi.org/10.1021/acs.chemrev.1c00237) and references therein). It is unique, continuous, differentiable with respect to atomic coordinates, invariant with respect to rotation, translation and nuclear permutation.\n",
    "\n",
    "Provided that the atomic cartesian coordinates of various crystalline NPs be known, $g(r)$ can be simulated in real space for nanoparticles, modeled as attenuated bulk crystals:\n",
    "$$g(r)=\\frac{1}{r}\\left[\\frac{1}{N}\\sum_{i}\\sum_{j\\neq i}\\frac{b_{i}^{*}b_{j}}{<b>}\\delta(r-r_{ij})\\right]-4\\pi r\\rho_{0}\\gamma_{0}$$\n",
    "\n",
    "where:\n",
    "- $b_i$ =  scattering power of atom *i*\n",
    "- $\\rho_0$ = average atom number density\n",
    "- $\\gamma_0$ = characteristic function describing the autocorrelation of the shape of the scattering domains in the material\n",
    "- $\\delta(r-r_{ij})$ = delta function, usually replaced by a Gaussian distribution function\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"intro\">\n",
    "\n",
    "We aim to **automatically identify nanostructures** based on their pair distribution function (PDF) profiles, obtained experimentally from WAXS. To train such a **classifier**, we first constructed a database of reference profiles generated *in silico*, by building 3D structural models and computing their corresponding PDFs using the appropriate scattering formula.\n",
    "\n",
    "For practical reasons, and to reduce the complexity of the problem, we relied instead on a database of radial distribution function (RDF) profiles. Experimental PDFs were then converted into RDF-like profiles to make them consistent with the simulated data. Rather than performing a direct comparison between experimental and simulated profiles, we use **neural networks** to carry out the classification task. \n",
    "\n",
    "The task therefore reduces to the classification of one-dimensional profiles. Conceptually, this is the 1D analogue of the well-known “cats *vs.* dogs” image classification problem in computer vision, where 2D images are replaced by 1D structural fingerprints.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f67b91-180a-49cc-87ce-305cd613fd43",
   "metadata": {},
   "source": [
    "#### Experimental profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401db187-0993-4b7f-a308-14468ddd5301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.signal import find_peaks\n",
    "pathway2rdfExp = \"./ML-data/RDFs/expRDF/\"\n",
    "\n",
    "#########################################################################################################################################\n",
    "\n",
    "t4pPC.centertxt(\"RDF of a pentagonal bipyramid AuNP (recorded by N. Ratel-Ramond, LPCNO, Toulouse, France)\",size=14,weight='bold')\n",
    "_,grDecaOrg = np.genfromtxt(pathway2rdfExp+\"AuNP-CGSR_021_decahedron.csv\", delimiter=\"\\t\", unpack=True, skip_header=1)\n",
    "rDeca,grDeca = np.genfromtxt(pathway2rdfExp+\"AuNP-CGSR_021_decahedron_corrected.csv\", delimiter=\"\\t\", unpack=True, skip_header=1)\n",
    "print(f\"rdf between {rDeca[0]} Å and {rDeca[-1]} Å. Step = {rDeca[1]-rDeca[0]} Å. Number of values = {len(rDeca)}\")\n",
    "peaksDeca, _ = find_peaks(grDeca,height=1)\n",
    "print(f\"First intense peak found at: {rDeca[peaksDeca[0]]:.2f} Å. g(r) = {grDeca[peaksDeca[0]]:.2f}\")\n",
    "\n",
    "_ = plt.figure(figsize=(10,4))\n",
    "_ = plt.plot(rDeca,grDecaOrg,label=\"Original PDF file\")\n",
    "_ = plt.plot(rDeca,grDeca,label=\"Adapted RDF profile\")\n",
    "_ = plt.xlabel(\"d / Å\")\n",
    "_ = plt.ylabel(\"G / Å$^{-2}$\")\n",
    "_ = plt.legend()\n",
    "_ = plt.axhline(y=0,linewidth=1,color='r')\n",
    "_ = plt.xlim(0,30)\n",
    "_ = plt.show()\n",
    "\n",
    "#########################################################################################################################################\n",
    "\n",
    "t4pPC.centertxt(\"RDF of an icosahedral AuNP (recorded by N. Ratel-Ramond, LPCNO, Toulouse, France)\",size=14,weight='bold')\n",
    "_,grIcoOrg = np.genfromtxt(pathway2rdfExp+\"AuNP-NS_Ti25C_1000_0001_0001_icosahedron.csv\", delimiter=\"\\t\", unpack=True, skip_header=1)\n",
    "rIco,grIco = np.genfromtxt(pathway2rdfExp+\"AuNP-NS_Ti25C_1000_0001_0001_icosahedron_corrected.csv\", delimiter=\"\\t\", unpack=True, skip_header=1)\n",
    "print(f\"rdf between {rIco[0]} Å and {rIco[-1]} Å. Step = {rIco[1]-rIco[0]} Å. Number of values = {len(rIco)}\")\n",
    "peaksIco, _ = find_peaks(grIco,height=1)\n",
    "print(f\"First intense peak found at: {rIco[peaksIco[0]]:.2f} Å. g(r) = {grIco[peaksIco[0]]:.2f}\")\n",
    "\n",
    "_ = plt.figure(figsize=(10,4))\n",
    "_ = plt.plot(rIco,grIcoOrg,label=\"Original PDF file\")\n",
    "_ = plt.plot(rIco,grIco,label=\"Adapted RDF profile\")\n",
    "_ = plt.xlabel(\"d / Å\")\n",
    "_ = plt.ylabel(\"G / Å$^{-2}$\")\n",
    "_ = plt.legend()\n",
    "_ = plt.axhline(y=0,linewidth=1,color='r')\n",
    "_ = plt.xlim(0,30)\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2a32f8-7cdb-4924-9b2c-6a515f51b1de",
   "metadata": {},
   "source": [
    "<div class=\"rq\">\n",
    "\n",
    "Both blue profiles are characteristic of a fivefold symmetry shape, such as decahedra or icosahedra. The first profile, with its decreasing intensity of positive peaks, is archetypal of decahedra.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f473ae40-9e8f-4ce9-9253-136316aaf2fe",
   "metadata": {},
   "source": [
    "#### Identification of the shape of NPs on the basis of experimental RDF profiles\n",
    "\n",
    "<div class=\"intro\">\n",
    "    \n",
    "A database of 11 classes of nanoparticles (NPs) was built separately with the in-house `pyNanoMatBuilder` library. This library creates the atomic coordinates of NPs with a given crystal structure by specifying either the shape, or by making a Wulff construction. \n",
    "\n",
    "The classes (bcc cube, bcc regular dodecahedron, cuboctahedron, dodecahedron, fcc cube, fcc truncated octahedron, fcc dihedral rhombic dodecahedron, hcp sphere, icosahedron, pentagonal bipyramid, fcc octahedron) are summarized below.. There is one line per class, 4 to 5 NPs of different sizes were considered for each class. **Data augmentation** was applied for each size by randomly moving each atom around its equilibrium position, resulting in 20 RDF variants per class and per size. Only the RDF (black), and its first derivative (blue), of the generating NP is displayed in the figure below.  \n",
    "<img width=\"95%\" src=\"./ML-Figures/RDFs/rdf_grid-C.png\" style=\"display:block; margin-left:auto; margin-right:auto\" id=\"libRDFs\"/> \n",
    "</div>\n",
    "\n",
    "#### Data augmentation?\n",
    "\n",
    "<div class=\"intro\">\n",
    "    \n",
    "Run the cell below to see the 21 RDF variants for the 309-atoms (*i.e.* 4 shells) icosahedron\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44edc048-716f-40b8-b56c-ee5e2034176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "pathway = \"./ML-data/RDFs/simulRDF/\"\n",
    "csv1DFico = pathway+\"RDF_profiles_ico.csv\"\n",
    "\n",
    "if os.path.exists(csv1DFico):\n",
    "    dfFico=pd.read_csv(csv1DFico, sep=\"\\t\", index_col = 0) \n",
    "    print(f\"dfFico. Structure (shape) :{dfFico.shape}\")\n",
    "    display(dfFico)\n",
    "else:\n",
    "    print(f'{csv1DFico} was not found')\n",
    "    print('Create first this csv file, by running the codes of the appendix of the present notebook') \n",
    "\n",
    "dfFico309 = dfFico.filter(regex='ico_040_|r')\n",
    "print(f\"{bg.DARKREDB}The csv file contains {dfFico.shape[1]-1} RDF profiles, including {dfFico309.shape[1]-1} for the 309-atoms icosahedron, plotted below\")\n",
    "Y = list(dfFico309.columns)\n",
    "Y.remove('r')\n",
    "\n",
    "_ = plt.figure(figsize=(16,4))\n",
    "for y in Y:\n",
    "    _ = plt.plot(dfFico309['r'],dfFico309[y],lw=1)\n",
    "    _ = plt.xlabel(\"d / Å\")\n",
    "    _ = plt.ylabel(\"G / Å$^{-2}$\")\n",
    "_ = plt.axhline(y=0,linewidth=1,color='r')\n",
    "_ = plt.xlim(0,20)\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44ce948-1ecb-4c17-b51d-626875a34c96",
   "metadata": {},
   "source": [
    "#### Comparaison of the experimental profiles with simulated RDF profiles of some structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee071f65-7cb5-4b4c-8453-9eac75582c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "pathway = \"./ML-data/RDFs/simulRDF/\"\n",
    "\n",
    "def readRDF(pathway, file, show=False):\n",
    "    csv1DF = pathway + file\n",
    "    if os.path.exists(csv1DF):\n",
    "        dfF=pd.read_csv(csv1DF, sep=\"\\t\", index_col = 0) \n",
    "        print(f\"dfF. Structure (shape) :{dfF.shape}\")\n",
    "        if show: display(dfF)\n",
    "    else:\n",
    "        print(f'{csv1DF} was not found')\n",
    "        print('Create first this csv file, by running the codes of the appendix of the present notebook') \n",
    "    return dfF\n",
    "\n",
    "dfFico = readRDF(pathway,\"RDF_profiles_ico.csv\")\n",
    "dfFico147 = dfFico.filter(regex='ico_030_|r')\n",
    "print(f\"{bg.DARKREDB}The RDF_profiles_ico.csv csv file contains {dfFico.shape[1]-1} RDF profiles, including one perfectly symmetric 147-atoms icosahedron, plotted below{bg.OFF}\")\n",
    "\n",
    "dfFpbpy = readRDF(pathway,\"RDF_profiles_pbpy.csv\")\n",
    "dfFpbpy105 = dfFpbpy.filter(regex='pbpy_040_|r')\n",
    "print(f\"{bg.DARKREDB}The RDF_profiles_pbpy.csv csv file contains {dfFpbpy.shape[1]-1} RDF profiles, including one perfectly symmetric 105-atoms decahedron, plotted below{bg.OFF}\")\n",
    "\n",
    "dfFOh = readRDF(pathway,\"RDF_profiles_Oh.csv\")\n",
    "dfFOh146 = dfFOh.filter(regex='regfccOh_050_|r')\n",
    "print(f\"{bg.DARKREDB}The RDF_profiles_Oh.csv csv file contains {dfFOh.shape[1]-1} RDF profiles, including one perfectly symmetric 146-atoms octahedron, plotted below{bg.OFF}\")\n",
    "\n",
    "_ = plt.figure(figsize=(16,4))\n",
    "_ = plt.plot(rDeca/10,-0.2+grDeca/np.max(grDeca),label=\"Adapted exp RDF profile of deca, normalized\")\n",
    "_ = plt.plot(rIco/10,grIco/np.max(grIco),label=\"Adapted exp RDF profile of ico, normalized\")\n",
    "_ = plt.plot(dfFico147['r']/10,0.2+dfFico147.iloc[:, 1]/dfFico147.iloc[:, 1].max(),lw=1,label=\"147-atoms icosahedron, normalized\") # column 0 = r, column 1 = perfectly symmetrical icoshadron, without random atomic displacement\n",
    "_ = plt.plot(dfFpbpy105['r']/10,0.4+dfFpbpy105.iloc[:, 1]/dfFpbpy105.iloc[:, 1].max(),lw=1,label=\"105-atoms decahedron, normalized\")\n",
    "_ = plt.plot(dfFOh146['r']/10,0.6+dfFOh146.iloc[:, 1]/dfFOh146.iloc[:, 1].max(),lw=1,label=\"146-atoms fcc Octahedron, normalized\")\n",
    "_ = plt.xlabel(\"d / nm\")\n",
    "_ = plt.ylabel(\"G / Å$^{-2}$\")\n",
    "_ = plt.xlim(0,1.5)\n",
    "_ = plt.legend()\n",
    "_ = plt.grid()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb9275b-d37e-4590-872f-de5071511f3c",
   "metadata": {},
   "source": [
    "<div class=\"exE\" title=\"3D shape of these 3 NPs\">\n",
    "\n",
    "Use the `molView` class developped and used in the [MolecularRepresentations-Nano.ipynb notebook](MolecularRepresentations-Nano.ipynb) to display these three structures, saved in the `./ML-data/RDFs/coords_3DNPs/` folder, under the names:\n",
    "- `ico_030_000.xyz`\n",
    "- `pbpy_040_000.xyz`\n",
    "- `regfccOh_050_000.xyz`\n",
    "\n",
    "**Hint.** a `molView` class, developped in the [Representation and Metadata Generation of \\[Nano\\]materials for Machine Learning notebook](MolecularRepresentations-Nano.ipynb) is also available in the `tools4pyPhysChem utility`, aka `t4pPC`, which is imported in the very first cell of this notebook. Type the command `help(t4pPC.molView)` to read the documentation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47e17cd-f1b7-47dd-884a-4e528a324601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36278cf5-09e7-4ef9-baed-df57eaee2524",
   "metadata": {},
   "source": [
    "<div class=\"sol\">\n",
    "\n",
    "Want to see a possible answer? Uncomment the `# %load ./SolutionsToExercises/...` command below, and then run the cell twice (one to load the python code, the second one to run it)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee1a93b-a7ef-4fb6-b099-8cd56b86f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./SolutionsToExercises/ML/viewNP.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b9339-e919-45dc-9176-ac54db20f041",
   "metadata": {},
   "source": [
    "### Definition of useful functions\n",
    "\n",
    "<div class=\"intro\">\n",
    "\n",
    "The following utility functions provide a convenient toolkit for analyzing and visualizing the performance of neural networks.\n",
    "\n",
    "- `plotEpochs` plots the training and validation loss, as well as user-defined metrics, over epochs.\n",
    "- `printAcc` summarizes the model performance on both training and test sets, reporting loss, accuracy, and mean absolute error.\n",
    "- `plotConfusionMatrix` displays confusion matrices for training and test data side by side, together with global metrics such as accuracy, MAE, and MSE.\n",
    "- `rescale1D` rescales and interpolates one-dimensional profiles to a common reference, which is useful when comparing experimental and simulated data.\n",
    "\n",
    "Together, these tools allow for a clear assessment of model training, prediction quality, and classification reliability.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7980709-f48d-4bf7-ace5-64fadf529d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotEpochs(history,metrics):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    # c'est pratique d'utiliser un dataframe et les fonctions de tracé associées\n",
    "    df=pd.DataFrame(data=history.history)\n",
    "\n",
    "    nEpoch = df.shape[0]\n",
    "    print(nEpoch)\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (8,4)\n",
    "    # on va d'abord tracer les courbes de la fonction qui a été minimisée au cours de l'apprentissage\n",
    "    figLOSS=df.plot(y=[\"loss\",\"val_loss\"],linestyle='-', marker='o',fontsize=14)\n",
    "    figLOSS.set_xlabel('epoch',fontdict={'fontsize':16})\n",
    "    figLOSS.set_ylabel('loss',fontdict={'fontsize':16})\n",
    "    yMin = np.min(df[[\"loss\",\"val_loss\"]].iloc[nEpoch//3:nEpoch,:].to_numpy())\n",
    "    yMax = np.max(df[[\"loss\",\"val_loss\"]].iloc[nEpoch//3:nEpoch,:].to_numpy())\n",
    "    yMin = round(yMin-0.05,1)\n",
    "    yMax = round(yMax+0.05,1)\n",
    "    print(yMin,yMax)\n",
    "    figLOSS.set_ylim([yMin,yMax])\n",
    "    figLOSS.legend(loc='upper right', shadow=True, fontsize='x-large')\n",
    "    plt.show()\n",
    "\n",
    "    for i in range(len(metrics)):\n",
    "        ErrTraining = metrics[i]\n",
    "        ErrVal = \"val_\" + ErrTraining\n",
    "        figACC=df.plot(y=[ErrTraining,ErrVal],linestyle='-', marker='o',fontsize=14)\n",
    "        figACC.set_xlabel('epoch',fontdict={'fontsize':16})\n",
    "        figACC.set_ylabel(ErrTraining,fontdict={'fontsize':16})\n",
    "        yMin = np.min(df[[ErrTraining,ErrVal]].iloc[nEpoch//3:nEpoch,:].to_numpy())\n",
    "        yMax = np.max(df[[ErrTraining,ErrVal]].iloc[nEpoch//3:nEpoch,:].to_numpy())\n",
    "        yMin = round(yMin-0.05,1)\n",
    "        yMax = round(yMax+0.05,1)\n",
    "        print(yMin,yMax)\n",
    "        figACC.set_ylim([yMin,yMax])\n",
    "        figACC.legend(loc='lower right', shadow=True, fontsize='x-large')\n",
    "        plt.show()\n",
    "\n",
    "def printAcc(model, x_train, y_train, x_test, y_test):\n",
    "    evalANN_on_TrainSet = model.evaluate(x_train, y_train, verbose=0)\n",
    "    print(f\"{color.GREEN}x_train / loss      : {evalANN_on_TrainSet[0]:5.4f}{color.OFF}\")\n",
    "    print(f\"{color.GREEN}x_train / accuracy  : {evalANN_on_TrainSet[1]:5.4f}{color.OFF}\")\n",
    "    print(f\"{color.GREEN}x_train / mae       : {evalANN_on_TrainSet[2]:5.4f}{color.OFF}\")\n",
    "    print()\n",
    "    evalANN_on_TestSet = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"{color.BLUE} x_test / loss      : {evalANN_on_TestSet[0]:5.4f}{color.OFF}\")\n",
    "    print(f\"{color.BLUE} x_test / accuracy  : {evalANN_on_TestSet[1]:5.4f}{color.OFF}\")\n",
    "    print(f\"{color.BLUE} x_test / mae       : {evalANN_on_TestSet[2]:5.4f}{color.OFF}\")\n",
    "\n",
    "def plotConfusionMatrix(y_trainC, y_trainC_hat, y_testC, y_testC_hat, uv, saveSVGImg=None):\n",
    "    from sklearn.metrics import ConfusionMatrixDisplay\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.gridspec as gridspec\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    \n",
    "    cm_labels = uv\n",
    "    cm_tr = confusion_matrix(np.argmax(y_trainC_hat,axis=1),np.argmax(y_trainC.to_numpy(),axis=1))\n",
    "    cm_tt = confusion_matrix(np.argmax(y_testC_hat,axis=1),np.argmax(y_testC.to_numpy(),axis=1))\n",
    "\n",
    "    cm2i = 1/2.54\n",
    "    widthSize = len(uv)*cm2i*1.5\n",
    "    fig=plt.figure(figsize=(2*widthSize,widthSize*0.8))\n",
    "    gs = gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[1, 1])\n",
    "    \n",
    "    ax00 = fig.add_subplot(gs[0, 0], title=\"Training set. Confusion Matrix\")\n",
    "    sns.heatmap(pd.DataFrame(cm_tr, columns=cm_labels, index=cm_labels), ax=ax00, cmap=plt.cm.Blues, annot = True)\n",
    "    ax00.set_xlabel(\"real\", fontsize = 20)\n",
    "    ax00.set_ylabel(\"predicted\", fontsize = 20)\n",
    "    \n",
    "    ax01=fig.add_subplot(gs[0, 1], title=\"Test set. Confusion Matrix\")\n",
    "    sns.heatmap(pd.DataFrame(cm_tt, columns=cm_labels, index=cm_labels), ax=ax01, cmap=plt.cm.Blues, annot = True)\n",
    "    ax01.set_xlabel(\"real\", fontsize = 20)\n",
    "    ax01.set_ylabel(\"predicted\", fontsize = 20)\n",
    "    if saveSVGImg is not None:\n",
    "        plt.savefig(saveSVGImg, format='svg', dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "    atr = accuracy_score(np.argmax(y_trainC.to_numpy(),axis=1),np.argmax(y_trainC_hat,axis=1))\n",
    "    att = accuracy_score(np.argmax(y_testC.to_numpy(),axis=1),np.argmax(y_testC_hat,axis=1))\n",
    "    maetr = mean_absolute_error(np.argmax(y_trainC.to_numpy(),axis=1),np.argmax(y_trainC_hat,axis=1))\n",
    "    maett = mean_absolute_error(np.argmax(y_testC.to_numpy(),axis=1),np.argmax(y_testC_hat,axis=1))\n",
    "    msetr = mean_squared_error(np.argmax(y_trainC.to_numpy(),axis=1),np.argmax(y_trainC_hat,axis=1))\n",
    "    msett = mean_squared_error(np.argmax(y_testC.to_numpy(),axis=1),np.argmax(y_testC_hat,axis=1))\n",
    "    print(f\"{hl.BOLD}{fg.BLUE}                Accurracy       MAE         MSE\")\n",
    "    print(f\"{hl.BOLD}{fg.PURPLE} Training set.      {atr:.2f}        {maetr:.2f}        {msetr:.2f}\")\n",
    "    print(f\"{hl.BOLD}{fg.GREEN}     Test set.      {att:.2f}        {maett:.2f}        {msett:.2f}\")\n",
    "    return\n",
    "\n",
    "def rescale1D(x,f,peakref,xmax,dxTarget):\n",
    "    import numpy as np\n",
    "    dx = x[1]-x[0]\n",
    "    f = f/f[peakref]\n",
    "    x = x/x[peakref]\n",
    "    xmaxTarget = xmax / x[peakref]\n",
    "    xnew = np.arange(0,xmaxTarget,dxTarget)\n",
    "    ynew = np.interp(xnew, x, f)\n",
    "    return xnew,ynew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b759c8-72bd-4085-a884-27c39a0c7bf0",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "#### Read the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ccffeb-6a25-45ff-a801-882272bb28ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "pathway = \"./ML-data/RDFs/simulRDF/\"\n",
    "csv1DF = pathway+\"RDF_profiles.csv\"\n",
    "csv1DL = pathway+\"RDF_labels.csv\"\n",
    "\n",
    "if os.path.exists(csv1DF) and os.path.exists(csv1DL):\n",
    "    dfF=pd.read_csv(csv1DF, sep=\"\\t\", index_col = 0) \n",
    "    dfY=pd.read_csv(csv1DL, sep=\"\\t\", index_col = 0) \n",
    "    print(f\"dfF. Structure (shape) :{dfF.shape}\")\n",
    "    print(f\"dfY. Structure (shape) :{dfY.shape}\")\n",
    "    display(dfF)\n",
    "    display(dfY)\n",
    "else:\n",
    "    print(f'{csv1DF} and/or {csv1DL} were not found')\n",
    "    print('Create first these files, by running the codes of the appendix of the present notebook') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14ff5cb-4eeb-4c8d-a4a3-8a4ec50e289d",
   "metadata": {},
   "source": [
    "#### Standardization of the data and one-hot-encoding of the polynomial type\n",
    "\n",
    "<div class=\"app\">\n",
    "\n",
    "Analyze carefully what is done in the next cell. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37863a5e-ff2b-47bc-84fc-e4b98a34ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDFcols = list(dfF.columns)\n",
    "RDFcols.remove('r')\n",
    "dfFS = dfF.copy()\n",
    "dfFS = dfFS.drop('r',axis='columns')\n",
    "for nameRDF in RDFcols:\n",
    "    peaks, _ = find_peaks(dfFS[nameRDF],height=1)\n",
    "    peakRef = peaks[0]\n",
    "    # print(peaks[0], rvalues[peaks[0]],dfFS[nameRDF].iloc[peaks[0]])\n",
    "    dfFS[nameRDF] = dfFS[nameRDF]/dfFS[nameRDF].iloc[peaks[0]]\n",
    "\n",
    "deltaR = dfF['r'].iloc[1] - dfF['r'].iloc[0]\n",
    "Rmax = 30\n",
    "nrows = int(Rmax / deltaR)\n",
    "print(f\"{bg.LIGHTBLUEB}Keeping {nrows} rows, such that Rmax = {Rmax} Å (δr = {deltaR} Å)\")\n",
    "dfFS = dfFS.iloc[np.linspace(0,nrows,nrows)]\n",
    "display(dfFS)\n",
    "display(dfFS.describe().style.format(\"{0:.2f}\").set_caption(\"Training set after rescaling wrt intensity\"))\n",
    "dfFS = dfFS.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea5ecd8-6d87-4420-83e0-b73defc81c62",
   "metadata": {},
   "source": [
    "<div class=\"app\">\n",
    "\n",
    "Why has the dfFS dataframe been transposed in the previous cell (`dfFS = dfFS.T`)?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c03805-5c54-4783-a6d7-828e317a01de",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlines = dfFS.shape[0]\n",
    "ncols = dfFS.shape[1]\n",
    "dfFS[\"F\"]=dfY[\"F\"]\n",
    "display(dfFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a7eebd-9102-4b10-8d25-88ec5b617a67",
   "metadata": {},
   "source": [
    "<div class=\"app\">\n",
    "\n",
    "What is one hot encoding?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4932fd-55f2-493a-a58c-bbc7574ad35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uv = dfFS[\"F\"].unique()\n",
    "print(f\"{len(uv):2} unique categories: {uv}\") \n",
    "mapc2i = {}\n",
    "for x in range(len(uv)):\n",
    "    mapc2i[uv[x]] = x\n",
    "print(f\"One-hot-encoding = correspondence between a unique category and an integer number: {mapc2i}\")\n",
    "F_ohe = t4pPC.y2c(mapc2i,dfFS[\"F\"])\n",
    "dfohe = pd.DataFrame(F_ohe,columns=uv,index=dfFS.index)\n",
    "dfFSohe = pd.concat([dfFS,dfohe],axis=1)\n",
    "# col = dfFSohe.pop('order')\n",
    "# dfFSohe.insert(dfFSohe.shape[1], col.name, col)\n",
    "display(dfFSohe)\n",
    "\n",
    "del dfohe,dfFS,mapc2i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11858c01-307b-4c1f-a209-8284057d2357",
   "metadata": {},
   "source": [
    "#### Construction of the training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29590d1c-e386-45dd-a6a5-8de51605f482",
   "metadata": {},
   "source": [
    "<div class=\"app\">\n",
    "\n",
    "What is the purpose of `frac=0.8`, in the first line?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22949836-1589-4dc6-b4f0-0fdba7630b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = dfFSohe.sample(frac=0.8, axis='index', random_state=42)\n",
    "data_test  = dfFSohe.drop(data_train.index) \n",
    "# catY = np.concatenate(([\"F\"],uv,[\"order\"]))\n",
    "catY = np.concatenate(([\"F\"],uv))\n",
    "x_train = data_train.drop(catY,axis=1)\n",
    "x_test = data_test.drop(catY,axis=1)\n",
    "# for function classification\n",
    "y_train = data_train[uv]\n",
    "y_test = data_test[uv]\n",
    "# for the identification of the order of the polynomial\n",
    "# yOrder_train = data_train[\"order\"]\n",
    "# yOrder_test = data_test[\"order\"]\n",
    "#\n",
    "t4pPC.centerTitle(\"Training set\")\n",
    "display(x_train)\n",
    "display(y_train)\n",
    "# display(yOrder_train[0:5])\n",
    "t4pPC.centerTitle(\"Test set\")\n",
    "display(x_test)\n",
    "display(y_test)\n",
    "# display(yOrder_test[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44552692-c700-48a5-8057-5ec54bdfcdf2",
   "metadata": {},
   "source": [
    "### First model: supervised learning of the polynomial type = classification with dense layers only\n",
    "\n",
    "#### Definition and compilation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11052d6-30e4-4a3f-8b1f-798b5839ce60",
   "metadata": {},
   "source": [
    "<div class=\"app\">\n",
    "\n",
    "**1.** Analyze the `modelCD` model defined below. Is it:\n",
    "- a dense NN?\n",
    "- a convolutional NN?\n",
    "- deep learning?\n",
    "\n",
    "**2.** Why is it the `categorical_crossentropy` loss function that is used?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adff833-4e01-4f84-9d44-42f24e5ef4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "nFeatures = x_train.shape[1]\n",
    "print(f\"nFeatures = {nFeatures}\")\n",
    "modelCD = keras.models.Sequential() #modelCD as classification with dense layers\n",
    "modelCD.add(keras.layers.Input((nFeatures,), name='iLayer'))\n",
    "modelCD.add(keras.layers.Dense(50, activation='relu', name='hLayer1'))\n",
    "modelCD.add(keras.layers.Dense(20, activation='relu', name='hLayer2'))\n",
    "modelCD.add(keras.layers.Dense(len(uv), activation='softmax', name='oLayer'))\n",
    "\n",
    "metrics = ['accuracy','mae']\n",
    "\n",
    "modelCD.compile(optimizer = 'adam',\n",
    "              loss      = 'categorical_crossentropy',\n",
    "              metrics   = metrics )\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "fig = plot_model(modelCD, dpi=300, show_dtype=True , show_shapes=True, show_layer_names=True, show_layer_activations=True)\n",
    "t4pPC.displayModel(fig, width = 600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e484e-00a7-4aa2-99ce-76a60341cd99",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94667402-ee56-44af-8de8-9e81b0287e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets all state generated by Keras\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "vID.chrono_start()\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=10, min_lr=0.001)\n",
    "modelCD.summary()\n",
    "vID.chrono_start()\n",
    "historyCD = modelCD.fit(x_train,\n",
    "                      y_train,\n",
    "                      epochs          = 200,\n",
    "                      batch_size      = 10,\n",
    "                      verbose         = 1,\n",
    "                      validation_data = (x_test, y_test),\n",
    "                      callbacks=[es,reduce_lr])\n",
    "vID.chrono_show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce12d207-d86b-4e97-85fc-9e73e3d784d0",
   "metadata": {},
   "source": [
    "#### Evaluation of the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4bd41b-2029-47e0-a126-a6ab5e4551f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotEpochs(historyCD,metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c230d-c886-4165-8358-4fd93b4f52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "printAcc(modelCD, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79e585-d961-40bf-ab61-68267d5dec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a0b9b-9e37-40c0-9e86-fe6e5427beef",
   "metadata": {},
   "source": [
    "<div class='introT' title = \"Confusion matrices\">\n",
    "\n",
    "Let's first consider a test, such as a blood test, that returns either YES (the patient has syndrome XX)  or NO (the patient does not have the suspected syndrome). A confusion matrix is a table used to investigate the performance of a classification model where the actual test values are known. It has two rows and two columns describing the **true positives**, **false positives**, **false negatives** and **true negatives**:\n",
    "\n",
    "<img width=\"300px\" src=\"./DS4B-svg/2by2ConfusionMatrix.svg\" style=\"display:block; margin-left:auto; margin-right:auto\" id=\"CNN\"/> \n",
    "\n",
    "In the field of machine learning and specifically the problem of statistical classification, a **confusion matrix**, also known as **error matrix** is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one.\n",
    "\n",
    "Each column of the matrix represents the instances in an actual class while each row represents the instances in a predicted class, or vice versa &ndash; both variants are found in the literature.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b794134-dc07-4dfc-8a3d-cfaaf0e06e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hat = modelCD.predict(x_train)\n",
    "y_test_hat = modelCD.predict(x_test)    \n",
    "\n",
    "plotConfusionMatrix(y_train, y_train_hat, y_test, y_test_hat, uv, saveSVGImg='./ML-SavedFigures/CNN/ConfusionMatrix.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15830d89-9357-40e1-b4b7-5094bd3ee063",
   "metadata": {},
   "source": [
    "#### Identification of structures from experimental RDF profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65537a6d-5e93-4b4e-975c-c96a7a983959",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaksDeca, _ = find_peaks(grDeca,height=1)\n",
    "peaksIco, _ = find_peaks(grIco,height=1)\n",
    "\n",
    "print(f\"Icosahedron. First intense peak found at: {rIco[peaksIco[0]]:.2f} Å. g(r) = {grIco[peaksIco[0]]:.2f}\")\n",
    "print(f\"Decahedron.  First intense peak found at: {rDeca[peaksDeca[0]]:.2f} Å. g(r) = {grDeca[peaksDeca[0]]:.2f}\")\n",
    "_ = plt.plot(rIco,grIco,label='exp icosahedron')\n",
    "_ = plt.plot(rDeca,grDeca,label='exp decahedron')\n",
    "_ = plt.xlabel(\"d / Å\")\n",
    "_ = plt.ylabel(\"G / Å$^{-2}$\")\n",
    "_ = plt.xlim(0,20)\n",
    "_ = plt.legend()\n",
    "_ = plt.show()\n",
    "\n",
    "print(len(rIco),rIco[1]-rIco[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa31bdc-5e1d-4c4b-afc5-5644a113bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Rmax)\n",
    "rIcoScaled, grIcoScaled = rescale1D(rIco,grIco,peaksIco[0],Rmax,0.025)\n",
    "rDecaScaled, grDecaScaled = rescale1D(rDeca,grDeca,peaksDeca[0],Rmax,0.025)\n",
    "peaksDecaScaled, _ = find_peaks(grDecaScaled,height=0.1)\n",
    "peaksIcoScaled, _ = find_peaks(grIcoScaled,height=0.1)\n",
    "\n",
    "print(f\"Icosahedron.       First intense peak found at: {rIco[peaksIcoScaled[0]]:.2f} Å. g(r) = {grIcoScaled[peaksIcoScaled[0]]:.2f}\")\n",
    "print(f\"Decahedron (pbpy). First intense peak found at: {rDecaScaled[peaksDecaScaled[0]]:.2f} Å. g(r) = {grDecaScaled[peaksDecaScaled[0]]:.2f}\")\n",
    "_ = plt.plot(rIcoScaled,grIcoScaled,label='icosahedron')\n",
    "_ = plt.plot(rDecaScaled,grDecaScaled,label='decahedron')\n",
    "_ = plt.xlabel(\"d / Å\")\n",
    "_ = plt.ylabel(\"G / Å$^{-2}$\")\n",
    "_ = plt.xlim(0,8)\n",
    "_ = plt.legend()\n",
    "_ = plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fde7dfc-14cc-424f-88d6-9852e7afe7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t4pPC.centerTitle(\"icosahedron\")\n",
    "gr = grIcoScaled.reshape(1,nFeatures)\n",
    "NPproba = modelCD.predict(gr)\n",
    "\n",
    "df = pd.DataFrame({'shape': uv,'probability': NPproba[0]})\n",
    "df_sorted = df.sort_values('probability')\n",
    "_ = df_sorted.plot(kind='barh', y='probability', x='shape',legend = False)\n",
    "_ = plt.xlabel('Probability')\n",
    "plt.show()\n",
    "\n",
    "#================================================================================\n",
    "t4pPC.centerTitle(\"decahedron\")\n",
    "gr = grDecaScaled.reshape(1,nFeatures)\n",
    "NPproba = modelCD.predict(gr)\n",
    "\n",
    "df = pd.DataFrame({'shape': uv,'probability': NPproba[0]})\n",
    "df_sorted = df.sort_values('probability')\n",
    "_ = df_sorted.plot(kind='barh', y='probability', x='shape',legend = False)\n",
    "_ = plt.xlabel('Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d19923-5486-44f5-9fee-2be0dc68ea4a",
   "metadata": {},
   "source": [
    "<div class=\"app\">\n",
    "The classification is in general not satisfying. \n",
    "\n",
    "**1.** Comment?\n",
    "\n",
    "**2.** Run again the code with a deep learning dense NN, with 100 neurons in the first layer and 50 in the second one\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd22dc7-024f-4743-b394-17fe2fe71f74",
   "metadata": {},
   "source": [
    "### Second model: supervised learning of the polynomial type = classification with 1D convolutional / pooling / dense layers\n",
    "\n",
    "#### Definition of the model\n",
    "\n",
    "<div class=\"introT\" title=\"conv1D input and output\">\n",
    "\n",
    "- **Input** = 3D tensor with shape `(batch, steps, channels)`\n",
    "    - `batch` = number of samples of the dataset\n",
    "- **Output** = 3D tensor with shape `(batch, new_steps, filters)`\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"introT\" title=\"conv1D main variables\">\n",
    "\n",
    "The `filters` parameters is just how many different windows you will have. (All of them with the same length, which is `kernel_size`). How many different results or channels you want to produce.\n",
    "\n",
    "When you use `filters = 100` and `kernel_size=4`, you are creating 100 different filters, each of them with length 4. The result will bring 100 different convolutions\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab2764-b997-486e-98f3-8e1cee84b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "nFeatures = x_train.shape[1]\n",
    "modelCC = keras.models.Sequential()\n",
    "modelCC.add(keras.layers.Input(shape=(nFeatures, 1), name=\"iLayer\"))\n",
    "modelCC.add(keras.layers.Conv1D(filters = 64, kernel_size = 40, padding = 'same', activation='relu'))\n",
    "modelCC.add(keras.layers.MaxPooling1D(pool_size=10))\n",
    "modelCC.add(keras.layers.Flatten())\n",
    "modelCC.add(keras.layers.Dense(20, activation='relu', name='hidden_Layer1'))\n",
    "modelCC.add(keras.layers.Dense(10, activation='relu', name='hidden_Layer2'))\n",
    "modelCC.add(keras.layers.Dense(len(uv), activation='softmax', name='output_Layer'))\n",
    "\n",
    "metrics = ['accuracy','mae']\n",
    "\n",
    "modelCC.compile(optimizer = 'adam',\n",
    "                loss      = 'categorical_crossentropy',\n",
    "                metrics   = metrics)\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "fig = plot_model(modelCC, dpi=300, show_dtype=True , show_shapes=True, show_layer_names=True, show_layer_activations=True)\n",
    "t4pPC.displayModel(fig, width = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e83138-026b-495f-91ea-dde04b795281",
   "metadata": {},
   "source": [
    "#### Reshaping of the data\n",
    "\n",
    "<div class=\"rqT\" title=\"reshape?\">\n",
    "\n",
    "Reshaping means changing the shape of an array.\n",
    "\n",
    "The shape of an array is the number of elements in each dimension. Wanna try it? <a href=\"./AdvancedPython.ipynb#reshape\">Load the \"AdvancedPython.ipynb\" notebook</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f16b77-fb22-4a2b-a236-e9b5f9adb440",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"shape of x_train = {x_train.shape}\")\n",
    "print(f\"shape of y_train = {y_train.shape}\")\n",
    "x_train4C1D = x_train.values.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "x_test4C1D = x_test.values.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "print(f\"shape of x_train4C1D = {x_train4C1D.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962798cd-dc19-43b6-b623-34c7d44bf601",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247fee3-ebc2-4057-a308-9e2a20ed0514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets all state generated by Keras\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "vID.chrono_start()\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=10, min_lr=0.001, mode='auto')\n",
    "\n",
    "historyCC = modelCC.fit(x_train4C1D,\n",
    "                      y_train,\n",
    "                      epochs          = 200,\n",
    "                      batch_size      = 10,\n",
    "                      verbose         = 1,\n",
    "                      validation_data = (x_test4C1D, y_test),\n",
    "                      callbacks=[es,reduce_lr])\n",
    "vID.chrono_show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97608563-c743-46a6-bdec-c1763ffad2e1",
   "metadata": {},
   "source": [
    "#### Evaluation of the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb3e29-b30b-4362-882c-8fde9bd2a0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotEpochs(historyCC,metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc1e76d-5554-4197-89fb-06ceb9276d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "printAcc(modelCC, x_train4C1D, y_train, x_test4C1D, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e87cf93-1f94-41c0-a054-b3b9bc45def2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hat = modelCC.predict(x_train)\n",
    "y_test_hat = modelCC.predict(x_test)    \n",
    "\n",
    "plotConfusionMatrix(y_train, y_train_hat, y_test, y_test_hat, uv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24016fff-7a33-480b-a000-50cdaacf4494",
   "metadata": {},
   "source": [
    "#### Identification of structures from experimental RDF profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7fcfa2-a0b2-4f65-ada4-a153db196813",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaksDeca, _ = find_peaks(grDeca,height=1)\n",
    "peaksIco, _ = find_peaks(grIco,height=1)\n",
    "\n",
    "print(f\"Icosahedron. First intense peak found at: {rIco[peaksIco[0]]:.2f} Å. g(r) = {grIco[peaksIco[0]]:.2f}\")\n",
    "print(f\"Decahedron.  First intense peak found at: {rDeca[peaksDeca[0]]:.2f} Å. g(r) = {grDeca[peaksDeca[0]]:.2f}\")\n",
    "_ = plt.plot(rIco,grIco,label='icosahedron')\n",
    "_ = plt.plot(rDeca,grDeca,label='decahedron')\n",
    "_ = plt.xlabel(\"d / Å\")\n",
    "_ = plt.ylabel(\"G / Å$^{-2}$\")\n",
    "_ = plt.xlim(0,20)\n",
    "_ = plt.legend()\n",
    "_ = plt.show()\n",
    "\n",
    "print(len(rIco),rIco[1]-rIco[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a08e9eb-f5c5-4629-8153-4713c3084781",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Rmax)\n",
    "rIcoScaled, grIcoScaled = rescale1D(rIco,grIco,peaksIco[0],Rmax,0.025)\n",
    "rDecaScaled, grDecaScaled = rescale1D(rDeca,grDeca,peaksDeca[0],Rmax,0.025)\n",
    "peaksDecaScaled, _ = find_peaks(grDecaScaled,height=0.1)\n",
    "peaksIcoScaled, _ = find_peaks(grIcoScaled,height=0.1)\n",
    "\n",
    "print(f\"Icosahedron.      First intense peak found at: {rIco[peaksIcoScaled[0]]:.2f} Å. g(r) = {grIcoScaled[peaksIcoScaled[0]]:.2f}\")\n",
    "print(f\"Decahedron (pbp). First intense peak found at: {rDecaScaled[peaksDecaScaled[0]]:.2f} Å. g(r) = {grDecaScaled[peaksDecaScaled[0]]:.2f}\")\n",
    "_ = plt.plot(rIcoScaled,grIcoScaled,label='exp icosahedron')\n",
    "_ = plt.plot(rDecaScaled,grDecaScaled,label='exp decahedron')\n",
    "_ = plt.xlabel(\"d / Å\")\n",
    "_ = plt.ylabel(\"G / Å$^{-2}$\")\n",
    "_ = plt.xlim(0,8)\n",
    "_ = plt.legend()\n",
    "_ = plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2933c-a938-4810-8bbc-63b74db5ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "t4pPC.centerTitle(\"icosahedron\")\n",
    "gr = grIcoScaled.reshape(1,nFeatures)\n",
    "NPproba = modelCC.predict(gr)\n",
    "\n",
    "df = pd.DataFrame({'shape': uv,'probability': NPproba[0]})\n",
    "df_sorted = df.sort_values('probability')\n",
    "_ = df_sorted.plot(kind='barh', y='probability', x='shape',legend = False)\n",
    "_ = plt.xlabel('Probability')\n",
    "plt.show()\n",
    "\n",
    "#================================================================================\n",
    "t4pPC.centerTitle(\"decahedron\")\n",
    "gr = grDecaScaled.reshape(1,nFeatures)\n",
    "NPproba = modelCC.predict(gr)\n",
    "\n",
    "df = pd.DataFrame({'shape': uv,'probability': NPproba[0]})\n",
    "df_sorted = df.sort_values('probability')\n",
    "_ = df_sorted.plot(kind='barh', y='probability', x='shape',legend = False)\n",
    "_ = plt.xlabel('Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7017ae2b-464a-4ea8-b03a-e2475365fa8f",
   "metadata": {},
   "source": [
    "<div class=\"rqE\">\n",
    "\n",
    "The CNN usually outperforms the DNN, but it misclassifies the icosahedron as a a decahedron, *i.e.* a pentagonal bipyramid.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef2c097-e067-4ce2-84b2-03689bae456c",
   "metadata": {},
   "source": [
    "<div class=\"exE\" title=\"Advanced Application\">\n",
    "\n",
    "Define a convolutional neural network that also learns from the first derivative of the rdf profile.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeeceb6-c3be-46e3-8d9e-214de2ca4480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d2023f-1f1b-477f-8974-dd819694f0f9",
   "metadata": {},
   "source": [
    "# Bibliography and useful links\n",
    "\n",
    "**General resources**\n",
    "\n",
    "- [Convolution article on Wikipedia](https://en.wikipedia.org/wiki/Convolution)\n",
    "- [Hands-on machine learning with scikit-learn, keras and tensorflow (3rd edition, **2022**), Aurélien Géron, O'Reilly editions](https://github.com/ageron/handson-ml3)\n",
    "- Some images were generated with [DALL·E](https://openai.com/dall-e-2)\n",
    "- [DeepMath :  : Mathématiques des réseaux de neurones (in French)](https://exo7math.github.io/deepmath-exo7/)\n",
    "- [Neural Network Models for Combined Classification and Regression](https://machinelearningmastery.com/neural-network-models-for-combined-classification-and-regression/)\n",
    "\n",
    "**Articles**\n",
    "\n",
    "- [Fages, Jolibois, Poteau, Recognition of the three-dimensional structure of small metal nanoparticles by a supervised artificial neural network (**2021**) *Theoret. Chem. Acc.*](https://doi.org/10.1007/s00214-021-02795-0)\n",
    "- [Terban, Billinge, Structural Analysis of Molecular Materials Using the Pair Distribution Function (**2022**) *Chem. Rev.* ](https://doi.org/10.1021/acs.chemrev.1c00237)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7d03ca-c5f7-4b80-8865-d2ac47aebc72",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1998a632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vID.end(cwd0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a59e2c-d53a-4bf1-a59a-28175c746361",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "<div class=\"intro\">\n",
    "\n",
    "Concatenation of the RDF profiles into dataframes, then saved into csv files\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1433a592-6be4-42a4-a81b-264c28a56c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfF = pd.DataFrame()\n",
    "dfFico = pd.DataFrame()\n",
    "dfFpbpy = pd.DataFrame()\n",
    "dfFOh = pd.DataFrame()\n",
    "dfY = pd.DataFrame(columns=[\"F\"])\n",
    "import os, sys\n",
    "\n",
    "pathway = \"./ML-data/RDFs/simulRDF/\"\n",
    "\n",
    "shapes = []\n",
    "for root, dir_names, file_names in os.walk(pathway):\n",
    "    print(f\"The root is {root}\")\n",
    "    print(f\"The directory name is: {dir_names}\")\n",
    "    i = -1\n",
    "    if not dir_names:\n",
    "        i += 1\n",
    "        parent_folder = root.split(\"/\")\n",
    "        parent_folder = parent_folder[-1]\n",
    "        print(f\"Parent folder = category = {parent_folder}\")\n",
    "        print(f\"The file names are: {file_names}\")\n",
    "        print(f\"*\"*40)\n",
    "        for f in sorted(file_names):\n",
    "            fPrefix = f.split(\".\")[0]\n",
    "            fPathway = root + \"/\" + f\n",
    "            dfFi = pd.read_csv(fPathway, sep='\\t', usecols=[1,2])\n",
    "            dfFi.rename(columns={'g': fPrefix}, inplace=True)\n",
    "            deltaR = dfFi['r'].iloc[1] - dfFi['r'].iloc[0]\n",
    "            if i == 0:\n",
    "                deltaRRef = deltaR\n",
    "            else:\n",
    "                if deltaR != deltaRRef: sys.exit(f\"The {f} RDF profile does not have the same Δr step as the previously read profiles\")\n",
    "            dfF = pd.concat([dfF,dfFi[fPrefix]],axis=1).fillna(0)\n",
    "            if parent_folder == 'ico': dfFico = pd.concat([dfFico,dfFi[fPrefix]],axis=1).fillna(0)\n",
    "            if parent_folder == 'pbpy': dfFpbpy = pd.concat([dfFpbpy,dfFi[fPrefix]],axis=1).fillna(0)\n",
    "            if parent_folder == 'regfccOh': dfFOh = pd.concat([dfFOh,dfFi[fPrefix]],axis=1).fillna(0)\n",
    "            dfY.loc[fPrefix] = [parent_folder]\n",
    "    else:\n",
    "        print(f\"=\"*20)\n",
    "\n",
    "## add r\n",
    "dfF[\"r\"] = dfF.index*deltaRRef\n",
    "rColumn = dfF.pop(\"r\")\n",
    "dfF.insert(0, \"r\", rColumn)\n",
    "\n",
    "dfFico[\"r\"] = dfFico.index*deltaRRef\n",
    "rColumn = dfFico.pop(\"r\")\n",
    "dfFico.insert(0, \"r\", rColumn)\n",
    "\n",
    "dfFpbpy[\"r\"] = dfFpbpy.index*deltaRRef\n",
    "rColumn = dfFpbpy.pop(\"r\")\n",
    "dfFpbpy.insert(0, \"r\", rColumn)\n",
    "\n",
    "dfFOh[\"r\"] = dfFOh.index*deltaRRef\n",
    "rColumn = dfFOh.pop(\"r\")\n",
    "dfFOh.insert(0, \"r\", rColumn)\n",
    "\n",
    "display(dfF)\n",
    "display(dfY)\n",
    "dfF.to_csv(pathway+\"RDF_profiles.csv\",sep=\"\\t\")\n",
    "dfY.to_csv(pathway+\"RDF_labels.csv\",sep=\"\\t\")\n",
    "dfFico.to_csv(pathway+\"RDF_profiles_ico.csv\",sep=\"\\t\")\n",
    "dfFpbpy.to_csv(pathway+\"RDF_profiles_pbpy.csv\",sep=\"\\t\")\n",
    "dfFOh.to_csv(pathway+\"RDF_profiles_Oh.csv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f7a0c7-995a-4af2-95c2-6b069fa80a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
