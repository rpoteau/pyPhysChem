# after ChatGPT 5.0 instant answer, 2025-09-28

t4pPC.centerTitle("Definition of the model")
# Resets all state generated by Keras
tf.keras.backend.clear_session()

input = Input(shape=(128, 128, 1))

x = layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding = 'same', input_shape=(128, 128, 1))(input)
x = layers.MaxPool2D((2, 2))(x)
x = layers.Dropout(0.3)(x)

x = layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding = 'same')(x)
x = layers.MaxPool2D((2, 2))(x)
x = layers.Dropout(0.3)(x)

x = layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding = 'same')(x)
x = layers.MaxPool2D((2, 2))(x)
x = layers.Dropout(0.3)(x)

# --- Global Average Pooling instead of Flatten ---
# x = layers.GlobalAveragePooling2D()(x)
x = layers.Flatten()(x)
x = layers.Dense(64, activation='relu')(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(3, activation='softmax')(x)

model = models.Model(input, x)

#from tensorflow.keras.optimizers import SGD
#optimizer = SGD(learning_rate=0.1)
optimizer = 'adam'

model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=["accuracy"])
model.summary()

t4pPC.centerTitle("Shape of the model")
from tensorflow.keras.utils import plot_model
fig = plot_model(model, dpi=300, show_dtype=True , show_shapes=True, show_layer_names=True, show_layer_activations=True)
t4pPC.displayModel(fig, width = 600)

nepochs = 40

t4pPC.centerTitle("Fitting of the parameters of the model")
# Define early stopping
early_stop = EarlyStopping(
    monitor='val_loss',    # you can also use 'val_accuracy'
    patience=5,            # number of epochs with no improvement before stopping
    restore_best_weights=True # roll back to the best model
)

# Train the model with early stopping
history = model.fit(
    training_data,
    validation_data=val_data,
    epochs=nepochs,
    callbacks=[early_stop]
)

t4pPC.centerTitle("Accurracy and loss function")
accuracy = history.history['accuracy'][1:]
loss = history.history['loss'][1:]
val_accuracy = history.history['val_accuracy'][1:]
val_loss = history.history['val_loss'][1:]

#plot accuracy for model
plt.plot(range(len(accuracy)), accuracy, marker= 'o', label = 'Accuracy')
plt.plot(range(len(val_accuracy)), val_accuracy, marker= 's', label = 'Val_accuracy')
plt.xlabel('# Epochs')
plt.ylim(0.0,1.00)
plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(2))
plt.ylabel('')
plt.title('Accuracy and Validation Accuracy wrt Epochs')
plt.legend()
plt.show()

#plot loss for model
plt.plot(range(len(loss)), loss, marker= 'o', label = 'loss')
plt.plot(range(len(val_loss)), val_loss, marker= 's', label = 'val_loss')
plt.xlabel('# Epochs')
plt.ylabel('')
plt.ylim(0.0,1.4)
plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(2))
plt.title('Loss and Validation Loss wrt Epochs')
plt.legend()
plt.show()

t4pPC.centerTitle("Confusion matrix")
true_classes = test_data.classes
class_labels = list(test_data.class_indices.keys())
#test_data
predictions = model.predict(test_data)
predicted_classes = np.argmax(predictions, axis=1)

cm = metrics.confusion_matrix(true_classes, predicted_classes)
plot_confusion_matrix(cm,['circle', 'square', 'triangle'])
plt.show()

