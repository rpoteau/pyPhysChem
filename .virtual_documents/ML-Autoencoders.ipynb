import os
import sys

cwd0 = './config/'
sys.path.append(cwd0)

import visualID_Eng as vID
from visualID_Eng import color, fg, bg, hl
vID.init(cwd0)
import tools4pyPhysChem as t4pPC

#cancel the "last show-up" behaviour of Jupyter notebooks
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"








import sklearn
import tensorflow as tf
import keras
import numpy as np
import sklearn

import pandas as pd
from IPython.display import display

import matplotlib.pyplot as plt

plt.rc('font', size=14)
plt.rc('axes', labelsize=14, titlesize=14)
plt.rc('legend', fontsize=14)
plt.rc('xtick', labelsize=10)
plt.rc('ytick', labelsize=10)

tf.config.experimental.list_physical_devices()
if not tf.config.list_physical_devices('GPU'):
    print("No GPU was detected. Neural nets can be very slow without a GPU.")

from pathlib import Path

IMAGES_PATH = Path() / "ML-SavedFigures/autoencoders"
IMAGES_PATH.mkdir(parents=True, exist_ok=True)

MODELS_PATH = Path() / "ML-SavedModels/autoencoders"
MODELS_PATH.mkdir(parents=True, exist_ok=True)

def get_var_name(arg):
    return [k for k,v in globals().items() if v == arg]

def save_fig(fig_id, IMAGES_PATH=".", tight_layout=True, fig_extension="png", resolution=300):
    '''
    saves a figure to a given subfolder, that will be created if it does not exist
    input:
        - fig_id = name of the figure, without an extension
        - IMAGES_PATH = pathway to the folder. Default: "."
        - tight_layout = boolean. Automatically adjusts subplot params so that the subplot(s) fits in to the figure area. Default: True
        - fig_extension = image type. Default: png
        - resolution = resolution of the figure, in dpi. Default: 300
    '''
    path = IMAGES_PATH / f"{fig_id}.{fig_extension}"
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)
    return

def saveModel(model, model_id, MODELS_PATH=".", model_extension="keras"):
    '''
    saves a model to a given subfolder, that will be created if it does not exist
    input:
        - model = keras model
        - model_id = name of the model file, without an extension
        - MODELS_PATH = pathway to the folder. Default: "."
        - model_extension = model type. Default: keras
    '''
    path = MODELS_PATH / f"{model_id}.{model_extension}"
    model.save(path)
    return

def saveDataFrame(df, df_id, MODELS_PATH=".", df_extension="xlsx"):
    
    path = MODELS_PATH / f"{df_id}.{df_extension}"
    print(f"Saving {path}")
    if df_extension == 'xlsx':
        df.to_excel(path, engine='xlsxwriter') 
    elif df_extension == 'pickle':
        df.to_pickle(path)
    return

def loadDataFrame(df_id, MODELS_PATH=".", df_extension="xlsx"):

    import pandas as pd
    path = MODELS_PATH / f"{df_id}.{df_extension}"
    print(f"Loading {path}")
    if df_extension == 'xlsx':
        return pd.read_excel(path, index_col=0) 
    elif df_extension == 'pickle':
        return pd.read_pickle(path)

def plotModel(model, model_id, MODELS_PATH=".", fig_extension="png", resolution=300,
              show_shapes=True, show_layer_names=True, show_layer_activations=True):
    '''
    saves a model plot to a given subfolder, that will be created if it does not exist
    input:
        - model = keras model
        - model_id = name of the model file, without an extension
        - MODELS_PATH = pathway to the folder. Default: "."
        - model_extension = model type. Default: png
    '''
    from tensorflow.keras.utils import plot_model
    from matplotlib.pyplot import plot
    path = MODELS_PATH / f"{model_id}.{fig_extension}"
    plot_model(model, to_file=path, dpi=resolution, show_shapes=show_shapes, 
               show_layer_names=show_layer_names, show_layer_activations=show_layer_activations)
    img = plt.imread(path)
    plt.imshow(img)
    plt.axis("off")
    plt.show()
    return

def plotEpochs(history,metrics):
    import matplotlib.pyplot as plt
    import pandas as pd
    import numpy as np
    # c'est pratique d'utiliser un dataframe et les fonctions de tracé associées
    df=pd.DataFrame(data=history.history)

    nEpoch = df.shape[0]
    print(nEpoch)
    
    plt.rcParams["figure.figsize"] = (8,4)
    # on va d'abord tracer les courbes de la fonction qui a été minimisée au cours de l'apprentissage
    figLOSS=df.plot(y=["loss","val_loss"],linestyle='-', marker='o',fontsize=14)
    figLOSS.set_xlabel('epoch',fontdict={'fontsize':16})
    figLOSS.set_ylabel('loss',fontdict={'fontsize':16})
    yMin = np.min(df[["loss","val_loss"]].iloc[nEpoch//3:nEpoch,:].to_numpy())
    yMax = np.max(df[["loss","val_loss"]].iloc[nEpoch//3:nEpoch,:].to_numpy())
    yMin = round(yMin-0.05,1)
    yMax = round(yMax+0.05,1)
    print(yMin,yMax)
    figLOSS.set_ylim([yMin,yMax])
    figLOSS.legend(loc='upper right', shadow=True, fontsize='x-large')
    plt.show()

    for i in range(len(metrics)):
        ErrTraining = metrics[i]
        ErrVal = "val_" + ErrTraining
        figACC=df.plot(y=[ErrTraining,ErrVal],linestyle='-', marker='o',fontsize=14)
        figACC.set_xlabel('epoch',fontdict={'fontsize':16})
        figACC.set_ylabel(ErrTraining,fontdict={'fontsize':16})
        yMin = np.min(df[[ErrTraining,ErrVal]].iloc[nEpoch//3:nEpoch,:].to_numpy())
        yMax = np.max(df[[ErrTraining,ErrVal]].iloc[nEpoch//3:nEpoch,:].to_numpy())
        yMin = round(yMin-0.05,1)
        yMax = round(yMax+0.05,1)
        print(yMin,yMax)
        figACC.set_ylim([yMin,yMax])
        figACC.legend(loc='lower right', shadow=True, fontsize='x-large')
        plt.show()








def plotFashionToGrid(X, y, fashion_names, nrows=5, ncols=8, startFrom=0, yActual=None, saveFig = False, IMAGES_PATH=".",save_imgs=False, fig_extension="png"):
    '''
    plots nrows x ncols images with plt.imshow(), with their label as title
    input:
        - X = array with nimg b&w images coded as (npix,npix). Size = (nimg, npix, npix)  
        - y = 1D array made of nimg integers = class number
        - fashion_names = array with the corresponding class names
        - nrows, ncols = dimension of the plot grid (nrows x ncols must be <= nimg)
        - startFrom = index of the first image that will be shown
        - yActual = only useful when evaluating a prediction. It will contain the
          actual integer labels of the images
          In this case, y must be set to the value predicted by the model
          When a prediction is wrong, the label will be shown in red
        - saveFig = saves the figure to IMAGES_PATH
        - IMAGES_PATH = pathway to save images. Default: "."
    '''
    from matplotlib import pyplot as plt
    from PIL import Image

    plt.rc('font', size=14)
    plt.rc('axes', labelsize=14, titlesize=12)
    plt.rc('legend', fontsize=14)
    plt.rc('xtick', labelsize=10)
    plt.rc('ytick', labelsize=10)
    plt.figure(figsize=(ncols * 1.2, nrows * 1.2))
    nimg = nrows * ncols
    if nimg > len(y) - startFrom:
        print(f"Sample size = {len(y)}. Cannot plot {nrows} x {ncols} = {nimg} images, with startFrom set as {startFrom}")
        print(f"startFrom automatically setup to {len(y) - nimg}")
        startFrom = len(y) - nimg
    for row in range(nrows):
        for col in range(ncols):
            index = ncols * row + col + startFrom
            plt.subplot(nrows, ncols, index + 1 - startFrom) # the third argument represents the index of the subplot that will be used
            plt.imshow(X[index], cmap="binary", interpolation="nearest",vmin=0.0, vmax=1.0) # the data X is resampled to the pixel size of the image on the figure canvas, using the "nearest" interpolation method
            plt.axis('off')
            colorTitle = 'black'
            if yActual is not None and yActual[index] != y[index]:
                colorTitle='red'
            plt.title(fashion_names[y[index]],color=colorTitle)
            if save_imgs:
                file = IMAGES_PATH + f"img{index:06d}.{fig_extension}" 
                I8 = (((X[index] - X[index].min()) / (X[index].max() - X[index].min())) * 255.9).astype(np.uint8)
                img = Image.fromarray(I8)
                img.save(file)

    plt.subplots_adjust(wspace=0.2, hspace=0.5)
    if (saveFig):
        save_fig(IMAGES_PATH,"fashion_mnist_grid",fig_extension=fig_extension)
    plt.show()
    return

def compareReconstructionsWithOriginalImages(model, imgDataset, nimg=5, startFrom=0, saveFig = False, IMAGES_PATH="."):

    selectedImg = imgDataset[startFrom:startFrom+nimg]
    reconstrucedImg = model.predict(selectedImg)
    
    plt.rc('font', size=14)
    plt.rc('axes', labelsize=14, titlesize=12)
    plt.rc('legend', fontsize=14)
    plt.rc('xtick', labelsize=10)
    plt.rc('ytick', labelsize=10)
    plt.figure(figsize=(nimg*1.5, 2*1.5)) #1 row for actual images, 1 row for reconstructed images
    if nimg > imgDataset.shape[0] - startFrom:
        print(f"Dataset size = {imgDataset.shape[0]}. Cannot plot {nimg} images, with startFrom set as {startFrom}")
        print(f"startFrom automatically setup to {imgDataset.shape[0] - nimg}")
        startFrom = imgDataset.shape[0] - nimg
    for img_index in range(nimg):
        plt.subplot(2, nimg, img_index+1) # the third argument represents the index of the subplot that will be used
        plt.imshow(selectedImg[img_index], cmap='binary', interpolation="nearest",vmin=0.0, vmax=1.0) # the data X is resampled to the pixel size of the image on the figure canvas, using the "nearest" interpolation method
        plt.axis('off')
        plt.subplot(2, nimg, nimg+img_index+1) # the third argument represents the index of the subplot that will be used
        plt.imshow(reconstrucedImg[img_index], cmap='binary', interpolation="nearest",vmin=0.0, vmax=1.0) # the data X is resampled to the pixel size of the image on the figure canvas, using the "nearest" interpolation method
        plt.axis('off')
    plt.subplots_adjust(wspace=0.2, hspace=0.5)
    if (saveFig):
        save_fig(IMAGES_PATH,"fashion_mnist_autoencoder")
    plt.show()
    return

def ShowLatentSpaceRepresentation(encoder, decoder, ae, imgDataset, img_index):
    img = imgDataset[img_index].reshape(1,imgDataset.shape[1],imgDataset.shape[2])
    LatentSpaceRepresentation = encoder.predict(img)
    Reconstructed_img = decoder.predict(LatentSpaceRepresentation)
    print(Reconstructed_img.shape)
    plt.rc('font', size=14)
    plt.rc('axes', labelsize=14, titlesize=12)
    plt.rc('legend', fontsize=14)
    plt.rc('xtick', labelsize=10)
    plt.rc('ytick', labelsize=10)
    plt.figure(figsize=(24, 2))
    plt.subplots(1, 3, gridspec_kw={"width_ratios": [1, 2, 1]})
    plt.subplot(1, 3, 1) 
    plt.imshow(imgDataset[img_index], cmap='binary', interpolation="nearest",vmin=0.0, vmax=1.0)
    plt.axis('off')
    plt.subplot(1, 3, 2)
    plt.imshow(LatentSpaceRepresentation, cmap='binary', interpolation="nearest")
    plt.yticks([])
    plt.subplot(1, 3, 3) 
    plt.imshow(Reconstructed_img.reshape(28,28), cmap='binary', interpolation="nearest",vmin=0.0, vmax=1.0) 
    plt.axis('off')
    plt.show()
    return LatentSpaceRepresentation, Reconstructed_img





fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist

imgW = X_train_full[0].shape[1]
imgH = X_train_full[0].shape[0]
print(f"Size of images = W: {imgW} pixels x H: {imgH} pixels = {imgW*imgH} pixels")

fashion_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
               "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]
nclass = len(fashion_names)

X_train_full = X_train_full/255.0
X_test = X_test/255.0

t4pPC.centerTitle("Sample of the fashion items")
plotFashionToGrid(X_train_full, y_train_full, fashion_names, nrows=5, ncols=8, startFrom=100, IMAGES_PATH="./ML-Figures/FashionMNIST/IMGS/", save_imgs=True)

t4pPC.centerTitle("Fashion names")
for i, f in enumerate(fashion_names):
    print(f"{i:2d}: {f}")

X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]
X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]
t4pPC.centerTitle("Training set")
X_train.shape
y_train.shape
t4pPC.centerTitle("Validation set")
X_valid.shape
y_valid.shape
t4pPC.centerTitle("Test set")
X_test.shape
y_test.shape







tf.keras.backend.clear_session()

tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU

stacked_encoder = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[imgH,imgW]),
    tf.keras.layers.Dense(100, activation="relu"),
    tf.keras.layers.Dense(28, activation="relu"),
])
stacked_decoder = tf.keras.Sequential([
    tf.keras.layers.Dense(100, activation="relu"),
    tf.keras.layers.Dense(imgW * imgH),
    tf.keras.layers.Reshape([imgH, imgW])
])
stacked_aeModel = tf.keras.Sequential([stacked_encoder, stacked_decoder])

t4pPC.centerTitle("Summary of the model")
stacked_encoder.build()
stacked_decoder.build()
stacked_aeModel.build()
stacked_aeModel.summary()

t4pPC.centerTitle("Plot models")
plotModel(stacked_encoder, "StackedEncoder", MODELS_PATH=MODELS_PATH)
print()
plotModel(stacked_decoder, "StackedDecoder", MODELS_PATH=MODELS_PATH)



stacked_aeModel.compile(loss="mse", optimizer="nadam")
print(X_train.shape)





history_ae = stacked_aeModel.fit(X_train,
                                 X_train,
                                 epochs=20,
                                 batch_size=50,
                                 validation_data=(X_valid, X_valid)
                                )





from varname import nameof
file = MODELS_PATH / f"{nameof(stacked_aeModel)}.keras"
stacked_aeModel.save(file)

file = MODELS_PATH / f"{nameof(stacked_encoder)}.keras"
stacked_encoder.save(file)

file = MODELS_PATH / f"{nameof(stacked_decoder)}.keras"
stacked_decoder.save(file)





from varname import nameof
file = MODELS_PATH / f"{nameof(stacked_aeModel)}.keras"
stacked_aeModel = keras.models.load_model(file)

file = MODELS_PATH / f"{nameof(stacked_encoder)}.keras"
stacked_encoder = keras.models.load_model(file)

file = MODELS_PATH / f"{nameof(stacked_decoder)}.keras"
stacked_decoder = keras.models.load_model(file)





compareReconstructionsWithOriginalImages(stacked_aeModel,X_test)









lsr, rimg = ShowLatentSpaceRepresentation(stacked_encoder,stacked_decoder, stacked_aeModel, X_test,0)





print(lsr)
plt.imshow(rimg.reshape(28,28), cmap=plt.cm.binary, interpolation="nearest"), plt.show()
lsrN = (lsr - lsr.min()) / (lsr.max()- lsr.min())
print(lsrN)
print(lsrN.shape)
rimgN = stacked_decoder.predict(lsrN)
plt.imshow(rimgN.reshape(28,28), cmap=plt.cm.binary, interpolation="nearest"), plt.show()





import random
import numpy as np
random.seed(42)
rndlsr = [random.uniform(0.,2.) for i in range(28)]
print(len(rndlsr))
rndlsr=np.array(rndlsr)
rndlsr = rndlsr.reshape((1,-1))
print(rndlsr)
print(rndlsr.shape)
plt.imshow(rndlsr, cmap=plt.cm.binary, interpolation="nearest")
plt.yticks([])
plt.show()


rndrimg = stacked_decoder.predict(rndlsr)
plt.imshow(rndrimg.reshape(28,28), cmap=plt.cm.binary, interpolation="nearest")
plt.show()





cm = 1/2.54
ncols = 8
nrows = 3
plt.figure(figsize=(ncols*4*cm, nrows*5.5*cm))
indexLSR = 0
indexIMG = 0
index = 0
for row in range(nrows):
    for col in range(ncols):
        index += 1
        indexLSR = (row)*2*ncols + col + 1
        indexIMG = indexLSR + ncols
        lsrPixByPix = np.zeros_like(rndlsr)
        lsrPixByPix[0,index-1] = 1.0
        imgPixByPix = stacked_decoder.predict(lsrPixByPix,verbose=0)
        _ = plt.subplot(nrows*2, ncols, indexLSR)
        _ = plt.imshow(lsrPixByPix, cmap=plt.cm.binary, interpolation="nearest")
        _ = plt.yticks([])
        _ = plt.subplot(nrows*2, ncols, indexIMG)
        _ = plt.imshow(imgPixByPix.reshape(28,28), cmap=plt.cm.binary, interpolation="nearest")
        _ = plt.axis("off")
plt.tight_layout()
plt.show()






fh_img = plt.imread("./ML-Figures/FashionMNIST/FreehandDrawing_Shirt_28x28.png")
fh_img = fh_img.reshape(1,fh_img.shape[0],fh_img.shape[1]); plt.show()
lsr, rimg = ShowLatentSpaceRepresentation(stacked_encoder,stacked_decoder, stacked_aeModel, fh_img,0)


fh_img = plt.imread("./ML-Figures/FashionMNIST/FreehandDrawing_Sneaker_28x28.png")
fh_img = fh_img.reshape(1,fh_img.shape[0],fh_img.shape[1]); plt.show()
lsr, rimg = ShowLatentSpaceRepresentation(stacked_encoder,stacked_decoder, stacked_aeModel, fh_img,0)


fh_img = plt.imread("./ML-Figures/FashionMNIST/FreehandDrawing_SneakerVertical_28x28.png")
fh_img = fh_img.reshape(1,fh_img.shape[0],fh_img.shape[1]); plt.show()
lsr, rimg = ShowLatentSpaceRepresentation(stacked_encoder,stacked_decoder, stacked_aeModel, fh_img,0)





t4pPC.centerTitle('Original image')
fh_img = plt.imread("./ML-Figures/FashionMNIST/img000100.png")
fh_img = fh_img.reshape(1,fh_img.shape[0],fh_img.shape[1]); plt.show()
lsr, rimg = ShowLatentSpaceRepresentation(stacked_encoder,stacked_decoder, stacked_aeModel, fh_img,0)

t4pPC.centerTitle('Image with defect. 1')
fh_img = plt.imread("./ML-Figures/FashionMNIST/img000100Defect1.png")
fh_img = fh_img.reshape(1,fh_img.shape[0],fh_img.shape[1]); plt.show()
lsr, rimg = ShowLatentSpaceRepresentation(stacked_encoder,stacked_decoder, stacked_aeModel, fh_img,0)

t4pPC.centerTitle('Image with defect. 2')
fh_img = plt.imread("./ML-Figures/FashionMNIST/img000100Defect2.png")
fh_img = fh_img.reshape(1,fh_img.shape[0],fh_img.shape[1]); plt.show()
lsr, rimg = ShowLatentSpaceRepresentation(stacked_encoder,stacked_decoder, stacked_aeModel, fh_img,0)






from sklearn.manifold import TSNE
X_valid_compressed = stacked_encoder.predict(X_valid)
tsne = TSNE(init="pca", learning_rate="auto", random_state=42)
X_valid_2D = tsne.fit_transform(X_valid_compressed)
print(X_valid_2D)

print(y_valid)


import matplotlib as mpl
from matplotlib.lines import Line2D
cmap = plt.cm.tab10
t4pPC.centerTitle('Fashion indexes, names and colors in the selected color map scheme')
LegendElements = []
for i, f in enumerate(fashion_names):
    print(i, f, cmap(i))
    LegendElement = Line2D([0], [0], marker='o', color='w', label=f, markerfacecolor=cmap(i), markersize=15)
    LegendElements.append(LegendElement)

t4pPC.centerTitle('TSNE scheme')
Z = X_valid_2D.copy()
Z = (Z - Z.min()) / (Z.max() - Z.min())  # normalize to the 0-1 range
cm = 1/2.54
fig, ax = plt.subplots(figsize=(30*cm, 20*cm))
scatter = ax.scatter(Z[:, 0], Z[:, 1], c=y_valid, s=10, cmap="tab10")
fig.legend(handles=LegendElements, loc='center right',fontsize=10)

image_positions = np.array([[1., 1.]])
for index, position in enumerate(Z):
    dist = ((position - image_positions) ** 2).sum(axis=1)
    if dist.min() > 0.02: # if far enough from other images
        image_positions = np.r_[image_positions, [position]]
        imagebox = mpl.offsetbox.AnnotationBbox(
            mpl.offsetbox.OffsetImage(X_valid[index], cmap="binary"),
            position, bboxprops={"edgecolor": cmap(y_valid[index]), "lw": 2})
        _ = ax.add_artist(imagebox)
        
fig.subplots_adjust(right=0.88)
plt.show()








from rdkit import Chem
Chem.MolFromSmiles('C[C@@H](CN1c2ccccc2Sc2ccc(C#N)cc21)C[NH+](C)C')


import re
from rdkit.Chem import PandasTools,Draw
from IPython.display import display
import pickle
# Fix seed for reproducible results
SEED = 22

def canonicalize_smiles(smiles):
    '''This function takes a non-canonical SMILES and returns the canonical version
    
    Args: 
        -smiles: str, non-canonical SMILES of a molecule
    
    Out:
        - canonical_smiles: str, canonical SMILES of the molecule
    '''
    
    mol = Chem.MolFromSmiles(smiles) #create a mol object from input smiles.
    can = Chem.MolToSmiles(mol) #convert the previous mol object to SMILES. By default, RDKit canonicalizes the SMILES when applying this MolToSmiles() function

    return can

class tokenization:
    def __init__(self,
                 smilesFile,separator='\t',
                 smilesHeader = 'Smiles',
                 reduceNumberOfMolTo=None,
                 debug=False,
                ):
        self.df = pd.read_csv(smilesFile,sep=separator)
        if reduceNumberOfMolTo is not None:
            self.df=self.df.sample(n=reduceNumberOfMolTo)
            msg = f', reduced to {reduceNumberOfMolTo} samples'
        else:
            msg = ''
        t4pPC.centerTitle(f"Reading the {smilesFile} file"+msg)
        smilesHeaderCan = "Canonicalized " + smilesHeader
        self.df[smilesHeaderCan] = self.df[smilesHeader].apply(canonicalize_smiles)
        display(self.df)
        self.smiles2AtomLevelTokens(smilesHeader, debug)
        self.atomicLevelTokens2Pixels()
        PandasTools.AddMoleculeColumnToFrame(self.df, smilesCol=smilesHeaderCan)
        PandasTools.RenderImagesInAllDataFrames(True)
        display(self.df)
        
    def atomLevelTokenization(self, data, with_begin_and_end=True):        # fonction to tokenize
        """Tokenizes a SMILES string."""
        REGEXPS = {                                           # group of characters that we want to tokenize
                "brackets": re.compile(r"(\[[^\]]*\])"),
                "2_ring_nums": re.compile(r"(%\d{2})"),
                # "2char_symbols": re.compile(r"(Cl|Br)")
                "2char_symbols": re.compile(r"(He|Li|Be|Ne|Na|Mg|Al|Si|Cl|Br|At)")
            }
        
        REGEXP_ORDER = ["brackets", "2_ring_nums", "2char_symbols"]
    
        def split_by(data, regexps):
            if not regexps:
                return list(data)
            regexp = REGEXPS[regexps[0]]
            splitted = regexp.split(data)
            tokens = []
            for i, split in enumerate(splitted):
                if i % 2 == 0:
                    tokens += split_by(split, regexps[1:])
                else:
                    tokens.append(split)
            return tokens
    
        tokens = split_by(data, REGEXP_ORDER)
        
        if with_begin_and_end:
            tokens = ["^"] + tokens + ["$"]
        return tokens
    
    def smiles2AtomLevelTokens(self, NameOfSmilesColumn,debug=False):
        smiles2TokensList=[]
        if debug: t4pPC.centertxt("Tokens for each SMILES",size=14,weight='bold',bgc="#82c3c3",fgc="black")
        for i,smiles in enumerate(self.df[NameOfSmilesColumn]):
            smiles_tokeniser=self.atomLevelTokenization(smiles, with_begin_and_end=False)
            smiles2TokensList=smiles2TokensList+[smiles_tokeniser]
            if debug: print(f"{smiles}, {smiles2TokensList[i]}")
        # Dictionary
        t4pPC.centertxt("Smiles to atom-level tokens",size=14,weight='bold',bgc="#82c3c3",fgc="black")
        counter = {}
        
        longestWord = ""
        maxLength = 0
        
        # Cycle through each sentence in the list
        for sentence in smiles2TokensList:
            # Cycle through each character in the list tokken
            for charactr in sentence:
                # If the character is already in the dictionary, increment the count
                if charactr in counter:
                    counter[charactr] += 1
                # Otherwise, add the character to the dictionary with a count of 1
                else:
                    counter[charactr] = 1
        
            if len(sentence) > maxLength:
                maxLength = len(sentence)
                longestWord = sentence
                nUniqueTokens = len(np.unique(np.array(sentence)))
            
        dictionary = list(counter.keys())
        
    
        if debug:
            for charactr, nombre in counter.items():
                print(f"there is/are {nombre:4d} '{charactr}'...")
            print("... in the database")
            print("------------------------")
            print(f"Total = {len(counter)} tokens")
            print("------------------------")
        
        print("dictionary :", dictionary)
        print()
        print(f"the longuest word is made of {maxLength} tokens, built on {nUniqueTokens} unique tokens:{fg.BLUE}{longestWord}{hl.OFF}")
        self.df['smiles2Tokens'] = smiles2TokensList
        self.dictionary = dictionary
        self.longestWord = longestWord
        
    def atomicLevelTokens2Pixels(self):
        '''
        Transforms tokenized SMILES into a token-based pixelized picture 
        '''
        t4pPC.centertxt("Tokenized smiles transformed into pixelized images",size=14,weight='bold',bgc="#82c3c3",fgc="black")
        dictLength=len(self.dictionary)
        index_dict = {charactr: idx for idx, charactr in enumerate(self.dictionary)}
        pixels = []
        for token in self.df["smiles2Tokens"]:
            pixelI = np.zeros((len(self.longestWord), dictLength), dtype=int)
            for i, charactr in enumerate(token):
                if charactr in index_dict:
                    pixelI[i, index_dict[charactr]] = 1
            pixels.append(np.array(pixelI))
        self.df['pixels'] = pixels
        self.tokenSize=(len(self.longestWord),len(self.dictionary))

    def plot1pixelizedSmile(self,imgIndex,smilesHeader='Smiles',printMsg=False):
        smilesHeaderCan = "Canonicalized " + smilesHeader
        if printMsg:
            t4pPC.centertxt(f"{self.df[smilesHeaderCan].iloc[imgIndex]}",size=14,weight='bold',bgc="#82c3c3",fgc="black")
            print(f"tokenized smile = {self.df['smiles2Tokens'].iloc[imgIndex]}")
            display(self.df['ROMol'].iloc[imgIndex])
        cm2i = 1/2.54
        img = self.df['pixels'].iloc[imgIndex]
        _ = plt.figure(figsize=(0.4*cm2i*len(self.dictionary),0.4*cm2i*len(self.longestWord)))
        _ = plt.imshow(img, cmap='binary', interpolation="nearest",vmin=0.0, vmax=1.0) # the data X is resampled to the pixel size of the image on the figure canvas, using the "nearest" interpolation method
        _ = plt.axis('on')
        xpos = np.r_[:len(self.dictionary)]
        ypos = np.r_[:len(self.df["smiles2Tokens"].iloc[imgIndex])]
        _ = plt.xticks(xpos, self.dictionary, rotation=270, size = 9)
        _ = plt.yticks(ypos, self.df["smiles2Tokens"].iloc[imgIndex], rotation=0,size = 9)
        _ = plt.show()
        
    def pickle(self,fileWithPathway):
        with open(fileWithPathway, 'wb') as f:
            pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)
        
    @staticmethod
    def unpickle(fileWithPathway):
        with open(fileWithPathway, 'rb') as f:
            return pickle.load(f)





def smiles_to_image(smiles):
    mol = Chem.MolFromSmiles(smiles)
    return Draw.MolToImage(mol)

def normalizeImage(img,threshold=0.3):
    imgmax = np.max(img)
    img = img/imgmax
    img[img<threshold] = 0
    img[img>=threshold] = 1.0
    return img

def normalizeImage2(img,threshold=0.2):
    """
    each line must have at most one pixel switched on, unless no pixel value on a line is beyond the threshold
    """
    imgmax = np.max(img)
    img = img/imgmax
    print("shape = ",img.shape)
    for irow in range(img.shape[1]):
        row = img[0,irow,:]
        row2 = np.zeros_like(row)
        if np.max(row) >= threshold:
            row2[np.where(row==np.max(row))] = 1
        img[0,irow,:] = row2
    return img
    
def compareReconstructionsWithOriginalTokens(dftoken, encoderModel, edModel, index, imgDataset, smilesList, indexMol, saveFig = False, IMAGES_PATH="."):
    
    from scipy import ndimage
    cm2i = 1/2.54
    plt.rc('font', size=12)
    plt.rc('axes', labelsize=10, titlesize=12)
    plt.rc('legend', fontsize=12)
    plt.rc('xtick', labelsize=10)
    plt.rc('ytick', labelsize=10)
    plt.figure(figsize=((4*0.4*cm2i*len(dftoken.dictionary),0.4*cm2i*len(dftoken.longestWord)))) # 1 col for 2D image
                                    # 1 col for actual images
                                    # 1 col for latent space representation,
                                    # 1 col for reconstructed images
    if indexMol > imgDataset.shape[0]-1:
        print(f"Dataset size = {imgDataset.shape[0]}. Cannot plot {indexMol}th image")
        print(f"Plot automatically setup to {imgDataset.shape[0]-1}")
        indexMol = imgDataset.shape[0]-1

    print(f"Index of the molecule = {index[indexMol]}")
    print((dftoken.df["Canonicalized Smiles"][index[indexMol]]))
    selectedImg = imgDataset[indexMol]
    print(selectedImg.shape)
    selectedImg4prediction = selectedImg.reshape(1,len(dftoken.longestWord),len(dftoken.dictionary))
    # print(selectedImg4prediction.shape)
    reconstrucedImg = edModel.predict(selectedImg4prediction)
    # print(reconstrucedImg)
    reconstrucedImg = normalizeImage2(reconstrucedImg)
    # print(np.max(reconstrucedImg))
    reconstrucedImg = reconstrucedImg.reshape(len(dftoken.longestWord),len(dftoken.dictionary))
    LatentSpaceRepresentation = encoderModel.predict(selectedImg4prediction)
    selectedSmiles = smilesList[indexMol]
    # display(dftoken.df["smiles2Tokens"])
    # print(smilesList)
    # for i,s in enumerate(smilesList): 
    #     print(i," = ",s)
    #     print(dftoken.df["smiles2Tokens"].loc[index[i]])
    #     plt.imshow(smiles_to_image(s))
    #     plt.show()
    plt.subplot(1, 4, 1)
    mol_img = smiles_to_image(selectedSmiles)
    plt.imshow(mol_img)
    plt.axis('off')

    plt.subplot(1, 4, 2) # the third argument represents the index of the subplot that will be used
    plt.imshow(selectedImg, cmap='binary', interpolation="nearest",vmin=0.0, vmax=1.0) # the data X is resampled to the pixel size of the image on the figure canvas, using the "nearest" interpolation method
    plt.axis('on')
    xpos = np.r_[:len(dftoken.dictionary)]
    ypos = np.r_[:len(dftoken.df["smiles2Tokens"].loc[index[indexMol]])]
    plt.xticks(xpos, dftoken.dictionary, rotation=270, size = 9)
    plt.yticks(ypos, dftoken.df["smiles2Tokens"].loc[index[indexMol]], rotation=0,size = 9)

    plt.subplot(1, 4, 3) # the third argument represents the index of the subplot that will be used
    rotatedLSR = ndimage.rotate(LatentSpaceRepresentation, 90)
    plt.imshow(rotatedLSR, cmap='binary', interpolation="nearest")
    plt.yticks([])
    plt.xticks([])
    plt.axis('on')
    
    plt.subplot(1, 4, 4) # the third argument represents the index of the subplot that will be used
    plt.imshow(reconstrucedImg, cmap='binary', interpolation="nearest",vmin=0.0, vmax=1.0) # the data X is resampled to the pixel size of the image on the figure canvas, using the "nearest" interpolation method
    plt.axis('on')
    xpos = np.r_[:len(dftoken.dictionary)]
    ypos = np.r_[:len(dftoken.df["smiles2Tokens"].loc[index[indexMol]])]
    plt.xticks(xpos, dftoken.dictionary, rotation=270, size = 9)
    plt.yticks(ypos, dftoken.df["smiles2Tokens"].loc[index[indexMol]], rotation=0,size = 9)
        
    plt.subplots_adjust(wspace=0.2, hspace=0.5)
    if (saveFig):
        save_fig(IMAGES_PATH,"tokenization_autoencoder")
    plt.show()






SMILESxtr = tokenization('./Molecules-data/smiles_prop.csv',reduceNumberOfMolTo=3,debug=True)
SMILESxtr.plot1pixelizedSmile(1,printMsg=True)
PandasTools.SaveXlsxFromFrame(SMILESxtr.df, './Molecules-data/smiles_prop_3xtr_WithTokenMol.xlsx', molCol='ROMol')





def makeXY(data_train,data_valid):
    t4pPC.centerTitle("Making the training and validation sets")
    x_train = np.array(list(data_train['pixels'].values))
    y_train = data_train['log(P)'].values
    smiles_train = data_train['Smiles'].values
    index_train = data_train.index.values
    x_valid  = np.array(list(data_valid['pixels'].values))
    y_valid  = data_valid['log(P)'].values
    smiles_valid = data_valid['Smiles'].values
    index_valid = data_valid.index.values
    
    t4pPC.centertxt("Training set",size=14,weight='bold',bgc="#82c3c3",fgc="black")
    print("x_train: ",x_train.shape)
    print("y_train: ",y_train.shape)
    print("smiles_train: ",smiles_train.shape)
    t4pPC.centertxt("Validation set",size=14,weight='bold',bgc="#82c3c3",fgc="black")
    print("x_valid: ",x_valid.shape)
    print("y_valid: ",y_valid.shape)
    print("smiles_valid: ",smiles_valid.shape)
    return index_train,x_train,y_train,smiles_train,index_valid,x_valid,y_valid,smiles_valid






nameOfDataFrame = '10000SMILES_df'
vID.chrono_start()
SMILESxtr = tokenization('./Molecules-data/smiles_prop.csv',reduceNumberOfMolTo=10000,debug=False)
vID.chrono_show()
saveDataFrame(SMILESxtr.df, nameOfDataFrame, MODELS_PATH=MODELS_PATH, df_extension="xlsx")


vID.chrono_start()
data_train = SMILESxtr.df.sample(frac=0.8, axis=0)
data_valid  = SMILESxtr.df.drop(data_train.index)
saveDataFrame(data_train, nameOfDataFrame+'_train', MODELS_PATH=MODELS_PATH, df_extension="pickle")
saveDataFrame(data_valid, nameOfDataFrame+'_valid', MODELS_PATH=MODELS_PATH, df_extension="pickle")

index_train,x_train,y_train,smiles_train,index_valid,x_valid,y_valid,smiles_valid = makeXY(data_train,data_valid)
vID.chrono_show()

filename = MODELS_PATH / f"{nameOfDataFrame}_tokenized.pickle"
t4pPC.centerTitle(f"Saving the token instance in {filename}")
print(filename)
SMILESxtr.pickle(filename)





tf.keras.backend.clear_session()

tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU

stacked_SMILESencoder = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[SMILESxtr.tokenSize[0],SMILESxtr.tokenSize[1]],name='inputLayer'),
    tf.keras.layers.Dense(100, activation="relu"),
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(SMILESxtr.tokenSize[1], activation="relu"), #len of the dictionary. Why not? :-)
])

stacked_SMILESdecoder = tf.keras.Sequential([
    tf.keras.layers.Dense(100, activation="relu"),
    tf.keras.layers.Dense(SMILESxtr.tokenSize[0]*SMILESxtr.tokenSize[1]),
    tf.keras.layers.Reshape([SMILESxtr.tokenSize[0], SMILESxtr.tokenSize[1]])
    
])

stacked_edSMILESmodel = tf.keras.Sequential([stacked_SMILESencoder, stacked_SMILESdecoder])

#t4pPC.centerTitle("Summary of the model")
stacked_SMILESencoder.build()
stacked_SMILESdecoder.build()
stacked_edSMILESmodel.build()
stacked_edSMILESmodel.summary()

#t4pPC.centerTitle("Plot models")
plotModel(stacked_SMILESencoder, "StackedEncoder", MODELS_PATH=MODELS_PATH)
print()
plotModel(stacked_SMILESdecoder, "StackedDecoder", MODELS_PATH=MODELS_PATH)





from tensorflow.keras.callbacks import EarlyStopping

metrics = ['mae']
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)

vID.chrono_start()
stacked_edSMILESmodel.compile(loss="mse",
                              optimizer="nadam",
                              metrics = metrics
                             )
history_edSMILES = stacked_edSMILESmodel.fit(x_train, x_train,
                                             epochs=50,
                                             validation_data=(x_valid, x_valid),
                                             batch_size=100,
                                             callbacks=[es]
                                            )
vID.chrono_show()


plotEpochs(history_edSMILES,metrics)





from varname import nameof
file = MODELS_PATH / f"{nameof(stacked_edSMILESmodel)}.keras"
stacked_edSMILESmodel.save(file)

file = MODELS_PATH / f"{nameof(stacked_SMILESencoder)}.keras"
stacked_SMILESencoder.save(file)

file = MODELS_PATH / f"{nameof(stacked_SMILESdecoder)}.keras"
stacked_SMILESdecoder.save(file)





t4pPC.centerTitle("Reading the trained models")
from varname import nameof
file = MODELS_PATH / f"{nameof(stacked_edSMILESmodel)}.keras"
stacked_edSMILESmodel = keras.models.load_model(file)

file = MODELS_PATH / f"{nameof(stacked_SMILESencoder)}.keras"
stacked_SMILESencoder = keras.models.load_model(file)

file = MODELS_PATH / f"{nameof(stacked_SMILESdecoder)}.keras"
stacked_SMILESdecoder = keras.models.load_model(file)

file_train = nameOfDataFrame+'_train'
file_valid = nameOfDataFrame+'_valid'
t4pPC.centerTitle(f"Reading training and validation dataframes, in {file_train} and {file_valid}")
data_train = loadDataFrame(file_train, MODELS_PATH=MODELS_PATH, df_extension="pickle")
data_valid = loadDataFrame(file_valid, MODELS_PATH=MODELS_PATH, df_extension="pickle")

index_train, x_train, y_train, smiles_train, index_valid, x_valid, y_valid, smiles_valid = makeXY(data_train,data_valid)


filename = MODELS_PATH / f"{nameOfDataFrame}_tokenized.pickle"
t4pPC.centerTitle(f"Reading the tokenized dataframe, in {filename}")
SMILESxtr = tokenization.unpickle(filename)
display(SMILESxtr.df)
print("Dictionary = ",SMILESxtr.dictionary)
print()
print("Longest word = ",SMILESxtr.longestWord)





# t = np.array([[0,0,0.2,1],
#              [3.,0.1,0.2,1],
#              [0.1,0.6,0.2,0.1],
#              [0.2,0.25,0.02,0.1]])
# print(t)

# for irow in range(t.shape[0]):
#     row = t[irow,:]
#     row2 = np.zeros_like(row)
#     print(row, max(row),np.argmax(row))
#     if np.max(row) >= 0.3:
#         row2[np.where(row==np.max(row))] = 1
#     print(row2)
#     t[irow,:] = row2

# print(t)


compareReconstructionsWithOriginalTokens(SMILESxtr, stacked_SMILESencoder, stacked_edSMILESmodel, index_train, x_train, smiles_train, indexMol = 10)
compareReconstructionsWithOriginalTokens(SMILESxtr, stacked_SMILESencoder, stacked_edSMILESmodel, index_valid, x_valid, smiles_valid, indexMol = 10)





vID.chrono_start()
from sklearn.manifold import TSNE
x_validEncoded = stacked_SMILESencoder.predict(x_valid)
tsne = TSNE(init="pca", learning_rate="auto", random_state=42)
x_validEncoded2D = tsne.fit_transform(x_validEncoded)
vID.chrono_show()



import plotly.express as px
import molplotly

Z = x_validEncoded2D.copy()
Z = (Z - Z.min()) / (Z.max() - Z.min())  # normalize to the 0-1 range
cm2i = 1/2.54

temp_df = pd.DataFrame({
    'X': Z[:, 0],
    'Y': Z[:, 1],
    'Log(P)': y_valid,
    'smiles': smiles_valid,
})

# Create a scatter plot with plotly
fig = px.scatter(temp_df, x='X', y='Y', color='Log(P)', hover_data=['smiles'])

fig.update_layout(
    width=1000,  # Adjust width and height if necessary
    height=1000,
    xaxis=dict(scaleanchor='y', scaleratio=2),
    yaxis=dict(scaleanchor='x', scaleratio=2)
)
# Add molecules with molplotly
app = molplotly.add_molecules(
    fig=fig,
    df=temp_df,
    smiles_col='smiles',
    title_col='Log(P)'
)

# Run the application
app.run(mode='inline', port=8768, height=1600)








# read the data
nameOfDataFrame = '10000SMILES_df'
vID.chrono_start()
SMILESxtr = tokenization('./Molecules-data/smiles_prop.csv',reduceNumberOfMolTo=10000,debug=False)
vID.chrono_show()
saveDataFrame(SMILESxtr.df, nameOfDataFrame, MODELS_PATH=MODELS_PATH, df_extension="xlsx")

# Make training and validation sets
vID.chrono_start()
data_train = SMILESxtr.df.sample(frac=0.8, axis=0)
data_valid  = SMILESxtr.df.drop(data_train.index)
saveDataFrame(data_train, nameOfDataFrame+'_train', MODELS_PATH=MODELS_PATH, df_extension="pickle")
saveDataFrame(data_valid, nameOfDataFrame+'_valid', MODELS_PATH=MODELS_PATH, df_extension="pickle")

index_train,x_train,y_train,smiles_train,index_valid,x_valid,y_valid,smiles_valid = makeXY(data_train,data_valid)
vID.chrono_show()

filename = MODELS_PATH / f"{nameOfDataFrame}_tokenized.pickle"
t4pPC.centerTitle(f"Saving the token instance in {filename}")
print(filename)
SMILESxtr.pickle(filename)






# Creation  and compilation of the model
tf.keras.backend.clear_session()

tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU

stacked_SMILESencoder = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[SMILESxtr.tokenSize[0],SMILESxtr.tokenSize[1]],name='inputLayer'),
    tf.keras.layers.Dense(100, activation="relu",name='hLayer1'),
    tf.keras.layers.Dense(50, activation="relu", name='hLayer2'),
    tf.keras.layers.Dense(50, activation="relu", name='hLayer3'),
    tf.keras.layers.Dense(100, activation="sigmoid"), #len of the dictionary
    tf.keras.layers.ActivityRegularization(l1=1e-2)
])

stacked_SMILESdecoder = tf.keras.Sequential([
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(SMILESxtr.tokenSize[0]*SMILESxtr.tokenSize[1]),
    tf.keras.layers.Reshape([SMILESxtr.tokenSize[0], SMILESxtr.tokenSize[1]])
])

stacked_edSMILESmodel = tf.keras.Sequential([stacked_SMILESencoder, stacked_SMILESdecoder])

#t4pPC.centerTitle("Summary of the model")
stacked_SMILESencoder.build()
stacked_SMILESdecoder.build()
stacked_edSMILESmodel.build()
stacked_edSMILESmodel.summary()

#t4pPC.centerTitle("Plot models")
plotModel(stacked_SMILESencoder, "StackedEncoder", MODELS_PATH=MODELS_PATH)
print()
plotModel(stacked_SMILESdecoder, "StackedDecoder", MODELS_PATH=MODELS_PATH)

# Training
from tensorflow.keras.callbacks import EarlyStopping

metrics = ['mae']
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)

vID.chrono_start()
stacked_edSMILESmodel.compile(loss="mse",
                              optimizer="nadam",
                              metrics = metrics
                             )
history_edSMILES = stacked_edSMILESmodel.fit(x_train, x_train,
                                             epochs=50,
                                             validation_data=(x_valid, x_valid),
                                             batch_size=100,
                                             callbacks=[es]
                                            )
vID.chrono_show()





compareReconstructionsWithOriginalTokens(SMILESxtr, stacked_SMILESencoder, stacked_edSMILESmodel, index_train, x_train, smiles_train, indexMol = 1260)
compareReconstructionsWithOriginalTokens(SMILESxtr, stacked_SMILESencoder, stacked_edSMILESmodel, index_valid, x_valid, smiles_valid, indexMol = 12)











import numpy as np
from matplotlib import pyplot as plt
from scipy import signal
from scipy.signal import find_peaks

pathway2rdfExp = "./ML-data/RDFs/expRDF/"

pathway = "./ML-data/RDFs/simulRDF/"


def rescale1D(x,f,peakref,xmax,dxTarget):
    import numpy as np
    dx = x[1]-x[0]
    f = f/f[peakref]
    x = x/x[peakref]
    xmaxTarget = xmax / x[peakref]
    xnew = np.arange(0,xmaxTarget,dxTarget)
    ynew = np.interp(xnew, x, f)
    return xnew,ynew

def makeXY(data_train,data_valid):
    t4pPC.centerTitle("Making the training and validation sets")
    x_train = np.array(list(data_train.drop(['Shape'],axis=1).values))
    y_train = data_train['Shape'].values
    index_train = data_train.index.values
    x_valid  = np.array(list(data_valid.drop(['Shape'],axis=1).values))
    y_valid  = data_valid['Shape'].values
    index_valid = data_valid.index.values
    
    t4pPC.centertxt("Training set",size=14,weight='bold',bgc="#82c3c3",fgc="black")
    print("x_train: ",x_train.shape)
    print("y_train: ",y_train.shape)
    t4pPC.centertxt("Validation set",size=14,weight='bold',bgc="#82c3c3",fgc="black")
    print("x_valid: ",x_valid.shape)
    print("y_valid: ",y_valid.shape)
    return index_train,x_train,y_train,index_valid,x_valid,y_valid

def ShowRDFLatentSpaceRepresentation(encoder, decoder, ae, RDF_Dataset, shapes, rStep, RDF_index):
    from scipy.signal import find_peaks
    import numpy as np
    nSteps = RDF_Dataset.shape[1]
    img = RDF_Dataset[RDF_index].reshape(1,nSteps)
    rValues = np.arange(0,nSteps*rStep,rStep)
    print(f"Shape of the NP = {shapes[RDF_index]}")
    print(f"Shape of the RDF input profile = {img.shape}")
    LatentSpaceRepresentation = encoder.predict(img)
    Reconstructed_img = decoder.predict(LatentSpaceRepresentation)
    print(Reconstructed_img.shape)
    Reconstructed_img = Reconstructed_img.reshape(nSteps)
    plt.rc('font', size=14)
    plt.rc('axes', labelsize=14, titlesize=12)
    plt.rc('legend', fontsize=14)
    plt.rc('xtick', labelsize=10)
    plt.rc('ytick', labelsize=10)
    plt.subplots(1, 2, gridspec_kw={"width_ratios": [2, 4]}, figsize=(18, 5))
    plt.subplot(1, 2, 1)
    plt.imshow(LatentSpaceRepresentation, cmap='binary', interpolation="nearest")
    plt.yticks([])
    plt.subplot(1, 2, 2) 
    plt.plot(rValues,RDF_Dataset[RDF_index],label='original RDF')
    plt.plot(rValues,Reconstructed_img,label='decoded RDF',ls='--')
    plt.legend()
    plt.show()
    return LatentSpaceRepresentation, Reconstructed_img





import pandas as pd
import numpy as np

import os

csv1DF = pathway+"RDF_profiles.csv"
csv1DL = pathway+"RDF_labels.csv"

if os.path.exists(csv1DF) and os.path.exists(csv1DL):
    dfF=pd.read_csv(csv1DF, sep="\t", index_col = 0) 
    dfY=pd.read_csv(csv1DL, sep="\t", index_col = 0) 
    print(f"dfF. Structure (shape) :{dfF.shape}")
    print(f"dfY. Structure (shape) :{dfY.shape}")
    display(dfF)
    display(dfY)
    rStep = dfF['r'].iloc[1]- dfF['r'].iloc[0]
else:
    print(f'{csv1DF} and/or {csv1DL} were not found')
    print('Create first these files, by running the codes of the appendix of the present notebook') 

print(rStep)





RDFcols = list(dfF.columns)
RDFcols.remove('r')
dfFS = dfF.copy()
dfFS = dfFS.drop('r',axis='columns')
for nameRDF in RDFcols:
    peaks, _ = find_peaks(dfFS[nameRDF],height=1)
    peakRef = peaks[0]
    # print(peaks[0], rvalues[peaks[0]],dfFS[nameRDF].iloc[peaks[0]])
    dfFS[nameRDF] = dfFS[nameRDF]/dfFS[nameRDF].iloc[peaks[0]]

deltaR = dfF['r'].iloc[1] - dfF['r'].iloc[0]
Rmax = 10
nrows = int(Rmax / deltaR)
print(f"{bg.LIGHTBLUEB}Keeping {nrows} rows, such that the maximum radius (Rmax) is limited to {Rmax} Å (δr = {deltaR} Å)")
dfFS = dfFS.iloc[np.linspace(0,nrows,nrows)]
display(dfFS)
display(dfFS.describe().style.format("{0:.2f}").set_caption("Training set after rescaling wrt intensity"))





dfFS = dfFS.T
nlines = dfFS.shape[0]
ncols = dfFS.shape[1]
dfFS["Shape"]=dfY["F"]
uv = dfFS["Shape"].unique()
print(f"{len(uv):2} unique categories: {uv}") 
display(dfFS)





vID.chrono_start()
nameOfDataFrame = 'RDF_df'
data_train = dfFS.sample(frac=0.8, axis=0)
data_valid  = dfFS.drop(data_train.index)
saveDataFrame(data_train, nameOfDataFrame+'_train', MODELS_PATH=MODELS_PATH, df_extension="pickle")
saveDataFrame(data_valid, nameOfDataFrame+'_valid', MODELS_PATH=MODELS_PATH, df_extension="pickle")

index_train,x_train,y_train,index_valid,x_valid,y_valid = makeXY(data_train,data_valid)
vID.chrono_show()

nRDFsteps = dfFS.shape[1]-1

# filename = MODELS_PATH / f"{nameOfDataFrame}_tokenized.pickle"
# t4pPC.centerTitle(f"Saving the token instance in {filename}")
# print(filename)
# dfFS.pickle(filename)





tf.keras.backend.clear_session()

tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU

sizeLS = len(uv)*2
stacked_RDFencoder = tf.keras.Sequential([
    tf.keras.layers.Input((nRDFsteps,),name='inputLayer'),
    tf.keras.layers.Dense(100, activation="relu"),
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(sizeLS, activation="relu"), #len of the dictionary. Why not? :-)
])

stacked_RDFdecoder = tf.keras.Sequential([
    tf.keras.layers.Dense(100, activation="relu"),
    tf.keras.layers.Dense(nRDFsteps),
    
])

stacked_edRDFmodel = tf.keras.Sequential([stacked_RDFencoder, stacked_RDFdecoder])

#t4pPC.centerTitle("Summary of the model")
stacked_RDFencoder.build()
stacked_RDFdecoder.build()
stacked_edRDFmodel.build()
stacked_edRDFmodel.summary()

#t4pPC.centerTitle("Plot models")
plotModel(stacked_RDFencoder, "StackedEncoder", MODELS_PATH=MODELS_PATH)
print()
plotModel(stacked_RDFdecoder, "StackedDecoder", MODELS_PATH=MODELS_PATH)





from tensorflow.keras.callbacks import EarlyStopping

metrics = ['mae']
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)

vID.chrono_start()
stacked_edRDFmodel.compile(loss="mse",
                              optimizer="nadam",
                              metrics = metrics
                             )
history_edRDF = stacked_edRDFmodel.fit(x_train, x_train,
                                             epochs=300,
                                             validation_data=(x_valid, x_valid),
                                             batch_size=100,
                                             callbacks=[es]
                                            )
vID.chrono_show()


plotEpochs(history_edRDF,metrics)





lsr, rimg = ShowRDFLatentSpaceRepresentation(stacked_RDFencoder,stacked_RDFdecoder, stacked_edRDFmodel, x_valid, y_valid, rStep, 5)





def MAE(rdf,stacked_ed):
    from sklearn.metrics import mean_absolute_error as MAE
    rdf = np.array(rdf)
    nSteps = len(rdf)
    rdf = rdf.reshape(1,nSteps)
    return MAE(rdf,stacked_ed(rdf))

for s in uv:
    dftmp = pd.DataFrame(data_valid[data_valid['Shape']==s].drop(columns=['Shape']).apply(MAE,stacked_ed=stacked_edRDFmodel,axis='columns'),columns=['MAE'])
    # display(dftmp)
    print(f"{s:10}. {dftmp['MAE'].mean():.3f}")





from sklearn.manifold import TSNE
x_valid_compressed = stacked_RDFencoder.predict(x_valid)
tsne = TSNE(init="pca", learning_rate="auto", random_state=42)
x_valid_2D = tsne.fit_transform(x_valid_compressed)
print(len(x_valid_2D))


# print(y_valid)
# print(uv)
y_valid_asNumber = []
for s in y_valid:
    y_valid_asNumber.append(list(uv).index(s))
y_valid_asNumber = np.array(y_valid_asNumber)
# print(y_valid_asNumber)


import matplotlib as mpl
from matplotlib.lines import Line2D
import plotly.express as px
cmap = plt.cm.tab10

t4pPC.centerTitle('TSNE scheme')
print(len(y_valid),len(x_valid_2D))
Z = x_valid_2D.copy()
Z = (Z - Z.min()) / (Z.max() - Z.min())  # normalize to the 0-1 range
cm = 1/2.54
df = pd.DataFrame()
df['x'] = Z[:, 0]
df['y'] = Z[:, 1]
df['color'] = y_valid_asNumber
df['global shape'] = y_valid
df['shape'] = data_valid.index
display(df)
# fig, ax = plt.subplots(figsize=(30*cm, 20*cm))
fig = px.scatter(df,x='x', y='y',color='global shape',hover_data='shape',
                 width=800, height=800,
                 template="simple_white",
                )
_ = fig.update_traces(marker_size=14)
fig.show()





import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf

from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from tensorflow.keras import layers, losses
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Model


(x_train, _), (x_test, _) = fashion_mnist.load_data()

x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.

print (x_train.shape)
print (x_test.shape)


class Autoencoder(Model):
  def __init__(self, latent_dim, shape):
    super(Autoencoder, self).__init__()
    self.latent_dim = latent_dim
    self.shape = shape
    self.encoder = tf.keras.Sequential([
      layers.Flatten(),
      layers.Dense(latent_dim, activation='relu'),
    ])
    self.decoder = tf.keras.Sequential([
      layers.Dense(tf.math.reduce_prod(shape).numpy(), activation='sigmoid'),
      layers.Reshape(shape)
    ])

  def __call__(self, x):
    encoded = self.encoder(x)
    decoded = self.decoder(encoded)
    return decoded


shape = x_test.shape[1:]
latent_dim = 64
autoencoder = Autoencoder(latent_dim, shape)


print(shape)
print(tf.math.reduce_prod(shape).numpy())


autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())
autoencoder.fit(x_train, x_train,
                epochs=10,
                shuffle=True,
                validation_data=(x_test, x_test))





vID.end(cwd0)



