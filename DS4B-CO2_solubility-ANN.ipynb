{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10024ca9-7e94-4f5c-a982-b4537b919d76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "cwd0 = './config/'\n",
    "sys.path.append(cwd0)\n",
    "\n",
    "import visualID_Eng as vID\n",
    "from visualID_Eng import color,fg,hl,bg\n",
    "vID.init(cwd0)\n",
    "\n",
    "import tools4pyPhysChem as t4pPC\n",
    "\n",
    "#cancel the \"last operation show-up\" specific of Jupyter notebooks\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e4b9b",
   "metadata": {},
   "source": [
    "\n",
    "# Prediction by an artificial neural network of the solubility of CO<sub>2</sub> in ionic liquids\n",
    "\n",
    "<div class=\"intro\">\n",
    "    \n",
    "<b>Reference</b>: \n",
    "Z. Song, H. Shi, X. Zhang & T. Zhou (**2020**), Prediction of CO<sub>2</sub> solubility in ionic liquids using machine learning methods, [<i>Chem. Eng. Sci.</i> <b>223</b>: 115752](https://www.doi.org/10.1016/j.ces.2020.115752) \n",
    "<br>\n",
    "<p style=\"text-align: center\"><img width=\"650px\" src=\"./DS4B-CO2-images/AbstractANNCO2-SongEtal.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_AbstractSong\"></p>\n",
    "<br>\n",
    "The main results are graphically reported below.\n",
    "<br>\n",
    "<p style=\"text-align: center\"><img width=\"900px\" src=\"./DS4B-CO2-images/ANNCO2-SongEtal-Results.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_ResultsSong\"></p>\n",
    "<br>\n",
    "\n",
    "**The goal of this tutorial is to reproduce the ANN part of this article, which is an example of application of a DFF**\n",
    "\n",
    "<img src=\"./DS4B-Slides/pngs/ZooNN/DFF-C.png\" alt=\"DFF\" width=\"180\" style=\"display: block; margin: 0 auto; align:center\" id=\"DFF\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af39c092-f4b7-460b-b698-32e0f2c55461",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os,sys\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89a89d-fb12-4ba1-8382-1609a0c4f848",
   "metadata": {},
   "source": [
    "<a id=\"data-read\"></a>\n",
    "## Basic data analysis and visualization\n",
    "\n",
    "<div class=\"intro\">\n",
    "\n",
    "The [pandas library](https://pandas.pydata.org/docs/index.html) provides high-performance, easy-to-use data structures and data analysis tools for the Python programming language. It is recommended to first have a look at the [10mn short introduction to pandas](https://pandas.pydata.org/docs/user_guide/10min.html).\n",
    "</div>\n",
    "\n",
    "### Read the database\n",
    "\n",
    "<div class=\"intro\">\n",
    "\n",
    "It is made of 57 columns:\n",
    "- the name of the IL, under its abbreviated form. For example [BMIM][PF6] is for [1-Butyl-3-methylimidazolium hexafluorophosphate](https://en.wikipedia.org/wiki/1-Butyl-3-methylimidazolium_hexafluorophosphate)\n",
    "- the abbreviated name of the cation and of the anion in two separated columns\n",
    "- the CO<sub>2</sub> solubility (x_CO2)\n",
    "- the temperature and pressure under which the solubility was measured \n",
    "- the 51 remaining columns are molecular descriptors. The authors chose:\n",
    "    - group contribution (GC) descriptors for the cations, *i.e.* the occurrences of functional groups in the molecule\n",
    "    - a one hot encoding scheme for anions, e.g. [1,0,...,0] (with 27 zeros) for [BF4], which is the first anion of this database, that contains ILs made of 28 different anions\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2251b-1f22-42f5-b930-e6032fe55170",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCO2f='DS4B-CO2-data'+'/'+'dataCO2.csv'\n",
    "dataCO2=pd.read_csv(dataCO2f,sep=\";\",header=0)\n",
    "display(dataCO2)\n",
    "# describe() generates descriptive statistics\n",
    "display(dataCO2.describe().style.format(\"{0:.2f}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aef9fe-02c4-4b9a-b46d-b357835d97f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t4pPC.centerTitle(\"Unique ILs\")\n",
    "nIL = dataCO2[\"IL\"].nunique()\n",
    "print(f\"There are {nIL} different ionic liquids in this database\")\n",
    "print(dataCO2[\"IL\"].unique())\n",
    "\n",
    "t4pPC.centerTitle(\"Unique cations\")\n",
    "nCat = dataCO2[\"cation\"].nunique()\n",
    "print(f\"There are {nCat} different cations in this database\")\n",
    "print(dataCO2[\"cation\"].unique())\n",
    "\n",
    "t4pPC.centerTitle(\"Unique anions\")\n",
    "nAn = dataCO2[\"anion\"].nunique()\n",
    "print(f\"There are {nAn} different anions in this database\")\n",
    "print(dataCO2[\"anion\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0742cf-0eaf-4638-ac79-6a508c518d39",
   "metadata": {},
   "source": [
    "### How do the cations an anions look like?\n",
    "\n",
    "<div class=\"intro\">\n",
    "\n",
    "Their SMILES coding is available in a separated csv file. `rdkit` will now be used to see the 2D structure of these species. The SMILES must first be converted into rdkit `mol` objects. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b469a7d-0559-49e3-8c21-365a20d9aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "print(f\"{color.BOLD}{color.BLUE}Version of rdkit : {rdkit.__version__}{color.OFF}\")\n",
    "fSMILESILs='DS4B-CO2-data'+'/'+'dataCO2_SMILES_ILsList.csv'\n",
    "SMILESILs=pd.read_csv(fSMILESILs,sep=\"\\t\",header=0)\n",
    "display(SMILESILs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35b4a72-1a5a-40e2-9ef0-8bf4ab359c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "ILcation=SMILESILs[SMILESILs['Cation or Anion'] == 'Cation']\n",
    "ILanion=SMILESILs[SMILESILs['Cation or Anion'] == 'Anion']\n",
    "PandasTools.AddMoleculeColumnToFrame(ILcation, smilesCol='SMILES')\n",
    "PandasTools.AddMoleculeColumnToFrame(ILanion, smilesCol='SMILES')\n",
    "pd.options.display.max_rows = 5\n",
    "display(ILcation)\n",
    "display(ILanion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe7f205-b940-44df-801c-8fd589f71a30",
   "metadata": {},
   "source": [
    "#### Display cations on a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c81547-d6bd-444f-b3b2-936c733cc2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPythonConsole.drawOptions.bondLineWidth=2\n",
    "#FrameToGridImage: Draw grid image of mols in pandas DataFrame\n",
    "imgCations = PandasTools.FrameToGridImage(ILcation, molsPerRow=6, legendsCol='Abbreviation', useSVG=True, subImgSize=(300, 200), maxMols=ILcation.shape[0])\n",
    "display(imgCations)\n",
    "with open('DS4B-CO2-images/GridCations.svg', 'w') as file: file.write(imgCations.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef69b545-4258-433c-9168-43d58c5342e6",
   "metadata": {},
   "source": [
    "#### Display anions on a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158f52f-1634-4581-b68a-1924284971b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPythonConsole.drawOptions.bondLineWidth=2\n",
    "#FrameToGridImage: Draw grid image of mols in pandas DataFrame\n",
    "imgAnions = PandasTools.FrameToGridImage(ILanion, molsPerRow=6, legendsCol='Abbreviation', useSVG=True, subImgSize=(300, 200), maxMols=ILcation.shape[0])\n",
    "display(imgAnions)\n",
    "with open('DS4B-CO2-images/GridAnions.svg', 'w') as file: file.write(imgAnions.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8e243f-2812-4219-b711-ea98eb6f937d",
   "metadata": {},
   "source": [
    "### Data distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5a9ce0-6f2d-4770-ba8f-275890e6e6fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vID.chrono_start()\n",
    "dataCO2.hist(figsize=(24,24))\n",
    "print(dataCO2[[\"x_CO2\", \"[CH3]\"]].groupby(\"[CH3]\").count())\n",
    "dataCO2.hist(\"[CH3]\",figsize=(10,6),facecolor='g')\n",
    "plt.show()\n",
    "vID.chrono_show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf1e66-0756-47f2-928d-d0d578613bba",
   "metadata": {},
   "source": [
    "## Prediction of the solubility of CO<sub>2</sub> by an ANN: <br>a textbook case of work that does not quite meet the standards of the field \n",
    "\n",
    "### ML scheme of the reference article\n",
    "\n",
    "<div class=\"intro\">\n",
    "    \n",
    "<p style=\"text-align: center\"><img width=\"800px\" src=\"./DS4B-CO2-images/ANN-CO2Song_etal.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_Song\"></p>\n",
    "\n",
    "**Methodological details (excerpts from the article):**\n",
    "- The MATLAB tansig and purelin transfer functions are employed in the hidden and output layers, respectively\n",
    "- The experimental data are divided into a training set (8093 points, 80% of the data) to build the model and a test set of the remaining 2023 data points to evaluate the predictive capability of the obtained model\n",
    "- <span style=\"color:red\"><b>!!!</b> Instead of performing random selection, we employ a hybrid artificial-random strategy to decompose the dataset. Specifically, the data points consisting of the least frequently used groups are equally divided into five folders <b>!!!</b></span> \n",
    "\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"rqT\" title=\"Warning!\">\n",
    "\n",
    "This \"*hybrid artificial-random strategy*\" is not a valid strategy. The recommended K-fold cross validation, applied to the 3rd model, cannot be applied here!\n",
    "\n",
    "**Let's observe now the weird behaviour of this model**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a3a8de-c71e-431e-b837-7da445154b8f",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e5190-7a77-48b3-bc55-a2da065d935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotModel(model, model_id, MODELS_PATH=\".\", fig_extension=\"png\", resolution=300,\n",
    "              show_shapes=True, show_layer_names=True, show_layer_activations=True):\n",
    "    '''\n",
    "    saves a model plot to a given subfolder, that will be created if it does not exist\n",
    "    input:\n",
    "        - model = keras model\n",
    "        - model_id = name of the model file, without an extension\n",
    "        - MODELS_PATH = pathway to the folder. Default: \".\"\n",
    "        - model_extension = model type. Default: png\n",
    "    '''\n",
    "    from tensorflow.keras.utils import plot_model\n",
    "    from matplotlib.pyplot import plot\n",
    "    path = MODELS_PATH / f\"{model_id}.{fig_extension}\"\n",
    "    plot_model(model, to_file=path, dpi=resolution, show_shapes=show_shapes, \n",
    "               show_layer_names=show_layer_names, show_layer_activations=show_layer_activations)\n",
    "    img = plt.imread(path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f804d0-8d1d-451b-8a0d-c9212dd9eae1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model 1: assesment of the reproducibility of the simulation of Song *et al.*<a id=\"model1\"></a>\n",
    "\n",
    "<div class = \"intro\">\n",
    "    \n",
    "Errors calculated between the actual and predicted x_CO2 values (noted $Y$ and $\\hat{Y}$ in equations below)  \n",
    "MAE =  Mean Absolute Error \n",
    "$$\\mathrm{MAE}=\\frac{1}{n}\\sum_{i}\\left|\\hat{Y}(x_{i})-Y(x_{i})\\right|$$\n",
    "\n",
    "MSE = Mean Squared Error  \n",
    "$$\\mathrm{MSE}=\\frac{1}{n}\\sum_{i}\\left(\\hat{Y}(x_{i})-Y(x_{i})\\right)^{2}$$\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e84e85f-58a8-452f-b174-9792ede9fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# separation of the data set into two subsets: (1) training of the ANN & (2) test of the ANN\n",
    "# library used: pandas\n",
    "data_train = dataCO2.sample(frac=0.8, axis=0)\n",
    "data_test  = dataCO2.drop(data_train.index)\n",
    "\n",
    "x_train = data_train.drop(['IL','cation','anion','x_CO2'],axis=1)\n",
    "y_train = data_train['x_CO2']\n",
    "x_test  = data_test.drop(['IL','cation','anion','x_CO2'],axis=1)\n",
    "y_test  = data_test['x_CO2']\n",
    "\n",
    "display(x_train,y_train)\n",
    "display(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f539f5e-f71d-405d-99c0-55c8bd352a47",
   "metadata": {},
   "source": [
    "#### Define the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f8c12-b7ca-44bb-8728-0fcbd53d4c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# ANN: 1 input layer (53 neurons) / 1 hidden layer (7 neurons with the reLu activation function) / 1 output layer (1 neuron with no activation function) \n",
    "# library used: keras\n",
    "\n",
    "def defANN(shape,acthL):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name='iLayer'))\n",
    "    model.add(keras.layers.Dense(7, activation=acthL, name='hLayer'))\n",
    "    model.add(keras.layers.Dense(1, name='oLayer'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model\n",
    "\n",
    "acthL='tanh'\n",
    "#acthL='relu'\n",
    "if 'ANNmodel' in globals():\n",
    "    del ANNmodel\n",
    "if 'ANNhistory' in globals():\n",
    "    del ANNhistory\n",
    "ANNmodel=defANN( (53,), acthL )\n",
    "ANNmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279ba872-e459-4065-9f2b-e908b02654c1",
   "metadata": {},
   "source": [
    "#### Plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb981664-36bd-4e20-b5d3-6892f013915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODELS_PATH = Path() / \"DS4B-CO2-SavedModels\"\n",
    "MODELS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "plotModel(ANNmodel, \"Model1_CO2\", MODELS_PATH=MODELS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d0902-821e-4f1e-8b67-bcd432ef8d3a",
   "metadata": {},
   "source": [
    "#### Train the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c209354-bbf5-4f42-8427-220724fb37e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# optimization of the ANN\n",
    "# library used: keras\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
    "vID.chrono_start()\n",
    "ANNhistory = ANNmodel.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs          = 300,\n",
    "                    batch_size      = 20,\n",
    "                    verbose         = 1,\n",
    "                    validation_data = (x_test, y_test),\n",
    "                    callbacks=[es])\n",
    "vID.chrono_show()\n",
    "\n",
    "#######################################################################################\n",
    "# accuracy of the ANN?\n",
    "# library used: numpy\n",
    "\n",
    "ytrain_hat=ANNmodel.predict(x_train)\n",
    "ytest_hat=ANNmodel.predict(x_test)\n",
    "# numpy.ravel() returns a contiguous flattened array\n",
    "diffyt = ytrain_hat - y_train.to_numpy()\n",
    "diffyp = ytest_hat - y_test.to_numpy()\n",
    "\n",
    "print()\n",
    "print(\"xCO2(predicted) - xCO2(actual)\")\n",
    "print(\n",
    "      \"Train.\",\"mean: \", np.mean(diffyt),\n",
    "      \"   std: \", np.std(diffyt),\n",
    "      \"   MAE: \", np.average(abs(diffyt)),\n",
    "      \"    R2: \", np.corrcoef(y_train.to_numpy(),ytrain_hat.flatten())[0,1]\n",
    "     )\n",
    "print(\n",
    "      \"Test.\",\"mean: \", np.mean(diffyp),\n",
    "      \"   std: \", np.std(diffyp),\n",
    "      \"   MAE: \", np.average(abs(diffyp)),\n",
    "      \"    R2: \", np.corrcoef(y_test.to_numpy(),ytest_hat.flatten())[0,1]\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1c7d4c-01a5-4f4d-b7ed-b5f67bf2b9df",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"rq\"><span style=\"color:red; font-weight:bold\">The MAE and R2 parameters seem rather good (<i>ca.</i> 0.07 and 0.92), but this prediction is actually very bad, as you will see by jumping to the <a href=\"#model-evaluation\">Model Evaluation</a> section!</span>\n",
    "<br>\n",
    "<br>  \n",
    "Unless you were lucky (an element of chance is involved in the learning process), the predicted <i>vs.</i> actual CO<sub>2</sub> solubility probably looks like this: \n",
    "<br>\n",
    "<p style=\"text-align: center\"><img width=\"550px\" src=\"./DS4B-CO2-images/predictionCO2_woStd-saved.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_predCO2woStd\"></p>\n",
    "<b>It differs a lot from the results of the reference article, reported in the introduction of this notebook.</b>\n",
    "<br><br>\n",
    "<span style=\"color:red; font-weight:bold\">Origin of this weird behaviour?</span>\n",
    "<br>\n",
    "Probably the absence of <b>standardization</b> of the data. This scattering is by the way reminiscent the variation of the solubility as a function of <i>T</i> or <i>p</i>, plotted below. Let's check that in the next script.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66724e90-748c-4c6e-86fe-ffe2933d42a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = 1/2.54\n",
    "plt.rcParams[\"figure.figsize\"] = (20*cm,12*cm) #graphic size\n",
    "fig1=dataCO2.plot(x=\"T (K)\", y=[\"x_CO2\"],linestyle='', marker='o',fontsize=14)\n",
    "fig2=dataCO2.plot(x=\"P (bar)\", y=[\"x_CO2\"],linestyle='', marker='x',fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c92a8-3c29-4734-99a6-c71655272d76",
   "metadata": {},
   "source": [
    "#### Save the Keras ANN model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e848a25-790e-4e37-8d1c-75494a710b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNmodel.save('./DS4B-CO2-SavedModels/ANNmodel-CO2-basic.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfc4fef-68ac-48d2-a760-d6d06303e386",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"model2\"></a>\n",
    "## Model 2 = standardization of *T* and *p* only\n",
    "\n",
    "<div class=\"rqT\" title=\"What is standardization?\">\n",
    "\n",
    "**Standardization** is the process of translating and scaling our features so that they are all *distributed around a mean of zero* with a *standard deviation of one*.\n",
    "    \n",
    "Because the Temperature and pressure in the CO<sub>2</sub> dataset are on totally different scales, we need to scale them, **without changing their relative distribution**\n",
    "    \n",
    "We want to standardize our data so that the *covariances* are easily comparable for each pair of features. If we don’t do it, features with larger ranges of numbers will have higher covariances, which could be an artificial feature.\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div class=\"intro\">\n",
    "\n",
    "This new model  is an improvement of model 1, but without standardizing the GC descriptors which covariances are small. <br>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "### Split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ac691-1f2f-4dba-9db1-708ba9de12ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets all state generated by Keras\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5e9aef-a30e-477e-9c61-3188ab01f331",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# separation of the data set into two subsets: (1) training of the ANN & (2) test of the ANN\n",
    "# library used: pandas\n",
    "data_train = dataCO2.sample(frac=0.8, axis=0)\n",
    "data_test  = dataCO2.drop(data_train.index)\n",
    "\n",
    "x_train = data_train.drop(['IL','cation','anion','x_CO2'],axis=1)\n",
    "y_train = data_train['x_CO2']\n",
    "x_test  = data_test.drop(['IL','cation','anion','x_CO2'],axis=1)\n",
    "y_test  = data_test['x_CO2']\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "# Train\n",
    "x_train = data_train.drop(['IL','cation','anion','x_CO2'],axis=1)\n",
    "xIL_train = x_train.drop(['T (K)','P (bar)'],axis=1)\n",
    "xTP_train = data_train[['T (K)','P (bar)']]\n",
    "y_train = data_train['x_CO2']\n",
    "x_trainTmp = x_train.copy() # for plotting purpose\n",
    "\n",
    "# Test\n",
    "x_test  = data_test.drop(['IL','cation','anion','x_CO2'],axis=1)\n",
    "xIL_test = x_test.drop(['T (K)','P (bar)'],axis=1)\n",
    "xTP_test = data_test[['T (K)','P (bar)']]\n",
    "y_test  = data_test['x_CO2']\n",
    "x_testTmp = x_test.copy() # for plotting purpose\n",
    "\n",
    "print(f\"{bg.DARKREDB}Training set before normalization:{bg.OFF}\")\n",
    "display(x_train)\n",
    "display(x_train.describe().style.format(\"{0:.2f}\").set_caption(\"Training set before normalization:\"))\n",
    "print(f\"{bg.DARKREDB}Test set before normalization:{bg.OFF}\")\n",
    "display(x_test)\n",
    "display(x_test.describe().style.format(\"{0:.2f}\").set_caption(\"Test set before normalization:\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e86de5-071e-47a0-b22d-d57bc4c7abb7",
   "metadata": {},
   "source": [
    "### Scale independently T & P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5870251-9d38-4cb2-b98e-0a44eabcd7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using scikit-learn\n",
    "from sklearn import preprocessing\n",
    "\n",
    "print(f\"{bg.LIGHTBLUEB}Training set (T,P) after normalization:{bg.OFF}\")\n",
    "scalerTP = preprocessing.StandardScaler()\n",
    "scalerTP.fit(xTP_train.values)\n",
    "xTP_train_scaled = scalerTP.transform(xTP_train.values) #returns a numpy array\n",
    "xTP_train = pd.DataFrame(xTP_train_scaled, index=xTP_train.index, columns=xTP_train.columns)\n",
    "display(xTP_train)\n",
    "display(xTP_train.describe().style.format(\"{0:.2f}\"))\n",
    "\n",
    "print(f\"{bg.LIGHTBLUEB}Test set (T,P) after normalization:{bg.OFF}\")\n",
    "xTP_test_scaled = scalerTP.transform(xTP_test.values) #returns a numpy array\n",
    "xTP_test = pd.DataFrame(xTP_test_scaled, index=xTP_test.index, columns=xTP_test.columns)\n",
    "display(xTP_test)\n",
    "display(xTP_test.describe().style.format(\"{0:.2f}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60033e9f-7eca-4265-bf8e-1deb13eabb81",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Merge the (xTP_train & xIL_train) dfs & (xTP_test & xIL_test) dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ffa8c2-cbf3-4639-8d1c-0ee3962f9747",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.concat([xTP_train,xIL_train], axis=1)\n",
    "x_test = pd.concat([xTP_test,xIL_test], axis=1)\n",
    "display(x_train.describe().style.format(\"{0:.2f}\"))\n",
    "display(x_test.describe().style.format(\"{0:.2f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324039ca-4daa-40c2-935b-8f28418dd841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# graphic representation of the previous tables\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "nbins=30\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "plt.rcParams[\"font.size\"] = (14) #font size\n",
    "plt.tick_params(axis='x',labelsize = 16)\n",
    "plt.tick_params(axis='y',labelsize = 12)\n",
    "gs = gridspec.GridSpec(2, 4, height_ratios=[1, 1], width_ratios=[1, 1, 1, 1])\n",
    "\n",
    "ax00=plt.subplot(gs[0,0], title=\"Tr. set before standardization\")\n",
    "fig00=sns.histplot(data=x_trainTmp,x=\"T (K)\", bins=nbins, kde=True, color=\"#1f77b4\", alpha=1.0)\n",
    "ax02=plt.subplot(gs[0,2], title=\"Tr. set before standardization\")\n",
    "fig02=sns.histplot(data=x_trainTmp,x=\"P (bar)\", bins=nbins, kde=True, color=\"#3e95ff\", alpha=1.0)\n",
    "ax01=plt.subplot(gs[0,1], title=\"Tr. set after standardization\")\n",
    "fig01=sns.histplot(data=x_train,x=\"T (K)\", bins=nbins, kde=True, color=\"#1f77b4\", alpha=1.0)\n",
    "ax03=plt.subplot(gs[0,3], title=\"Tr. set after standardization\")\n",
    "fig11=sns.histplot(data=x_train,x=\"P (bar)\", bins=nbins, kde=True, color=\"#3e95ff\", alpha=1.0)\n",
    "\n",
    "ax10=plt.subplot(gs[1,0], title=\"Ts. set before standardization\")\n",
    "fig10=sns.histplot(data=x_testTmp,x=\"T (K)\", bins=nbins, kde=True, color=\"#1f77b4\", alpha=1.0)\n",
    "ax12=plt.subplot(gs[1,2], title=\"Ts. set before standardization\")\n",
    "fig12=sns.histplot(data=x_testTmp,x=\"P (bar)\", bins=nbins, kde=True, color=\"#3e95ff\", alpha=1.0)\n",
    "ax11=plt.subplot(gs[1,1], title=\"Ts. set after standardization\")\n",
    "fig11=sns.histplot(data=x_test,x=\"T (K)\", bins=nbins, kde=True, color=\"#1f77b4\", alpha=1.0)\n",
    "ax13=plt.subplot(gs[1,3], title=\"Ts. set after standardization\")\n",
    "fig13=sns.histplot(data=x_test,x=\"P (bar)\", bins=nbins, kde=True, color=\"#3e95ff\", alpha=1.0)\n",
    "\n",
    "\n",
    "plt.savefig('./DS4B-CO2-images/Standardization_CO2data.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec1a1bb-acb3-42fe-acf1-0ee5408bcc60",
   "metadata": {},
   "source": [
    "### Delete the scaled & Tmp dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dfc80b-0d64-48b7-9968-f1d8e2747fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_testTmp, x_trainTmp, xTP_test_scaled, xTP_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc55bbc-962c-4b19-a405-23f158cae5c1",
   "metadata": {},
   "source": [
    "### Now define and train the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c6ed19-9bd9-45f5-b536-9bc521baf7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# ANN: 1 input layer (53 neurons) / 2 hidden layers (20 and 7 neurons) / 1 output layer (1 neuron) \n",
    "# library used: keras\n",
    "\n",
    "def defANN(shape,acthL):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name='iLayer'))\n",
    "    model.add(keras.layers.Dense(7, activation=acthL, name='hLayer'))\n",
    "    model.add(keras.layers.Dense(1, name='oLayer'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model\n",
    "\n",
    "acthL='tanh'\n",
    "#acthL='relu'\n",
    "if 'ANNmodel' in globals():\n",
    "    del ANNmodel\n",
    "if 'ANNhistory' in globals():\n",
    "    del ANNhistory\n",
    "ANNmodel=defANN( (53,), acthL )\n",
    "ANNmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550614e1-bdb2-444a-bf44-c779ab22ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# optimization of the ANN\n",
    "# library used: keras\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
    "\n",
    "vID.chrono_start()\n",
    "ANNhistory = ANNmodel.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs          = 300,\n",
    "                    batch_size      = 20,\n",
    "                    verbose         = 1,\n",
    "                    validation_data = (x_test, y_test),\n",
    "                    callbacks=[es])\n",
    "vID.chrono_show()\n",
    "\n",
    "#######################################################################################\n",
    "# accuracy of the ANN?\n",
    "# library used: numpy\n",
    "\n",
    "ytrain_hat=ANNmodel.predict(x_train)\n",
    "ytest_hat=ANNmodel.predict(x_test)\n",
    "# numpy.ravel() returns a contiguous flattened array\n",
    "diffyt = ytrain_hat.flatten() - y_train.to_numpy()\n",
    "diffyp = ytest_hat.flatten() - y_test.to_numpy()\n",
    "\n",
    "print()\n",
    "print(\"xCO2(predicted) - xCO2(actual)\")\n",
    "print(\n",
    "      \"Train.\",\"mean: \", np.mean(diffyt),\n",
    "      \"   std: \", np.std(diffyt),\n",
    "      \"   MAE: \", np.average(abs(diffyt)),\n",
    "      \"    R2: \", np.corrcoef(y_train.to_numpy(),ytrain_hat.flatten())[0,1]\n",
    "     )\n",
    "print(\n",
    "      \"Test.\",\"mean: \", np.mean(diffyp),\n",
    "      \"   std: \", np.std(diffyp),\n",
    "      \"   MAE: \", np.average(abs(diffyp)),\n",
    "      \"    R2: \", np.corrcoef(y_test.to_numpy(),ytest_hat.flatten())[0,1]\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a3b84-de2a-4918-af38-45eef3994a82",
   "metadata": {},
   "source": [
    "### Save the Keras ANN model and the scaler for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf2195a-f4e3-47c9-98ef-c5e21af74338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "ANNmodel.save('./DS4B-CO2-SavedModels/ANNmodel-CO2-StandardizedB.keras')\n",
    "\n",
    "# save the scalers\n",
    "from pickle import dump\n",
    "dump(scalerTP, open('./DS4B-CO2-SavedModels/scalerB_TP.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a55b603-ec7d-4a03-9565-fdfef9dc962f",
   "metadata": {},
   "source": [
    "<div class=\"rqE\">\n",
    "\n",
    "Once completed, jump to the <a href=\"#model-evaluation\">Model Evaluation</a> section\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d358c07-22d7-4fc6-a88c-6030d593ca8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Assessment of the stability of the ML model 2 by *K*-fold cross validation<a id=\"model4\"></a>\n",
    "\n",
    "<div class=\"rqE\">Model 2 is now adapted to assess how the results of the ML generalizes</div>\n",
    "<br>\n",
    "\n",
    "<div class=\"intro\">\n",
    "The main idea behind cross-validation is that each sample in a dataset has the opportunity of being tested. K-fold cross-validation is a special case of cross-validation where we iterate over a dataset set <i>k</i> times. The dataset is split into <i>k</i> parts at each iteration: one part is used for validation, and the remaining <i>k</i>−1  parts are merged into a training subset for model evaluation. The figure below illustrates the process of 5-fold cross-validation\n",
    "<p style=\"text-align: center\"><img width=\"520px\" src=\"DS4B-svg/KFoldCV.svg\" style=\"margin-left:auto; margin-right:auto\" id=\"img_5FoldCV\"></p>\n",
    "</div>\n",
    "\n",
    "### Apply a 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7192aadb-484d-4679-a0fc-6ce6d1fdd3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets all state generated by Keras\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3ba1f-2f90-441f-b931-28bc9e64748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# separation of the data set into two subsets: (1) training of the ANN & (2) test of the ANN\n",
    "# library used: pandas\n",
    "xdata = dataCO2.drop(['IL','cation','anion','x_CO2'],axis=1)\n",
    "xdataIL = xdata.drop(['T (K)','P (bar)'],axis=1)\n",
    "xdataTP = xdata[['T (K)','P (bar)']]\n",
    "ydata = dataCO2['x_CO2']\n",
    "\n",
    "#######################################################################################\n",
    "# ANN: 1 input layer (53 neurons) / 2 hidden layers (20 and 7 neurons) / 1 output layer (1 neuron) \n",
    "# library used: keras\n",
    "\n",
    "def defANN(shape,acthL):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape, name='iLayer'))\n",
    "    model.add(keras.layers.Dense(7, activation=acthL, name='hLayer'))\n",
    "    model.add(keras.layers.Dense(1, name='oLayer'))\n",
    "    \n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss      = 'mse',\n",
    "                  metrics   = ['mae', 'mse'] )\n",
    "    return model\n",
    "\n",
    "acthL='tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3138b7-5dc7-4804-93cb-caab9887319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# optimization of the ANN\n",
    "# library used: keras and scikit learn for the KFold cross-validator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from pickle import dump\n",
    "\n",
    "VERBOSE = 1\n",
    "EPOCHS = 300\n",
    "BATCH_SIZE = 25\n",
    "N_SPLIT = 5\n",
    "\n",
    "vID.chrono_start()\n",
    "\n",
    "# variables created to save at each iteration of the KFold process: the mean error, the standard deviation, MAE, R2\n",
    "meantT=list()\n",
    "stdtT=list()\n",
    "MAEtT=list()\n",
    "R2tT=list()\n",
    "meanvT=list()\n",
    "stdvT=list()\n",
    "MAEvT=list()\n",
    "R2vT=list()\n",
    "\n",
    "kfold = KFold(n_splits=N_SPLIT,shuffle=True,random_state=42) # k-fold is here!\n",
    "#print(list(kfold.split(x_train,y_train)))\n",
    "\n",
    "j = 0 # Variable for keeping count of split we are executing\n",
    "# The KFold cv provides train/test indices to split data in train/test sets\n",
    "for train_idx, val_idx in list(kfold.split(xdata,ydata)):\n",
    "\n",
    "    xIL_train_cv = xdataIL.iloc[train_idx]\n",
    "    xIL_valid_cv = xdataIL.iloc[val_idx]\n",
    "    xTP_train_cv = xdataTP.iloc[train_idx]\n",
    "    xTP_valid_cv = xdataTP.iloc[val_idx]\n",
    "    y_train_cv = ydata.iloc[train_idx]\n",
    "    y_valid_cv = ydata.iloc[val_idx]\n",
    "#    display(x_train_cv,x_valid_cv)\n",
    "    scalerTP = preprocessing.StandardScaler()\n",
    "    scalerTP.fit(xTP_train_cv.values)\n",
    "    xTPt_scaled = scalerTP.transform(xTP_train_cv.values) #returns a numpy array\n",
    "    xTPv_scaled = scalerTP.transform(xTP_valid_cv.values) #returns a numpy array\n",
    "    xtmp_train_cv = pd.DataFrame(xTPt_scaled, index=xTP_train_cv.index, columns=xTP_train_cv.columns) #conversion of a numpy array into a pandas dataframe\n",
    "    xtmp_valid_cv = pd.DataFrame(xTPv_scaled, index=xTP_valid_cv.index, columns=xTP_valid_cv.columns) #conversion of a numpy array into a pandas dataframe\n",
    "    x_train_cv = pd.concat([xtmp_train_cv,xIL_train_cv], axis=1)\n",
    "    x_valid_cv = pd.concat([xtmp_valid_cv,xIL_valid_cv], axis=1)\n",
    "    del xTPt_scaled, xTPv_scaled, xtmp_train_cv, xtmp_valid_cv, xIL_train_cv, xIL_valid_cv\n",
    "    del xTP_train_cv, xTP_valid_cv\n",
    "#    display(x_train_cv.describe().style.format(\"{0:.2f}\").set_caption(\"Training set after normalization (with scikit-learn):\"))\n",
    "#    display(x_valid_cv.describe().style.format(\"{0:.2f}\").set_caption(\"Validation set after normalization (with scikit-learn):\"))\n",
    "    print(f\"{color.BOLD}{color.RED}Fold {j}{color.OFF}\")\n",
    "    j+=1\n",
    "    ANNmodel=defANN( (53,), acthL )\n",
    "    ANNhistory = ANNmodel.fit(x_train_cv,\n",
    "                        y_train_cv,\n",
    "                        epochs          = EPOCHS,\n",
    "                        batch_size      = BATCH_SIZE,\n",
    "                        verbose         = VERBOSE,\n",
    "                        validation_data = (x_valid_cv, y_valid_cv),\n",
    "                        callbacks=[es])\n",
    "    ytrain_hat=ANNmodel.predict(x_train_cv)\n",
    "    yvalid_hat=ANNmodel.predict(x_valid_cv)\n",
    "    # numpy.ravel() returns a contiguous flattened array\n",
    "    diffyt = ytrain_hat.flatten() - y_train_cv.to_numpy()\n",
    "    diffyv = yvalid_hat.flatten() - y_valid_cv.to_numpy()\n",
    "\n",
    "    print()\n",
    "    print(\"xCO2(predicted) - xCO2(actual)\")\n",
    "    print(\n",
    "          \"Train.\",\"mean: \", np.mean(diffyt),\n",
    "          \"   std: \", np.std(diffyt),\n",
    "          \"   MAE: \", np.average(abs(diffyt)),\n",
    "          \"    R2: \", np.corrcoef(y_train_cv.to_numpy(),ytrain_hat.flatten())[0,1]\n",
    "         )\n",
    "    print(\n",
    "          \"Test.\",\"mean: \", np.mean(diffyv),\n",
    "          \"   std: \", np.std(diffyv),\n",
    "          \"   MAE: \", np.average(abs(diffyv)),\n",
    "          \"    R2: \", np.corrcoef(y_valid_cv.to_numpy(),yvalid_hat.flatten())[0,1]\n",
    "         )\n",
    "    meantT.append(np.mean(diffyt))\n",
    "    meanvT.append(np.mean(diffyv))\n",
    "    stdtT.append(np.std(diffyt))\n",
    "    stdvT.append(np.std(diffyv))\n",
    "    MAEtT.append(np.average(abs(diffyt)))\n",
    "    MAEvT.append(np.average(abs(diffyv)))\n",
    "    R2tT.append(np.corrcoef(y_train_cv.to_numpy(),ytrain_hat.flatten())[0,1])\n",
    "    R2vT.append(np.corrcoef(y_valid_cv.to_numpy(),yvalid_hat.flatten())[0,1])\n",
    "    \n",
    "    # Save the Keras ANN model for later use    \n",
    "    ModelName = './DS4B-CO2-SavedModels/ANNmodel-CO2-StandardizedB-' + str(j-1) + '.keras'\n",
    "    ANNmodel.save(ModelName)\n",
    "    # Scaler also saved!!!!!!!!!!!!!!\n",
    "    ScalerName = './DS4B-CO2-SavedModels/scalerB-' + str(j-1) + '.pkl'\n",
    "    dump(scalerTP, open(ScalerName, 'wb'))\n",
    "    \n",
    "vID.chrono_show()\n",
    "\n",
    "#######################################################################################\n",
    "# accuracy of the ANN?\n",
    "# library used: numpy\n",
    "print(f\"{color.BOLD}average MAE of the training set:{color.OFF}   {np.mean(MAEtT):.2f} +/- {np.std(MAEtT):.2f}\")\n",
    "print(f\"{color.BOLD}average MAE of the validation set:{color.OFF} {np.mean(MAEvT):.2f} +/- {np.std(MAEvT):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc4ebd-3fba-4c15-8d62-b13dee090127",
   "metadata": {},
   "source": [
    "### Visual assessment of the 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42c0ef6-d714-4dd0-bf09-f1f9a2c20b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "figCV, axCV = plt.subplots(1, 1)\n",
    "figCV.set_size_inches(12,5)\n",
    "axCV.errorbar(x=np.arange(len(meantT)), y=meantT, yerr=MAEtT, label='training sets', fmt='o-', capsize=10)\n",
    "axCV.errorbar(x=np.arange(len(meanvT))+0.1, y=meanvT, yerr=MAEvT, label='validation sets', fmt='o-', capsize=10)\n",
    "axCV.legend(loc='lower left', shadow=True, fontsize='14')\n",
    "axCV.set_xlabel('cross-validation k-values ',fontdict={'fontsize':16})\n",
    "axCV.set_ylabel('$\\hat{y}-y_{\\mathrm{actual}}$',fontdict={'fontsize':16})\n",
    "axCV.tick_params(labelsize = 14)\n",
    "plt.savefig('./DS4B-CO2-images/KFold-cv.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a307270f-8e31-437d-bfbc-b6e352258fa7",
   "metadata": {},
   "source": [
    "<div class=\"rqE\">\n",
    "Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. It generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split. Although the CO<sub>2</sub> data set contains more than 10k samples, it is always worth proving the stability of an ML algorithm. As a result, it is a pity that a scheme such as the one just determined above (and in principle close to the one reported below) is not available in the Song <i>et al.</i> publication, at least in the supplementary information part.\n",
    "<p style=\"text-align: center\"><img width=\"650px\" src=\"./DS4B-CO2-images/KFold-cvK.png\" style=\"margin-left:auto; margin-right:auto\" id=\"img_ResultsSong\"></p>\n",
    "<b>This error plot shows a good performance of the ML algorithm, as well as similar error bars, regardless of the training data set.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3e1c80-6eca-4dc8-83ac-c2eccd8352f7",
   "metadata": {},
   "source": [
    "<div class=\"rqE\">\n",
    "\n",
    "**Do you want to evaluate the last optimized model of the K-fold process?**\n",
    "\n",
    "evaluate the next Python cell for compatibility purpose with the **Model Evaluation** section\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e9d5d-130b-4e2f-b838-479f3d215539",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train_cv.copy()\n",
    "y_test = y_valid_cv.copy()\n",
    "ytest_hat = yvalid_hat.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb22670-4103-4161-81b5-74b167c55419",
   "metadata": {},
   "source": [
    "## Load and apply a previously saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7104c35a-166c-4023-b1dd-1457d0ca9294",
   "metadata": {},
   "source": [
    "### Load the database with unknown ILs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c397e-45dd-41c0-8ac4-b93c00150574",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCO2newf='DS4B-CO2-data'+'/'+'dataCO2-new.csv'\n",
    "dataCO2new=pd.read_csv(dataCO2newf,sep=\";\",header=0)\n",
    "display(dataCO2new)\n",
    "inputANN = dataCO2new.drop(['IL','cation','anion','xCO2 ref'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7684d3-e2eb-46c8-a162-31f459788f65",
   "metadata": {},
   "source": [
    "<div class='rqE'>\n",
    "\n",
    "The first ILs of this database have experimentally measured solubilities, whilst the major part have `NaN` unknown solubilities. They will be predicted for the temperature and pressure given in the table.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeb0a70-8c36-4f92-9759-9159b310433e",
   "metadata": {},
   "source": [
    "### Initial model of the reference paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a95253-ae39-4735-8e35-51b728d91483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# load models\n",
    "BasicModel= load_model('./DS4B-CO2-SavedModels/ANNmodel-CO2-basic.keras')\n",
    "# summarize model.\n",
    "BasicModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473ce31-9e23-404e-b375-d06a4196ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCO2new_withPredictions = dataCO2new[['IL','cation','anion','xCO2 ref','T (K)','P (bar)']].copy()\n",
    "xCO2newBasic = BasicModel.predict(inputANN)\n",
    "dataCO2new_withPredictions.insert(loc = 4, column = 'xCO2 BM', value = xCO2newBasic)\n",
    "\n",
    "pd.options.display.max_rows = len(dataCO2new_withPredictions)\n",
    "display(dataCO2new_withPredictions)\n",
    "pd.options.display.max_rows = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbfdac8-6994-4ac6-a8b0-70eced5e3522",
   "metadata": {},
   "source": [
    "### Models 3, *i.e.* model 2 validated by *k*-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28bd6f-0901-46e5-9abc-cae20c5c09ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCO2new_withPredictions = dataCO2new[['IL','cation','anion','xCO2 ref','T (K)','P (bar)']].copy()\n",
    "dataCO2new_withPredictions.insert(loc = 4, column = 'xCO2 BM', value = xCO2newBasic)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from pickle import load as loadScaler\n",
    "\n",
    "inputANNIL = inputANN.drop(['T (K)','P (bar)'],axis=1)\n",
    "inputANNTP = inputANN[['T (K)','P (bar)']]\n",
    "\n",
    "for i in range(5):\n",
    "    #del nameModel\n",
    "    scalerName = \"./DS4B-CO2-SavedModels/scalerB-\" + str(i) + \".pkl\"\n",
    "    scalerTP = loadScaler(open(scalerName, 'rb'))\n",
    "    nameModelVar = f'ModelStzB{i}'\n",
    "    fileName =  \"./DS4B-CO2-SavedModels/ANNmodel-CO2-StandardizedB-\" + str(i) + \".keras\"\n",
    "    exec(\"%s = '%s'\" %(\"nameModel\",nameModelVar))\n",
    "    columnName = 'xCO2 StzB'+str(i)\n",
    "    print(f\"{bg.DARKREDB}ANN model {nameModelVar} (file = {fileName}). xCO2 will be stored in the {columnName} column{bg.OFF}\")\n",
    "    nameModel = load_model(fileName)\n",
    "    nameModel.summary()\n",
    "    \n",
    "    inputANNTP_scaled = scalerTP.transform(inputANNTP.values) #returns a numpy array\n",
    "    dfinputANNTP_scaled = pd.DataFrame(inputANNTP_scaled, index=inputANNTP.index, columns=inputANNTP.columns)\n",
    "    inputANNB = pd.concat([dfinputANNTP_scaled,inputANNIL], axis=1)\n",
    "    \n",
    "    xCO2new = nameModel.predict(inputANNB)\n",
    "    dataCO2new_withPredictions.insert(loc = 5+i, column = columnName, value = xCO2new)\n",
    "    \n",
    "pd.options.display.max_rows = len(dataCO2new_withPredictions)\n",
    "display(dataCO2new_withPredictions)\n",
    "pd.options.display.max_rows = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48a2d85-40e2-4953-8c56-2aaa4ea74bb1",
   "metadata": {},
   "source": [
    "<a id=\"model-evaluation\"></a>\n",
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28abd585-a18e-4e4f-9986-12734924fa64",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loss, MAE and MSE as a function of epochs = during the ANN optimization with the back-propagation algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed620ba-ae39-455b-bef0-742671303917",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNscoreTrain= ANNmodel.evaluate(x_train, y_train, verbose=0)\n",
    "print(f\"{color.GREEN}x_train / loss      : {ANNscoreTrain[0]:5.4f}\")\n",
    "print(f\"x_train / mae       : {ANNscoreTrain[1]:5.4f}\")\n",
    "print(f\"x_train / mse       : {ANNscoreTrain[2]:5.4f}{color.OFF}\")\n",
    "print()\n",
    "ANNscoreEval = ANNmodel.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"{color.BLUE}x_test / loss       : {ANNscoreEval[0]:5.4f}\")\n",
    "print(f\"x_test / mae        : {ANNscoreEval[1]:5.4f}\")\n",
    "print(f\"x_test / mse        : {ANNscoreEval[2]:5.4f}{color.OFF}\")\n",
    "\n",
    "df=pd.DataFrame(data=ANNhistory.history)\n",
    "plt.rcParams[\"figure.figsize\"] = (14,6)\n",
    "figMAE=df.plot(y=[\"mae\",\"val_mae\"],linestyle='-', marker='o',fontsize=14)\n",
    "figMAE.set_xlabel('epoch',fontdict={'fontsize':16})\n",
    "figMAE.set_ylabel('mae',fontdict={'fontsize':16})\n",
    "figMAE.set_ylim([0.0,0.2])\n",
    "figMAE.legend(loc='upper right', shadow=True, fontsize='14')\n",
    "figMSE=df.plot(y=[\"mse\",\"val_mse\"],linestyle='-', marker='o',fontsize=14)\n",
    "figMSE.set_xlabel('epoch',fontdict={'fontsize':16})\n",
    "figMSE.set_ylabel('mse',fontdict={'fontsize':16})\n",
    "figMSE.set_ylim([0.0,0.1])\n",
    "figMSE.legend(loc='upper right', shadow=True, fontsize='14')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806b4298-dc60-43ff-8f86-3a0019d191f2",
   "metadata": {},
   "source": [
    "### Comparison between experimental and predicted CO<sub>2</sub> solubility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa0187-c307-4f8b-8664-0dbc3ac871b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train,ytrain_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb80d64e-5a35-42d2-81eb-5ab31fdbbace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "xD=[0,1]\n",
    "yD=[0,1]\n",
    "fig = plt.figure(figsize=(10, 14))\n",
    "plt.rcParams[\"font.size\"] = (18) #font size\n",
    "plt.tick_params(labelsize = 14)\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[10, 4], hspace=0, figure=fig)\n",
    "ax0=fig.add_subplot(gs[0,0])\n",
    "ax0.plot(y_train,ytrain_hat,'o',color=\"#1f77b4\",label='training set')\n",
    "ax0.plot(y_test,ytest_hat,'P',color=\"orange\",label='validation set')\n",
    "ax0.plot(xD,yD,'c',linestyle='--',lw=3)\n",
    "ax0.set_xlabel('experimental CO$_2$ solubility',fontsize=16)\n",
    "ax0.set_ylabel('predicted CO$_2$ solubility',fontsize=16)\n",
    "ax0.set_xlim(0,1);ax0.set_ylim(0,1) \n",
    "ax0.legend(loc='upper left', shadow=True, fontsize='16')\n",
    "plt.setp(ax0.get_xticklabels(), visible=False) #cosmetics\n",
    "\n",
    "# ax1=fig.add_subplot(gs[1,0])\n",
    "# ax1.plot(y_train,diffyt,'o',color=\"#1f77b4\",label='training set')\n",
    "# ax1.plot(y_test,diffyp,'P',color=\"orange\",label='validation set')\n",
    "# ax1.axhline(y=0, xmin=0., xmax=1.0, color='c', linestyle='--', linewidth=3)\n",
    "# ax1.set_xlabel('experimental CO$_2$ solubility',fontsize=16)\n",
    "# ax1.set_ylabel('residuals (exp./pred. error)',fontsize=16)\n",
    "# ax1.set_xlim(0.0,1.0); ax1.set_ylim(-0.3,0.3)\n",
    "# ax1.axes.get_yaxis().set_major_locator(MaxNLocator(prune='upper'))  #cosmetics\n",
    "# gs.tight_layout(fig)\n",
    "# plt.savefig('./DS4B-CO2-images/predictionCO2.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab4d6cc-4d2f-4913-98e6-43296f75d68c",
   "metadata": {},
   "source": [
    "### Residuals plotted as histograms (distribution of the error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124f7724-0059-4bd5-a7c9-a53616bffac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy.ravel() returns a contiguous flattened array\n",
    "print(color.BLUE,\n",
    "      \"Train.\",\"mean: {:.3f}\".format(np.mean(diffyt)),\n",
    "      \"   std: {:.3f}\".format(np.std(diffyt)),\n",
    "      \"   MAE: {:.3f}\".format(np.average(abs(diffyt))),\n",
    "      \"    R2: {:.3f}\".format(np.corrcoef(y_train.to_numpy(),ytrain_hat.flatten())[0,1])\n",
    "     )\n",
    "print(color.YELLOW,\n",
    "      \" Test.\",\"mean: {:.3f}\".format(np.mean(diffyp)),\n",
    "      \"   std: {:.3f}\".format(np.std(diffyp)),\n",
    "      \"   MAE: {:.3f}\".format(np.average(abs(diffyp))),\n",
    "      \"    R2: {:.3f}\".format(np.corrcoef(y_test.to_numpy(),ytest_hat.flatten())[0,1])\n",
    "     )\n",
    "\n",
    "# make a dataframe\n",
    "df1 = pd.DataFrame(\n",
    "    {\n",
    "        \"Error distribution\": diffyp,\n",
    "        \"Type\": 'Test set'    },\n",
    ")\n",
    "#display(df1)\n",
    "df2 = pd.DataFrame(\n",
    "    {\n",
    "        \"Error distribution\": diffyt,\n",
    "        \"Type\": 'Training set'    },\n",
    ")\n",
    "#display(df2)\n",
    "distrib=pd.concat([df1,df2],ignore_index=True)\n",
    "#display(distrib)\n",
    "\n",
    "xlim=0.2\n",
    "nbins=120\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "plt.rcParams[\"figure.figsize\"] = (8,13) #graphic size\n",
    "plt.rcParams[\"font.size\"] = (18) #font size\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1])\n",
    "plt.subplot(gs[0])\n",
    "fig1=sns.histplot(data=distrib,x=\"Error distribution\", hue=\"Type\", bins=nbins, kde=True, palette=['Orange','#1f77b4'], alpha=1.0)\n",
    "plt.xlabel(\"$x_{\\mathrm{CO_2}}^{\\mathrm{pred}}-x_{\\mathrm{CO_2}}^{\\mathrm{exp}}$\")\n",
    "plt.ylabel(\"number of data points in each interval\")\n",
    "plt.xlim(-xlim,xlim)\n",
    "plt.subplot(gs[1])\n",
    "plt.xlim(-xlim,xlim)\n",
    "fig2=sns.boxplot(data=distrib, x=\"Error distribution\", hue=\"Type\", notch=True, orient = 'h',palette=['Orange','#1f77b4'],legend=False)\n",
    "fig2.set(ylabel=None)\n",
    "plt.savefig('./DS4B-CO2-images/errorCO2.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c63194-d63a-4134-b57f-114f86f9fd9a",
   "metadata": {},
   "source": [
    "### Identify the most significant failures?\n",
    "\n",
    "#### Save y_hat of the training and of the test set in the dataCO2 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf20b05-a560-4702-9b3a-405150a1d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataCO2)\n",
    "df_ytrain_hat = pd.DataFrame(ytrain_hat,index=x_train.index,columns=[\"x_CO2_hat\"])\n",
    "df_ytrain_hat[\"set\"] = \"train\"\n",
    "#display(df_ytrain_hat)\n",
    "df_ytest_hat = pd.DataFrame(ytest_hat,index=x_test.index,columns=[\"x_CO2_hat_tss\"])\n",
    "df_ytest_hat[\"set_tss\"] = \"test\"\n",
    "#display(df_ytest_hat)\n",
    "dataCO2_hat = pd.concat([dataCO2,df_ytrain_hat,df_ytest_hat],axis=1)\n",
    "dataCO2_hat['x_CO2_hat'].update(dataCO2_hat.pop('x_CO2_hat_tss'))\n",
    "dataCO2_hat['set'].update(dataCO2_hat.pop('set_tss'))\n",
    "display(dataCO2_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8812d28-062f-4631-9da4-49160e0fa108",
   "metadata": {},
   "source": [
    "#### Calculate residuals and relative errors\n",
    "\n",
    "<div class=\"intro\">\n",
    "    \n",
    "They will be stored in a new `dataCO2_hat` dataframe\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c46d3a-8b59-4402-b2cf-e33d0c9e5c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCO2_hat[\"residual\"] = dataCO2_hat.loc[:,[\"x_CO2\",\"x_CO2_hat\"]].apply(lambda x: x.iloc[0]-x.iloc[1], axis=1)\n",
    "dataCO2_hat[\"relative err.\"] = dataCO2_hat.loc[:,[\"x_CO2\",\"x_CO2_hat\"]].apply(lambda x: round(100*abs(x.iloc[0]-x.iloc[1])/x.iloc[0],1), axis=1)\n",
    "move = \"x_CO2_hat\"\n",
    "column_to_move = dataCO2_hat.pop(move)\n",
    "dataCO2_hat.insert(6, move, column_to_move)\n",
    "move = \"residual\"\n",
    "column_to_move = dataCO2_hat.pop(move)\n",
    "dataCO2_hat.insert(7, move, column_to_move)\n",
    "move = \"relative err.\"\n",
    "column_to_move = dataCO2_hat.pop(move)\n",
    "dataCO2_hat.insert(8, move, column_to_move)\n",
    "move = \"set\"\n",
    "column_to_move = dataCO2_hat.pop(move)\n",
    "dataCO2_hat.insert(9, move, column_to_move)\n",
    "display(dataCO2_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c28ae6-7d0c-4675-8509-9b36ed96daac",
   "metadata": {},
   "source": [
    "#### Highlight the most significant errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5896dc-60e2-4905-973f-e6fc949608f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.20\n",
    "dataCO2_Large_Error = dataCO2_hat.query(\"abs(residual) >= @threshold\")\n",
    "display(dataCO2_Large_Error)\n",
    "dataCO2_Large_Error.to_csv(\"./DS4B-CO2-data/dataCO2_withPrediction_Wrong.csv\")\n",
    "dataCO2_Small_Error = dataCO2_hat.query(\"abs(residual) < @threshold\")\n",
    "display(dataCO2_Small_Error)\n",
    "dataCO2_Small_Error.to_csv(\"./DS4B-CO2-data/dataCO2_withPrediction_Right.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3780c4a3-caea-4bcc-8066-d515c41deea7",
   "metadata": {},
   "source": [
    "#### Plot x_CO2 & x_CO2_hat = f(T,P) for a given IL (bogus part...)\n",
    "\n",
    "##### Wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a6940-f571-4b67-a9c9-a993a51354ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go \n",
    "# import plotly.offline as off\n",
    "# from scipy.interpolate import griddata\n",
    "\n",
    "# off.init_notebook_mode()\n",
    "\n",
    "# TN = 'T (K)'\n",
    "# PN = 'P (bar)'\n",
    "# xCO2N = 'x_CO2'\n",
    "# xCO2hatN = 'x_CO2_hat'\n",
    "# #### Choose an IL that belongs to the dataCO2_Large_Error dataframe\n",
    "# ILN = '[HMIM][BF4]'\n",
    "\n",
    "# #Create meshgrid for T & P\n",
    "# Ti = np.linspace(min(dataCO2_hat.query(\"IL == @ILN\")[TN]), max(dataCO2_hat.query(\"IL == @ILN\")[TN]), num=100)\n",
    "# Pi = np.linspace(min(dataCO2_hat.query(\"IL == @ILN\")[PN]), max(dataCO2_hat.query(\"IL == @ILN\")[PN]), num=100)\n",
    "\n",
    "# T_grid, P_grid = np.meshgrid(Ti,Pi)\n",
    "\n",
    "# # Grid data\n",
    "# xCO2_grid = griddata((dataCO2_hat.query(\"IL == @ILN\")[TN],dataCO2_hat.query(\"IL == @ILN\")[PN]),dataCO2_hat.query(\"IL == @ILN\")[xCO2N],(T_grid,P_grid),method='cubic')\n",
    "# xCO2_hat_grid = griddata((dataCO2_hat.query(\"IL == @ILN\")[TN],dataCO2_hat.query(\"IL == @ILN\")[PN]),dataCO2_hat.query(\"IL == @ILN\")[xCO2hatN],(T_grid,P_grid),method='cubic')\n",
    "\n",
    "\n",
    "# # Plotly 3D Surface\n",
    "# fig_ref = go.Surface(x=T_grid,y=P_grid,z=xCO2_grid,\n",
    "#                        colorscale='viridis')\n",
    "# fig_hat = go.Surface(x=T_grid,y=P_grid,z=xCO2_hat_grid,\n",
    "#                        colorscale='greys')\n",
    "\n",
    "# layout = go.Layout(title='CO$_2$ solubility', autosize=False,\n",
    "#                   width=700, height=700,\n",
    "#                   margin=dict(l=65, r=50, b=65, t=90),\n",
    "#                   scene=dict(\n",
    "#                             xaxis_title=TN,\n",
    "#                             yaxis_title=PN,\n",
    "#                             zaxis_title=xCO2N,\n",
    "#     ),\n",
    "#                  )\n",
    "# off.iplot({\"data\": [fig_ref, fig_hat],\"layout\": layout})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7437f0-64a9-45f2-b0ec-92b354be527b",
   "metadata": {},
   "source": [
    "##### Good predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e35fc8-91b1-433e-acba-322639093463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go \n",
    "# import plotly.offline as off\n",
    "# from scipy.interpolate import griddata\n",
    "\n",
    "# off.init_notebook_mode()\n",
    "\n",
    "# TN = 'T (K)'\n",
    "# PN = 'P (bar)'\n",
    "# xCO2N = 'x_CO2'\n",
    "# xCO2hatN = 'x_CO2_hat'\n",
    "# #### Choose an IL that belongs to the dataCO2_Small_Error dataframe\n",
    "# ILN = '[BMIM][BF4]'\n",
    "\n",
    "# #Create meshgrid for T & P\n",
    "# Ti = np.linspace(min(dataCO2_hat.query(\"IL == @ILN\")[TN]), max(dataCO2_hat.query(\"IL == @ILN\")[TN]), num=100)\n",
    "# Pi = np.linspace(min(dataCO2_hat.query(\"IL == @ILN\")[PN]), max(dataCO2_hat.query(\"IL == @ILN\")[PN]), num=100)\n",
    "\n",
    "# T_grid, P_grid = np.meshgrid(Ti,Pi)\n",
    "\n",
    "# # Grid data\n",
    "# xCO2_grid = griddata((dataCO2_hat.query(\"IL == @ILN\")[TN],dataCO2_hat.query(\"IL == @ILN\")[PN]),dataCO2_hat.query(\"IL == @ILN\")[xCO2N],(T_grid,P_grid),method='cubic')\n",
    "# xCO2_hat_grid = griddata((dataCO2_hat.query(\"IL == @ILN\")[TN],dataCO2_hat.query(\"IL == @ILN\")[PN]),dataCO2_hat.query(\"IL == @ILN\")[xCO2hatN],(T_grid,P_grid),method='cubic')\n",
    "\n",
    "# # Plotly 3D Surface\n",
    "# fig_ref = go.Surface(x=T_grid,y=P_grid,z=xCO2_grid,\n",
    "#                        colorscale='viridis')\n",
    "# fig_hat = go.Surface(x=T_grid,y=P_grid,z=xCO2_hat_grid,\n",
    "#                        colorscale='greys')\n",
    "\n",
    "# layout = go.Layout(title='CO$_2$ solubility', autosize=False,\n",
    "#                   width=700, height=700,\n",
    "#                   margin=dict(l=65, r=50, b=65, t=90),\n",
    "#                   scene=dict(\n",
    "#                             xaxis_title=TN,\n",
    "#                             yaxis_title=PN,\n",
    "#                             zaxis_title=xCO2N,\n",
    "#     ),\n",
    "#                  )\n",
    "# off.iplot({\"data\": [fig_ref, fig_hat],\"layout\": layout})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb479dd2-902f-48fb-8688-25b353193be0",
   "metadata": {},
   "source": [
    "## \"Simple\" exercise\n",
    "\n",
    "<div class=\"exE\" title=\"Optimize the hyperparameters of Model 2 by hand\">\n",
    "\n",
    "Return to the <a href=\"#model2\">corresponding section</a> and change some hyperparameters. Carefully note the accuracy of each attempt, and watch out for a possible overfitting. Suggestions:\n",
    "- Try to increase the number of neurons of the hidden layer\n",
    "- Replace the `tanh` activation function with the `relu` activation function\n",
    "- Add a second hidden layer (this is **deep learning**!)\n",
    "\n",
    "Do you find a better model than the original model 2?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a15f35-ba16-4c3f-983a-d09a08f598b4",
   "metadata": {},
   "source": [
    "## Appendix: activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219b17ac-8e61-4375-8770-a21b382dc9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(f, z, eps=0.000001):\n",
    "    return (f(z + eps) - f(z - eps))/(2 * eps)\n",
    "\n",
    "z = np.linspace(-5, 5, 200)\n",
    "pw=4\n",
    "ph=4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482be7be-9749-4c8d-a305-2092bc12c56b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### relu function\n",
    "\n",
    "$$R(z)=\\begin{cases}\n",
    "\\begin{array}{cc}\n",
    "z & \\,\\,\\,\\,\\,\\,\\,z>0\\\\\n",
    "0 & \\,\\,\\,\\,\\,\\,\\,z\\leq0\n",
    "\\end{array}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3910e5-9987-4a22-a2e0-61eed17cf896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "figR, axR = plt.subplots(1, 1)\n",
    "figR.set_size_inches(pw,ph)\n",
    "axR.axhline(y=0, linewidth=2, linestyle='--', color='lightgray')\n",
    "axR.axvline(x=0, linewidth=2, linestyle='--', color='lightgray')\n",
    "axR.plot(z, relu(z),linewidth=4,label=\"ReLU\")\n",
    "axR.plot(z, derivative(relu, z), linewidth=4, alpha=0.6, label=\"dReLU/dx\")\n",
    "axR.legend(loc='upper left', shadow=True, fontsize='14')\n",
    "axR.set_xlabel('x',fontdict={'fontsize':16})\n",
    "axR.set_ylabel('f(x)',fontdict={'fontsize':16})\n",
    "axR.set_xlim(-5, 5)\n",
    "axR.set_ylim(-0.5, 7)\n",
    "axR.tick_params(labelsize = 14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f134febb-7d82-4412-8007-622fcf628a0d",
   "metadata": {},
   "source": [
    "### tanh function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7015ab-4de3-4248-a572-b96bd2d476eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "figT, axT = plt.subplots(1, 1)\n",
    "figT.set_size_inches(pw,ph)\n",
    "axT.axhline(y=0, linewidth=2, linestyle='--', color='lightgray')\n",
    "axT.axvline(x=0, linewidth=2, linestyle='--', color='lightgray')\n",
    "axT.plot(z, tanh(z),linewidth=4,label=\"tanh\")\n",
    "axT.plot(z, derivative(tanh, z), linewidth=4, alpha=0.6, label=\"dtanh/dx\")\n",
    "axT.legend(loc='upper left', shadow=True, fontsize='14')\n",
    "axT.set_xlabel('x',fontdict={'fontsize':16})\n",
    "axT.set_ylabel('f(x)',fontdict={'fontsize':16})\n",
    "axT.set_xlim(-5, 5)\n",
    "axT.set_ylim(-2, 2)\n",
    "axT.tick_params(labelsize = 14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bcf6a0-a188-4703-aae0-4d455a94d835",
   "metadata": {},
   "source": [
    "### softmax function\n",
    "$$\\sigma_{i}(\\boldsymbol{x})=\\frac{e^{x_{i}}}{\\sum_{j=1}^{K}e^{x_{j}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d6627-a1bb-4527-9532-4a81c4253d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x)) #this implentation has a better numerical stability\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "z2 = [-2., -1., -0.5, 0., 0.5, 1., 2.]\n",
    "figS, axS = plt.subplots(1, 1)\n",
    "figS.set_size_inches(pw,ph)\n",
    "axS.axhline(y=0, linewidth=2, linestyle='--', color='lightgray')\n",
    "axS.axvline(x=0, linewidth=2, linestyle='--', color='lightgray')\n",
    "axS.plot(z2, softmax(z2),linewidth=4,marker='o',label=\"softmax\")\n",
    "axS.legend(loc='upper left', shadow=True, fontsize='14')\n",
    "axS.set_xlabel('x',fontdict={'fontsize':16})\n",
    "axS.set_ylabel('f(x)',fontdict={'fontsize':16})\n",
    "axS.set_xlim(-2.1, 2.1)\n",
    "axS.set_ylim(0., 1.0)\n",
    "axS.tick_params(labelsize = 14)\n",
    "plt.show()\n",
    "print(f\"Sum of softmax values: {softmax(z2).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1998a632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vID.end(cwd0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4360861-b819-43c5-a78d-07b4020e0668",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ba2b25-05d5-4ece-8857-084aff57bde2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
